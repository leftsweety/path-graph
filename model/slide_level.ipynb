{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizx43/anaconda3/envs/gigapath/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "embedding_dir = '../patch_embeddings/'\n",
    "slide_20X_df = pd.read_hdf(embedding_dir+'20Xslide_embeding.h5')\n",
    "slide_10X_df = pd.read_hdf(embedding_dir+'10Xslide_embeding.h5')\n",
    "slide_5X_df = pd.read_hdf(embedding_dir+'5Xslide_embeding.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP import MLPModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.train_utils import CustomDataset\n",
    "slide_embed = slide_20X_df.iloc[:, :768]\n",
    "dataset = CustomDataset(slide_embed, slide_20X_df.drug_response)\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for features, labels in dataloader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        preds = torch.round(torch.sigmoid(outputs)).detach().cpu().numpy()  # Detach before converting to numpy\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy, f1, auc\n",
    "\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * features.size(0)\n",
    "\n",
    "            preds = torch.round(torch.sigmoid(outputs)).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Start Epoch 1/300--\n",
      "Epoch 1/300, Train Loss: 0.6705, Acc: 0.6628, F1: 0.0333, AUC: 0.4767\n",
      "Epoch 1/300, Valid Loss: 0.6665, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 2/300--\n",
      "Epoch 2/300, Train Loss: 0.6306, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Valid Loss: 0.6284, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 3/300--\n",
      "Epoch 3/300, Train Loss: 0.6458, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6299, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 4/300--\n",
      "Epoch 4/300, Train Loss: 0.6391, Acc: 0.6977, F1: 0.0000, AUC: 0.4959\n",
      "Epoch 4/300, Valid Loss: 0.6374, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 5/300--\n",
      "Epoch 5/300, Train Loss: 0.6350, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.6376, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 6/300--\n",
      "Epoch 6/300, Train Loss: 0.6328, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.6355, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 7/300--\n",
      "Epoch 7/300, Train Loss: 0.6302, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.6394, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 8/300--\n",
      "Epoch 8/300, Train Loss: 0.6150, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.6411, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 9/300--\n",
      "Epoch 9/300, Train Loss: 0.6330, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.6467, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 10/300--\n",
      "Epoch 10/300, Train Loss: 0.6114, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.6499, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 11/300--\n",
      "Epoch 11/300, Train Loss: 0.6192, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.6497, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 12/300--\n",
      "Epoch 12/300, Train Loss: 0.6095, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.6641, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 13/300--\n",
      "Epoch 13/300, Train Loss: 0.6248, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.6536, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 14/300--\n",
      "Epoch 14/300, Train Loss: 0.6149, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.6599, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 15/300--\n",
      "Epoch 15/300, Train Loss: 0.6010, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.6635, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 16/300--\n",
      "Epoch 16/300, Train Loss: 0.6304, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.6592, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 17/300--\n",
      "Epoch 17/300, Train Loss: 0.6006, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.6796, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 18/300--\n",
      "Epoch 18/300, Train Loss: 0.6286, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Valid Loss: 0.6686, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 19/300--\n",
      "Epoch 19/300, Train Loss: 0.6091, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Valid Loss: 0.6784, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 20/300--\n",
      "Epoch 20/300, Train Loss: 0.6047, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Valid Loss: 0.6848, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 21/300--\n",
      "Epoch 21/300, Train Loss: 0.6017, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Valid Loss: 0.6939, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 22/300--\n",
      "Epoch 22/300, Train Loss: 0.6199, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Valid Loss: 0.7007, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 23/300--\n",
      "Epoch 23/300, Train Loss: 0.6084, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Valid Loss: 0.6905, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 24/300--\n",
      "Epoch 24/300, Train Loss: 0.5982, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Valid Loss: 0.8020, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 25/300--\n",
      "Epoch 25/300, Train Loss: 0.5957, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Valid Loss: 0.7095, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 26/300--\n",
      "Epoch 26/300, Train Loss: 0.5973, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Valid Loss: 0.7377, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 27/300--\n",
      "Epoch 27/300, Train Loss: 0.6049, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Valid Loss: 0.6850, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 28/300--\n",
      "Epoch 28/300, Train Loss: 0.5915, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Valid Loss: 0.7402, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 29/300--\n",
      "Epoch 29/300, Train Loss: 0.5866, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Valid Loss: 0.7128, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 30/300--\n",
      "Epoch 30/300, Train Loss: 0.5896, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Valid Loss: 0.8485, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 31/300--\n",
      "Epoch 31/300, Train Loss: 0.5839, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Valid Loss: 0.7181, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 32/300--\n",
      "Epoch 32/300, Train Loss: 0.5840, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Valid Loss: 0.7330, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 33/300--\n",
      "Epoch 33/300, Train Loss: 0.5976, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Valid Loss: 0.7258, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 34/300--\n",
      "Epoch 34/300, Train Loss: 0.6019, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Valid Loss: 0.7264, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 35/300--\n",
      "Epoch 35/300, Train Loss: 0.5861, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Valid Loss: 0.7482, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 36/300--\n",
      "Epoch 36/300, Train Loss: 0.5937, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Valid Loss: 0.7200, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 37/300--\n",
      "Epoch 37/300, Train Loss: 0.5857, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Valid Loss: 0.7285, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 38/300--\n",
      "Epoch 38/300, Train Loss: 0.5888, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Valid Loss: 0.7688, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 39/300--\n",
      "Epoch 39/300, Train Loss: 0.6097, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Valid Loss: 0.7978, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 40/300--\n",
      "Epoch 40/300, Train Loss: 0.5977, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Valid Loss: 0.7323, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 41/300--\n",
      "Epoch 41/300, Train Loss: 0.5728, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Valid Loss: 0.7831, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 42/300--\n",
      "Epoch 42/300, Train Loss: 0.5615, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Valid Loss: 0.7827, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 43/300--\n",
      "Epoch 43/300, Train Loss: 0.5675, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 43/300, Valid Loss: 0.7873, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 44/300--\n",
      "Epoch 44/300, Train Loss: 0.5798, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Valid Loss: 0.7972, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 45/300--\n",
      "Epoch 45/300, Train Loss: 0.5760, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Valid Loss: 0.8245, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 46/300--\n",
      "Epoch 46/300, Train Loss: 0.5706, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Valid Loss: 0.8414, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 47/300--\n",
      "Epoch 47/300, Train Loss: 0.5754, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Valid Loss: 0.8504, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 48/300--\n",
      "Epoch 48/300, Train Loss: 0.5576, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Valid Loss: 0.8927, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 49/300--\n",
      "Epoch 49/300, Train Loss: 0.5481, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Valid Loss: 0.9421, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 50/300--\n",
      "Epoch 50/300, Train Loss: 0.5484, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Valid Loss: 0.9052, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 51/300--\n",
      "Epoch 51/300, Train Loss: 0.5312, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Valid Loss: 0.8971, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 52/300--\n",
      "Epoch 52/300, Train Loss: 0.5539, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Valid Loss: 0.8119, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 53/300--\n",
      "Epoch 53/300, Train Loss: 0.5501, Acc: 0.6919, F1: 0.0000, AUC: 0.4917\n",
      "Epoch 53/300, Valid Loss: 0.8848, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 54/300--\n",
      "Epoch 54/300, Train Loss: 0.5567, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 54/300, Valid Loss: 0.8974, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 55/300--\n",
      "Epoch 55/300, Train Loss: 0.5542, Acc: 0.7035, F1: 0.1053, AUC: 0.5170\n",
      "Epoch 55/300, Valid Loss: 0.9166, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 56/300--\n",
      "Epoch 56/300, Train Loss: 0.5677, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Valid Loss: 0.8964, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 57/300--\n",
      "Epoch 57/300, Train Loss: 0.5506, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Valid Loss: 0.9549, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 58/300--\n",
      "Epoch 58/300, Train Loss: 0.5348, Acc: 0.7209, F1: 0.3333, AUC: 0.5805\n",
      "Epoch 58/300, Valid Loss: 1.0043, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "--Start Epoch 59/300--\n",
      "Epoch 59/300, Train Loss: 0.5459, Acc: 0.7267, F1: 0.4051, AUC: 0.6073\n",
      "Epoch 59/300, Valid Loss: 1.1276, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 60/300--\n",
      "Epoch 60/300, Train Loss: 0.5678, Acc: 0.7035, F1: 0.3544, AUC: 0.5794\n",
      "Epoch 60/300, Valid Loss: 0.8848, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 61/300--\n",
      "Epoch 61/300, Train Loss: 0.5406, Acc: 0.7267, F1: 0.4051, AUC: 0.6073\n",
      "Epoch 61/300, Valid Loss: 0.9875, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 62/300--\n",
      "Epoch 62/300, Train Loss: 0.5547, Acc: 0.7209, F1: 0.4419, AUC: 0.6202\n",
      "Epoch 62/300, Valid Loss: 0.9563, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "--Start Epoch 63/300--\n",
      "Epoch 63/300, Train Loss: 0.5905, Acc: 0.6802, F1: 0.1538, AUC: 0.5118\n",
      "Epoch 63/300, Valid Loss: 0.9790, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 64/300--\n",
      "Epoch 64/300, Train Loss: 0.5385, Acc: 0.7093, F1: 0.0385, AUC: 0.5098\n",
      "Epoch 64/300, Valid Loss: 0.9174, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 65/300--\n",
      "Epoch 65/300, Train Loss: 0.5418, Acc: 0.7093, F1: 0.0741, AUC: 0.5155\n",
      "Epoch 65/300, Valid Loss: 1.0024, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 66/300--\n",
      "Epoch 66/300, Train Loss: 0.5480, Acc: 0.7093, F1: 0.3590, AUC: 0.5835\n",
      "Epoch 66/300, Valid Loss: 1.0850, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "--Start Epoch 67/300--\n",
      "Epoch 67/300, Train Loss: 0.5245, Acc: 0.7093, F1: 0.4048, AUC: 0.6006\n",
      "Epoch 67/300, Valid Loss: 1.0668, Acc: 0.5349, F1: 0.4737, AUC: 0.5628\n",
      "--Start Epoch 68/300--\n",
      "Epoch 68/300, Train Loss: 0.5639, Acc: 0.6337, F1: 0.2588, AUC: 0.5128\n",
      "Epoch 68/300, Valid Loss: 1.0064, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 69/300--\n",
      "Epoch 69/300, Train Loss: 0.6042, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 69/300, Valid Loss: 1.0853, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 70/300--\n",
      "Epoch 70/300, Train Loss: 0.5680, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 70/300, Valid Loss: 0.9963, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 71/300--\n",
      "Epoch 71/300, Train Loss: 0.5692, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 71/300, Valid Loss: 0.9391, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 72/300--\n",
      "Epoch 72/300, Train Loss: 0.5761, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Valid Loss: 0.9471, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 73/300--\n",
      "Epoch 73/300, Train Loss: 0.5560, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 73/300, Valid Loss: 1.0085, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 74/300--\n",
      "Epoch 74/300, Train Loss: 0.5565, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 74/300, Valid Loss: 1.1607, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 75/300--\n",
      "Epoch 75/300, Train Loss: 0.5960, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 75/300, Valid Loss: 1.1649, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 76/300--\n",
      "Epoch 76/300, Train Loss: 0.5718, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 76/300, Valid Loss: 1.0366, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 77/300--\n",
      "Epoch 77/300, Train Loss: 0.5761, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Valid Loss: 1.1659, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 78/300--\n",
      "Epoch 78/300, Train Loss: 0.5499, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 78/300, Valid Loss: 1.2721, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 79/300--\n",
      "Epoch 79/300, Train Loss: 0.5464, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Valid Loss: 1.0814, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 80/300--\n",
      "Epoch 80/300, Train Loss: 0.5711, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Valid Loss: 0.9993, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 81/300--\n",
      "Epoch 81/300, Train Loss: 0.5401, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 81/300, Valid Loss: 1.1452, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 82/300--\n",
      "Epoch 82/300, Train Loss: 0.5599, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 82/300, Valid Loss: 1.0671, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 83/300--\n",
      "Epoch 83/300, Train Loss: 0.5594, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 83/300, Valid Loss: 1.0622, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 84/300--\n",
      "Epoch 84/300, Train Loss: 0.5848, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 84/300, Valid Loss: 1.1781, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 85/300--\n",
      "Epoch 85/300, Train Loss: 0.5077, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Valid Loss: 1.2441, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 86/300--\n",
      "Epoch 86/300, Train Loss: 0.5732, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 86/300, Valid Loss: 1.4533, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 87/300--\n",
      "Epoch 87/300, Train Loss: 0.5323, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 87/300, Valid Loss: 1.3266, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 88/300--\n",
      "Epoch 88/300, Train Loss: 0.5709, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 88/300, Valid Loss: 1.1677, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 89/300--\n",
      "Epoch 89/300, Train Loss: 0.5421, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 89/300, Valid Loss: 1.2461, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 90/300--\n",
      "Epoch 90/300, Train Loss: 0.5094, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 90/300, Valid Loss: 1.2396, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 91/300--\n",
      "Epoch 91/300, Train Loss: 0.5227, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 91/300, Valid Loss: 1.2625, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 92/300--\n",
      "Epoch 92/300, Train Loss: 0.5129, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 92/300, Valid Loss: 1.2599, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 93/300--\n",
      "Epoch 93/300, Train Loss: 0.5931, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 93/300, Valid Loss: 1.1804, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 94/300--\n",
      "Epoch 94/300, Train Loss: 0.5106, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 94/300, Valid Loss: 1.3940, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 95/300--\n",
      "Epoch 95/300, Train Loss: 0.5104, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 95/300, Valid Loss: 1.4718, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 96/300--\n",
      "Epoch 96/300, Train Loss: 0.5927, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Valid Loss: 1.3792, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 97/300--\n",
      "Epoch 97/300, Train Loss: 0.5308, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 97/300, Valid Loss: 1.4009, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 98/300--\n",
      "Epoch 98/300, Train Loss: 0.5278, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 98/300, Valid Loss: 1.4150, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 99/300--\n",
      "Epoch 99/300, Train Loss: 0.5151, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 99/300, Valid Loss: 1.2249, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 100/300--\n",
      "Epoch 100/300, Train Loss: 0.4988, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 100/300, Valid Loss: 1.4291, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 101/300--\n",
      "Epoch 101/300, Train Loss: 0.5063, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 101/300, Valid Loss: 1.4903, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 102/300--\n",
      "Epoch 102/300, Train Loss: 0.4992, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 102/300, Valid Loss: 1.4185, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 103/300--\n",
      "Epoch 103/300, Train Loss: 0.5158, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 103/300, Valid Loss: 1.4735, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 104/300--\n",
      "Epoch 104/300, Train Loss: 0.4927, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 104/300, Valid Loss: 1.5529, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 105/300--\n",
      "Epoch 105/300, Train Loss: 0.5589, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 105/300, Valid Loss: 1.3561, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 106/300--\n",
      "Epoch 106/300, Train Loss: 0.5406, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 106/300, Valid Loss: 1.2654, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 107/300--\n",
      "Epoch 107/300, Train Loss: 0.6279, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 107/300, Valid Loss: 1.4051, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 108/300--\n",
      "Epoch 108/300, Train Loss: 0.5785, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 108/300, Valid Loss: 1.4585, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 109/300--\n",
      "Epoch 109/300, Train Loss: 0.5261, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 109/300, Valid Loss: 1.2532, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 110/300--\n",
      "Epoch 110/300, Train Loss: 0.6231, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 110/300, Valid Loss: 1.2773, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 111/300--\n",
      "Epoch 111/300, Train Loss: 0.5971, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 111/300, Valid Loss: 1.5669, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 112/300--\n",
      "Epoch 112/300, Train Loss: 0.5391, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 112/300, Valid Loss: 1.4342, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 113/300--\n",
      "Epoch 113/300, Train Loss: 0.5217, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 113/300, Valid Loss: 1.3858, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 114/300--\n",
      "Epoch 114/300, Train Loss: 0.5302, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 114/300, Valid Loss: 1.3971, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 115/300--\n",
      "Epoch 115/300, Train Loss: 0.4825, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 115/300, Valid Loss: 1.4787, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 116/300--\n",
      "Epoch 116/300, Train Loss: 0.4934, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 116/300, Valid Loss: 1.5771, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 117/300--\n",
      "Epoch 117/300, Train Loss: 0.5349, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 117/300, Valid Loss: 1.4551, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 118/300--\n",
      "Epoch 118/300, Train Loss: 0.5175, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 118/300, Valid Loss: 1.9221, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 119/300--\n",
      "Epoch 119/300, Train Loss: 0.4677, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 119/300, Valid Loss: 1.8960, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 120/300--\n",
      "Epoch 120/300, Train Loss: 0.5422, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 120/300, Valid Loss: 1.8407, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 121/300--\n",
      "Epoch 121/300, Train Loss: 0.5141, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 121/300, Valid Loss: 1.6424, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 122/300--\n",
      "Epoch 122/300, Train Loss: 0.4776, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 122/300, Valid Loss: 1.6144, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 123/300--\n",
      "Epoch 123/300, Train Loss: 0.4815, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 123/300, Valid Loss: 1.6176, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 124/300--\n",
      "Epoch 124/300, Train Loss: 0.5338, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 124/300, Valid Loss: 1.8115, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 125/300--\n",
      "Epoch 125/300, Train Loss: 0.5174, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 125/300, Valid Loss: 1.8334, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 126/300--\n",
      "Epoch 126/300, Train Loss: 0.4901, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 126/300, Valid Loss: 1.9991, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 127/300--\n",
      "Epoch 127/300, Train Loss: 0.4668, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 127/300, Valid Loss: 2.0412, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 128/300--\n",
      "Epoch 128/300, Train Loss: 0.4443, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 128/300, Valid Loss: 1.8920, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 129/300--\n",
      "Epoch 129/300, Train Loss: 0.4676, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 129/300, Valid Loss: 1.8043, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 130/300--\n",
      "Epoch 130/300, Train Loss: 0.5335, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 130/300, Valid Loss: 1.5597, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 131/300--\n",
      "Epoch 131/300, Train Loss: 0.4860, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 131/300, Valid Loss: 1.9574, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 132/300--\n",
      "Epoch 132/300, Train Loss: 0.4704, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 132/300, Valid Loss: 1.5554, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 133/300--\n",
      "Epoch 133/300, Train Loss: 0.4681, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 133/300, Valid Loss: 1.9693, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 134/300--\n",
      "Epoch 134/300, Train Loss: 0.5015, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 134/300, Valid Loss: 1.8674, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 135/300--\n",
      "Epoch 135/300, Train Loss: 0.4895, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 135/300, Valid Loss: 1.4772, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 136/300--\n",
      "Epoch 136/300, Train Loss: 0.4639, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 136/300, Valid Loss: 1.8994, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 137/300--\n",
      "Epoch 137/300, Train Loss: 0.5110, Acc: 0.7093, F1: 0.0385, AUC: 0.5098\n",
      "Epoch 137/300, Valid Loss: 1.6177, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 138/300--\n",
      "Epoch 138/300, Train Loss: 0.4670, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 138/300, Valid Loss: 1.8190, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 139/300--\n",
      "Epoch 139/300, Train Loss: 0.4464, Acc: 0.7151, F1: 0.3636, AUC: 0.5877\n",
      "Epoch 139/300, Valid Loss: 1.6945, Acc: 0.5349, F1: 0.4737, AUC: 0.5628\n",
      "--Start Epoch 140/300--\n",
      "Epoch 140/300, Train Loss: 0.4472, Acc: 0.7209, F1: 0.6250, AUC: 0.7393\n",
      "Epoch 140/300, Valid Loss: 2.4772, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "--Start Epoch 141/300--\n",
      "Epoch 141/300, Train Loss: 0.4764, Acc: 0.6512, F1: 0.4340, AUC: 0.5933\n",
      "Epoch 141/300, Valid Loss: 2.2506, Acc: 0.6279, F1: 0.3333, AUC: 0.5394\n",
      "--Start Epoch 142/300--\n",
      "Epoch 142/300, Train Loss: 0.4298, Acc: 0.7558, F1: 0.6379, AUC: 0.7470\n",
      "Epoch 142/300, Valid Loss: 2.5175, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "--Start Epoch 143/300--\n",
      "Epoch 143/300, Train Loss: 0.5554, Acc: 0.6686, F1: 0.5440, AUC: 0.6680\n",
      "Epoch 143/300, Valid Loss: 1.4392, Acc: 0.2558, F1: 0.4074, AUC: 0.3929\n",
      "--Start Epoch 144/300--\n",
      "Epoch 144/300, Train Loss: 0.5537, Acc: 0.6221, F1: 0.0845, AUC: 0.4592\n",
      "Epoch 144/300, Valid Loss: 1.9071, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 145/300--\n",
      "Epoch 145/300, Train Loss: 0.4810, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 145/300, Valid Loss: 2.1677, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 146/300--\n",
      "Epoch 146/300, Train Loss: 0.4761, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 146/300, Valid Loss: 1.6661, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 147/300--\n",
      "Epoch 147/300, Train Loss: 0.4370, Acc: 0.7384, F1: 0.4444, AUC: 0.6269\n",
      "Epoch 147/300, Valid Loss: 1.8165, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 148/300--\n",
      "Epoch 148/300, Train Loss: 0.4378, Acc: 0.7500, F1: 0.6055, AUC: 0.7202\n",
      "Epoch 148/300, Valid Loss: 1.7559, Acc: 0.5349, F1: 0.4444, AUC: 0.5443\n",
      "--Start Epoch 149/300--\n",
      "Epoch 149/300, Train Loss: 0.4573, Acc: 0.6802, F1: 0.2857, AUC: 0.5459\n",
      "Epoch 149/300, Valid Loss: 2.0165, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 150/300--\n",
      "Epoch 150/300, Train Loss: 0.5288, Acc: 0.6453, F1: 0.5414, AUC: 0.6629\n",
      "Epoch 150/300, Valid Loss: 1.7025, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 151/300--\n",
      "Epoch 151/300, Train Loss: 0.4761, Acc: 0.6802, F1: 0.0984, AUC: 0.5005\n",
      "Epoch 151/300, Valid Loss: 1.9136, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 152/300--\n",
      "Epoch 152/300, Train Loss: 0.4132, Acc: 0.6977, F1: 0.3659, AUC: 0.5809\n",
      "Epoch 152/300, Valid Loss: 2.0505, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "--Start Epoch 153/300--\n",
      "Epoch 153/300, Train Loss: 0.4415, Acc: 0.6860, F1: 0.4808, AUC: 0.6294\n",
      "Epoch 153/300, Valid Loss: 2.1966, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 154/300--\n",
      "Epoch 154/300, Train Loss: 0.4280, Acc: 0.7907, F1: 0.6842, AUC: 0.7832\n",
      "Epoch 154/300, Valid Loss: 2.1093, Acc: 0.7442, F1: 0.4762, AUC: 0.6441\n",
      "--Start Epoch 155/300--\n",
      "Epoch 155/300, Train Loss: 0.4570, Acc: 0.7267, F1: 0.5983, AUC: 0.7150\n",
      "Epoch 155/300, Valid Loss: 2.1496, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 156/300--\n",
      "Epoch 156/300, Train Loss: 0.4381, Acc: 0.7326, F1: 0.6034, AUC: 0.7192\n",
      "Epoch 156/300, Valid Loss: 2.2870, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 157/300--\n",
      "Epoch 157/300, Train Loss: 0.5702, Acc: 0.5640, F1: 0.4828, AUC: 0.5993\n",
      "Epoch 157/300, Valid Loss: 2.2009, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 158/300--\n",
      "Epoch 158/300, Train Loss: 0.4985, Acc: 0.6744, F1: 0.1765, AUC: 0.5134\n",
      "Epoch 158/300, Valid Loss: 1.6965, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 159/300--\n",
      "Epoch 159/300, Train Loss: 0.4511, Acc: 0.6744, F1: 0.3488, AUC: 0.5644\n",
      "Epoch 159/300, Valid Loss: 2.2957, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 160/300--\n",
      "Epoch 160/300, Train Loss: 0.4358, Acc: 0.7326, F1: 0.5741, AUC: 0.6965\n",
      "Epoch 160/300, Valid Loss: 2.1452, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 161/300--\n",
      "Epoch 161/300, Train Loss: 0.4525, Acc: 0.7384, F1: 0.6218, AUC: 0.7346\n",
      "Epoch 161/300, Valid Loss: 2.2059, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 162/300--\n",
      "Epoch 162/300, Train Loss: 0.4693, Acc: 0.7151, F1: 0.6016, AUC: 0.7181\n",
      "Epoch 162/300, Valid Loss: 2.0213, Acc: 0.5814, F1: 0.3571, AUC: 0.5234\n",
      "--Start Epoch 163/300--\n",
      "Epoch 163/300, Train Loss: 0.4701, Acc: 0.7209, F1: 0.6129, AUC: 0.7279\n",
      "Epoch 163/300, Valid Loss: 2.1777, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 164/300--\n",
      "Epoch 164/300, Train Loss: 0.4326, Acc: 0.7791, F1: 0.6607, AUC: 0.7636\n",
      "Epoch 164/300, Valid Loss: 1.8312, Acc: 0.5814, F1: 0.2500, AUC: 0.4865\n",
      "--Start Epoch 165/300--\n",
      "Epoch 165/300, Train Loss: 0.4474, Acc: 0.7151, F1: 0.5739, AUC: 0.6954\n",
      "Epoch 165/300, Valid Loss: 1.9211, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 166/300--\n",
      "Epoch 166/300, Train Loss: 0.4349, Acc: 0.7442, F1: 0.6562, AUC: 0.7671\n",
      "Epoch 166/300, Valid Loss: 2.0165, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 167/300--\n",
      "Epoch 167/300, Train Loss: 0.4441, Acc: 0.7326, F1: 0.6406, AUC: 0.7532\n",
      "Epoch 167/300, Valid Loss: 2.4019, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 168/300--\n",
      "Epoch 168/300, Train Loss: 0.4734, Acc: 0.7907, F1: 0.6604, AUC: 0.7605\n",
      "Epoch 168/300, Valid Loss: 1.7241, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "--Start Epoch 169/300--\n",
      "Epoch 169/300, Train Loss: 0.4384, Acc: 0.7500, F1: 0.6387, AUC: 0.7486\n",
      "Epoch 169/300, Valid Loss: 1.8387, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 170/300--\n",
      "Epoch 170/300, Train Loss: 0.4872, Acc: 0.7267, F1: 0.6050, AUC: 0.7207\n",
      "Epoch 170/300, Valid Loss: 1.7625, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 171/300--\n",
      "Epoch 171/300, Train Loss: 0.4771, Acc: 0.6570, F1: 0.5630, AUC: 0.6825\n",
      "Epoch 171/300, Valid Loss: 1.6747, Acc: 0.2791, F1: 0.4364, AUC: 0.4286\n",
      "--Start Epoch 172/300--\n",
      "Epoch 172/300, Train Loss: 0.4898, Acc: 0.6570, F1: 0.5874, AUC: 0.7052\n",
      "Epoch 172/300, Valid Loss: 2.2966, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "--Start Epoch 173/300--\n",
      "Epoch 173/300, Train Loss: 0.4568, Acc: 0.7151, F1: 0.6142, AUC: 0.7295\n",
      "Epoch 173/300, Valid Loss: 1.9398, Acc: 0.6047, F1: 0.4516, AUC: 0.5776\n",
      "--Start Epoch 174/300--\n",
      "Epoch 174/300, Train Loss: 0.4391, Acc: 0.7442, F1: 0.6207, AUC: 0.7331\n",
      "Epoch 174/300, Valid Loss: 2.3532, Acc: 0.7209, F1: 0.4545, AUC: 0.6268\n",
      "--Start Epoch 175/300--\n",
      "Epoch 175/300, Train Loss: 0.4529, Acc: 0.7442, F1: 0.6207, AUC: 0.7331\n",
      "Epoch 175/300, Valid Loss: 1.9246, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 176/300--\n",
      "Epoch 176/300, Train Loss: 0.4647, Acc: 0.7558, F1: 0.6379, AUC: 0.7470\n",
      "Epoch 176/300, Valid Loss: 2.1821, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "--Start Epoch 177/300--\n",
      "Epoch 177/300, Train Loss: 0.4574, Acc: 0.7326, F1: 0.6230, AUC: 0.7362\n",
      "Epoch 177/300, Valid Loss: 2.1731, Acc: 0.6512, F1: 0.4444, AUC: 0.5936\n",
      "--Start Epoch 178/300--\n",
      "Epoch 178/300, Train Loss: 0.5087, Acc: 0.6860, F1: 0.5781, AUC: 0.6975\n",
      "Epoch 178/300, Valid Loss: 2.1052, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 179/300--\n",
      "Epoch 179/300, Train Loss: 0.4455, Acc: 0.7326, F1: 0.6167, AUC: 0.7305\n",
      "Epoch 179/300, Valid Loss: 1.8327, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 180/300--\n",
      "Epoch 180/300, Train Loss: 0.4319, Acc: 0.7326, F1: 0.5818, AUC: 0.7022\n",
      "Epoch 180/300, Valid Loss: 2.2681, Acc: 0.6512, F1: 0.2105, AUC: 0.5197\n",
      "--Start Epoch 181/300--\n",
      "Epoch 181/300, Train Loss: 0.4034, Acc: 0.7500, F1: 0.6055, AUC: 0.7202\n",
      "Epoch 181/300, Valid Loss: 2.3386, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 182/300--\n",
      "Epoch 182/300, Train Loss: 0.5045, Acc: 0.7209, F1: 0.6250, AUC: 0.7393\n",
      "Epoch 182/300, Valid Loss: 1.7843, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 183/300--\n",
      "Epoch 183/300, Train Loss: 0.4185, Acc: 0.7616, F1: 0.6496, AUC: 0.7568\n",
      "Epoch 183/300, Valid Loss: 2.1271, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 184/300--\n",
      "Epoch 184/300, Train Loss: 0.4270, Acc: 0.7384, F1: 0.6617, AUC: 0.7743\n",
      "Epoch 184/300, Valid Loss: 2.4962, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 185/300--\n",
      "Epoch 185/300, Train Loss: 0.4425, Acc: 0.7326, F1: 0.6230, AUC: 0.7362\n",
      "Epoch 185/300, Valid Loss: 2.1302, Acc: 0.6279, F1: 0.3333, AUC: 0.5394\n",
      "--Start Epoch 186/300--\n",
      "Epoch 186/300, Train Loss: 0.4253, Acc: 0.7674, F1: 0.6721, AUC: 0.7780\n",
      "Epoch 186/300, Valid Loss: 2.4556, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "--Start Epoch 187/300--\n",
      "Epoch 187/300, Train Loss: 0.5433, Acc: 0.5930, F1: 0.5205, AUC: 0.6370\n",
      "Epoch 187/300, Valid Loss: 2.0953, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "--Start Epoch 188/300--\n",
      "Epoch 188/300, Train Loss: 0.4042, Acc: 0.7849, F1: 0.6726, AUC: 0.7734\n",
      "Epoch 188/300, Valid Loss: 2.0958, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 189/300--\n",
      "Epoch 189/300, Train Loss: 0.4228, Acc: 0.7558, F1: 0.6557, AUC: 0.7641\n",
      "Epoch 189/300, Valid Loss: 2.1439, Acc: 0.5581, F1: 0.3448, AUC: 0.5062\n",
      "--Start Epoch 190/300--\n",
      "Epoch 190/300, Train Loss: 0.4112, Acc: 0.7907, F1: 0.7000, AUC: 0.8002\n",
      "Epoch 190/300, Valid Loss: 2.4355, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 191/300--\n",
      "Epoch 191/300, Train Loss: 0.5491, Acc: 0.6453, F1: 0.5906, AUC: 0.7082\n",
      "Epoch 191/300, Valid Loss: 1.4669, Acc: 0.2791, F1: 0.4364, AUC: 0.4286\n",
      "--Start Epoch 192/300--\n",
      "Epoch 192/300, Train Loss: 0.4630, Acc: 0.6686, F1: 0.5714, AUC: 0.6907\n",
      "Epoch 192/300, Valid Loss: 2.2224, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 193/300--\n",
      "Epoch 193/300, Train Loss: 0.4040, Acc: 0.7500, F1: 0.6055, AUC: 0.7202\n",
      "Epoch 193/300, Valid Loss: 2.2256, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 194/300--\n",
      "Epoch 194/300, Train Loss: 0.4359, Acc: 0.7616, F1: 0.6720, AUC: 0.7795\n",
      "Epoch 194/300, Valid Loss: 2.0475, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 195/300--\n",
      "Epoch 195/300, Train Loss: 0.4390, Acc: 0.7616, F1: 0.6667, AUC: 0.7739\n",
      "Epoch 195/300, Valid Loss: 1.9377, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "--Start Epoch 196/300--\n",
      "Epoch 196/300, Train Loss: 0.4253, Acc: 0.7849, F1: 0.6838, AUC: 0.7847\n",
      "Epoch 196/300, Valid Loss: 2.6604, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "--Start Epoch 197/300--\n",
      "Epoch 197/300, Train Loss: 0.4055, Acc: 0.7733, F1: 0.6880, AUC: 0.7935\n",
      "Epoch 197/300, Valid Loss: 2.3637, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "--Start Epoch 198/300--\n",
      "Epoch 198/300, Train Loss: 0.4127, Acc: 0.7791, F1: 0.6935, AUC: 0.7976\n",
      "Epoch 198/300, Valid Loss: 2.4041, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 199/300--\n",
      "Epoch 199/300, Train Loss: 0.4300, Acc: 0.7558, F1: 0.6769, AUC: 0.7867\n",
      "Epoch 199/300, Valid Loss: 2.3875, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "--Start Epoch 200/300--\n",
      "Epoch 200/300, Train Loss: 0.4701, Acc: 0.6919, F1: 0.6241, AUC: 0.7413\n",
      "Epoch 200/300, Valid Loss: 2.4026, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 201/300--\n",
      "Epoch 201/300, Train Loss: 0.4166, Acc: 0.7267, F1: 0.6412, AUC: 0.7547\n",
      "Epoch 201/300, Valid Loss: 2.9667, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 202/300--\n",
      "Epoch 202/300, Train Loss: 0.3970, Acc: 0.7558, F1: 0.6818, AUC: 0.7924\n",
      "Epoch 202/300, Valid Loss: 3.0810, Acc: 0.6047, F1: 0.2609, AUC: 0.5037\n",
      "--Start Epoch 203/300--\n",
      "Epoch 203/300, Train Loss: 0.4556, Acc: 0.7093, F1: 0.6429, AUC: 0.7594\n",
      "Epoch 203/300, Valid Loss: 3.1297, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 204/300--\n",
      "Epoch 204/300, Train Loss: 0.4086, Acc: 0.7791, F1: 0.7031, AUC: 0.8089\n",
      "Epoch 204/300, Valid Loss: 2.7304, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 205/300--\n",
      "Epoch 205/300, Train Loss: 0.3834, Acc: 0.7965, F1: 0.7107, AUC: 0.8100\n",
      "Epoch 205/300, Valid Loss: 2.2938, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 206/300--\n",
      "Epoch 206/300, Train Loss: 0.4719, Acc: 0.6628, F1: 0.6027, AUC: 0.7206\n",
      "Epoch 206/300, Valid Loss: 2.5379, Acc: 0.5581, F1: 0.3448, AUC: 0.5062\n",
      "--Start Epoch 207/300--\n",
      "Epoch 207/300, Train Loss: 0.4340, Acc: 0.7616, F1: 0.6822, AUC: 0.7909\n",
      "Epoch 207/300, Valid Loss: 2.3201, Acc: 0.5814, F1: 0.4000, AUC: 0.5419\n",
      "--Start Epoch 208/300--\n",
      "Epoch 208/300, Train Loss: 0.4098, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
      "Epoch 208/300, Valid Loss: 1.9849, Acc: 0.5581, F1: 0.4865, AUC: 0.5800\n",
      "--Start Epoch 209/300--\n",
      "Epoch 209/300, Train Loss: 0.4108, Acc: 0.7849, F1: 0.6783, AUC: 0.7790\n",
      "Epoch 209/300, Valid Loss: 2.4010, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 210/300--\n",
      "Epoch 210/300, Train Loss: 0.3760, Acc: 0.7965, F1: 0.7107, AUC: 0.8100\n",
      "Epoch 210/300, Valid Loss: 2.3679, Acc: 0.6047, F1: 0.4848, AUC: 0.5961\n",
      "--Start Epoch 211/300--\n",
      "Epoch 211/300, Train Loss: 0.4107, Acc: 0.7674, F1: 0.6923, AUC: 0.8007\n",
      "Epoch 211/300, Valid Loss: 2.6463, Acc: 0.6512, F1: 0.3478, AUC: 0.5567\n",
      "--Start Epoch 212/300--\n",
      "Epoch 212/300, Train Loss: 0.3720, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 212/300, Valid Loss: 2.8738, Acc: 0.6279, F1: 0.2727, AUC: 0.5209\n",
      "--Start Epoch 213/300--\n",
      "Epoch 213/300, Train Loss: 0.4334, Acc: 0.7500, F1: 0.6667, AUC: 0.7769\n",
      "Epoch 213/300, Valid Loss: 2.5394, Acc: 0.5581, F1: 0.4242, AUC: 0.5431\n",
      "--Start Epoch 214/300--\n",
      "Epoch 214/300, Train Loss: 0.3825, Acc: 0.7791, F1: 0.6984, AUC: 0.8033\n",
      "Epoch 214/300, Valid Loss: 2.8087, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 215/300--\n",
      "Epoch 215/300, Train Loss: 0.5481, Acc: 0.6047, F1: 0.5641, AUC: 0.6793\n",
      "Epoch 215/300, Valid Loss: 2.3899, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 216/300--\n",
      "Epoch 216/300, Train Loss: 0.4209, Acc: 0.7500, F1: 0.6560, AUC: 0.7656\n",
      "Epoch 216/300, Valid Loss: 2.6437, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 217/300--\n",
      "Epoch 217/300, Train Loss: 0.6211, Acc: 0.4884, F1: 0.4054, AUC: 0.5173\n",
      "Epoch 217/300, Valid Loss: 2.6645, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 218/300--\n",
      "Epoch 218/300, Train Loss: 0.3769, Acc: 0.7558, F1: 0.5625, AUC: 0.6903\n",
      "Epoch 218/300, Valid Loss: 2.4526, Acc: 0.6047, F1: 0.5143, AUC: 0.6145\n",
      "--Start Epoch 219/300--\n",
      "Epoch 219/300, Train Loss: 0.4787, Acc: 0.7267, F1: 0.6179, AUC: 0.7321\n",
      "Epoch 219/300, Valid Loss: 2.5128, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "--Start Epoch 220/300--\n",
      "Epoch 220/300, Train Loss: 0.3889, Acc: 0.7733, F1: 0.6422, AUC: 0.7481\n",
      "Epoch 220/300, Valid Loss: 2.1835, Acc: 0.6512, F1: 0.3478, AUC: 0.5567\n",
      "--Start Epoch 221/300--\n",
      "Epoch 221/300, Train Loss: 0.4252, Acc: 0.7326, F1: 0.6462, AUC: 0.7589\n",
      "Epoch 221/300, Valid Loss: 2.4103, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 222/300--\n",
      "Epoch 222/300, Train Loss: 0.4197, Acc: 0.7907, F1: 0.6949, AUC: 0.7945\n",
      "Epoch 222/300, Valid Loss: 2.1512, Acc: 0.6744, F1: 0.4167, AUC: 0.5924\n",
      "--Start Epoch 223/300--\n",
      "Epoch 223/300, Train Loss: 0.3897, Acc: 0.8023, F1: 0.6964, AUC: 0.7914\n",
      "Epoch 223/300, Valid Loss: 1.9654, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "--Start Epoch 224/300--\n",
      "Epoch 224/300, Train Loss: 0.3927, Acc: 0.7616, F1: 0.6870, AUC: 0.7965\n",
      "Epoch 224/300, Valid Loss: 2.7821, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 225/300--\n",
      "Epoch 225/300, Train Loss: 0.3700, Acc: 0.8023, F1: 0.7302, AUC: 0.8311\n",
      "Epoch 225/300, Valid Loss: 2.7815, Acc: 0.6047, F1: 0.3704, AUC: 0.5406\n",
      "--Start Epoch 226/300--\n",
      "Epoch 226/300, Train Loss: 0.4223, Acc: 0.7500, F1: 0.6906, AUC: 0.8053\n",
      "Epoch 226/300, Valid Loss: 3.6026, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "--Start Epoch 227/300--\n",
      "Epoch 227/300, Train Loss: 0.4962, Acc: 0.7267, F1: 0.6299, AUC: 0.7434\n",
      "Epoch 227/300, Valid Loss: 2.1565, Acc: 0.5814, F1: 0.3571, AUC: 0.5234\n",
      "--Start Epoch 228/300--\n",
      "Epoch 228/300, Train Loss: 0.3388, Acc: 0.8198, F1: 0.7395, AUC: 0.8322\n",
      "Epoch 228/300, Valid Loss: 2.6442, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "--Start Epoch 229/300--\n",
      "Epoch 229/300, Train Loss: 0.4284, Acc: 0.7384, F1: 0.6617, AUC: 0.7743\n",
      "Epoch 229/300, Valid Loss: 2.4314, Acc: 0.5349, F1: 0.5000, AUC: 0.5813\n",
      "--Start Epoch 230/300--\n",
      "Epoch 230/300, Train Loss: 0.4321, Acc: 0.7500, F1: 0.6815, AUC: 0.7940\n",
      "Epoch 230/300, Valid Loss: 2.6824, Acc: 0.6047, F1: 0.3704, AUC: 0.5406\n",
      "--Start Epoch 231/300--\n",
      "Epoch 231/300, Train Loss: 0.4309, Acc: 0.7791, F1: 0.6885, AUC: 0.7919\n",
      "Epoch 231/300, Valid Loss: 1.9883, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "--Start Epoch 232/300--\n",
      "Epoch 232/300, Train Loss: 0.3595, Acc: 0.7907, F1: 0.6897, AUC: 0.7889\n",
      "Epoch 232/300, Valid Loss: 2.5593, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 233/300--\n",
      "Epoch 233/300, Train Loss: 0.3885, Acc: 0.7674, F1: 0.6825, AUC: 0.7893\n",
      "Epoch 233/300, Valid Loss: 2.3441, Acc: 0.5116, F1: 0.4878, AUC: 0.5640\n",
      "--Start Epoch 234/300--\n",
      "Epoch 234/300, Train Loss: 0.4209, Acc: 0.7616, F1: 0.6772, AUC: 0.7852\n",
      "Epoch 234/300, Valid Loss: 2.5854, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 235/300--\n",
      "Epoch 235/300, Train Loss: 0.3530, Acc: 0.8023, F1: 0.7119, AUC: 0.8085\n",
      "Epoch 235/300, Valid Loss: 2.5094, Acc: 0.6279, F1: 0.5556, AUC: 0.6502\n",
      "--Start Epoch 236/300--\n",
      "Epoch 236/300, Train Loss: 0.4496, Acc: 0.7267, F1: 0.6569, AUC: 0.7718\n",
      "Epoch 236/300, Valid Loss: 2.8313, Acc: 0.6977, F1: 0.4800, AUC: 0.6281\n",
      "--Start Epoch 237/300--\n",
      "Epoch 237/300, Train Loss: 0.3381, Acc: 0.8372, F1: 0.7627, AUC: 0.8503\n",
      "Epoch 237/300, Valid Loss: 2.9339, Acc: 0.5814, F1: 0.4000, AUC: 0.5419\n",
      "--Start Epoch 238/300--\n",
      "Epoch 238/300, Train Loss: 0.4096, Acc: 0.7442, F1: 0.6667, AUC: 0.7785\n",
      "Epoch 238/300, Valid Loss: 3.0811, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 239/300--\n",
      "Epoch 239/300, Train Loss: 0.4111, Acc: 0.7326, F1: 0.6515, AUC: 0.7645\n",
      "Epoch 239/300, Valid Loss: 3.2656, Acc: 0.6744, F1: 0.4167, AUC: 0.5924\n",
      "--Start Epoch 240/300--\n",
      "Epoch 240/300, Train Loss: 0.3465, Acc: 0.8314, F1: 0.7642, AUC: 0.8575\n",
      "Epoch 240/300, Valid Loss: 3.0656, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "--Start Epoch 241/300--\n",
      "Epoch 241/300, Train Loss: 0.4161, Acc: 0.7500, F1: 0.6815, AUC: 0.7940\n",
      "Epoch 241/300, Valid Loss: 2.6465, Acc: 0.5581, F1: 0.4865, AUC: 0.5800\n",
      "--Start Epoch 242/300--\n",
      "Epoch 242/300, Train Loss: 0.4712, Acc: 0.7035, F1: 0.6383, AUC: 0.7552\n",
      "Epoch 242/300, Valid Loss: 2.2803, Acc: 0.6279, F1: 0.5556, AUC: 0.6502\n",
      "--Start Epoch 243/300--\n",
      "Epoch 243/300, Train Loss: 0.3840, Acc: 0.7965, F1: 0.7107, AUC: 0.8100\n",
      "Epoch 243/300, Valid Loss: 2.6124, Acc: 0.6977, F1: 0.4348, AUC: 0.6096\n",
      "--Start Epoch 244/300--\n",
      "Epoch 244/300, Train Loss: 0.3672, Acc: 0.7965, F1: 0.7154, AUC: 0.8157\n",
      "Epoch 244/300, Valid Loss: 2.5581, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "--Start Epoch 245/300--\n",
      "Epoch 245/300, Train Loss: 0.4654, Acc: 0.7209, F1: 0.6471, AUC: 0.7620\n",
      "Epoch 245/300, Valid Loss: 2.5856, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 246/300--\n",
      "Epoch 246/300, Train Loss: 0.3810, Acc: 0.7907, F1: 0.7097, AUC: 0.8115\n",
      "Epoch 246/300, Valid Loss: 2.4294, Acc: 0.3953, F1: 0.4348, AUC: 0.4778\n",
      "--Start Epoch 247/300--\n",
      "Epoch 247/300, Train Loss: 0.4827, Acc: 0.7209, F1: 0.6471, AUC: 0.7620\n",
      "Epoch 247/300, Valid Loss: 2.7402, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 248/300--\n",
      "Epoch 248/300, Train Loss: 0.3749, Acc: 0.7907, F1: 0.7049, AUC: 0.8059\n",
      "Epoch 248/300, Valid Loss: 2.5314, Acc: 0.5581, F1: 0.4242, AUC: 0.5431\n",
      "--Start Epoch 249/300--\n",
      "Epoch 249/300, Train Loss: 0.3963, Acc: 0.7558, F1: 0.6818, AUC: 0.7924\n",
      "Epoch 249/300, Valid Loss: 3.3265, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 250/300--\n",
      "Epoch 250/300, Train Loss: 0.3742, Acc: 0.8140, F1: 0.7500, AUC: 0.8508\n",
      "Epoch 250/300, Valid Loss: 3.0235, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 251/300--\n",
      "Epoch 251/300, Train Loss: 0.3161, Acc: 0.8547, F1: 0.7863, AUC: 0.8683\n",
      "Epoch 251/300, Valid Loss: 2.8256, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "--Start Epoch 252/300--\n",
      "Epoch 252/300, Train Loss: 0.4097, Acc: 0.7791, F1: 0.6984, AUC: 0.8033\n",
      "Epoch 252/300, Valid Loss: 2.6385, Acc: 0.5581, F1: 0.4242, AUC: 0.5431\n",
      "--Start Epoch 253/300--\n",
      "Epoch 253/300, Train Loss: 0.4043, Acc: 0.7558, F1: 0.6818, AUC: 0.7924\n",
      "Epoch 253/300, Valid Loss: 2.5170, Acc: 0.6047, F1: 0.5405, AUC: 0.6330\n",
      "--Start Epoch 254/300--\n",
      "Epoch 254/300, Train Loss: 0.3348, Acc: 0.8314, F1: 0.7603, AUC: 0.8518\n",
      "Epoch 254/300, Valid Loss: 3.0687, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 255/300--\n",
      "Epoch 255/300, Train Loss: 0.3425, Acc: 0.8140, F1: 0.7419, AUC: 0.8394\n",
      "Epoch 255/300, Valid Loss: 4.2045, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "--Start Epoch 256/300--\n",
      "Epoch 256/300, Train Loss: 0.4269, Acc: 0.7616, F1: 0.6822, AUC: 0.7909\n",
      "Epoch 256/300, Valid Loss: 2.3393, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "--Start Epoch 257/300--\n",
      "Epoch 257/300, Train Loss: 0.4210, Acc: 0.7558, F1: 0.6957, AUC: 0.8094\n",
      "Epoch 257/300, Valid Loss: 2.6291, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 258/300--\n",
      "Epoch 258/300, Train Loss: 0.3862, Acc: 0.7849, F1: 0.6891, AUC: 0.7904\n",
      "Epoch 258/300, Valid Loss: 2.4967, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 259/300--\n",
      "Epoch 259/300, Train Loss: 0.3758, Acc: 0.7733, F1: 0.6977, AUC: 0.8048\n",
      "Epoch 259/300, Valid Loss: 2.6606, Acc: 0.5581, F1: 0.3448, AUC: 0.5062\n",
      "--Start Epoch 260/300--\n",
      "Epoch 260/300, Train Loss: 0.3717, Acc: 0.7907, F1: 0.7188, AUC: 0.8229\n",
      "Epoch 260/300, Valid Loss: 3.1754, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 261/300--\n",
      "Epoch 261/300, Train Loss: 0.3883, Acc: 0.8023, F1: 0.7385, AUC: 0.8425\n",
      "Epoch 261/300, Valid Loss: 2.9919, Acc: 0.6744, F1: 0.4167, AUC: 0.5924\n",
      "--Start Epoch 262/300--\n",
      "Epoch 262/300, Train Loss: 0.3938, Acc: 0.7674, F1: 0.6970, AUC: 0.8064\n",
      "Epoch 262/300, Valid Loss: 3.1399, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "--Start Epoch 263/300--\n",
      "Epoch 263/300, Train Loss: 0.6409, Acc: 0.4477, F1: 0.5026, AUC: 0.5904\n",
      "Epoch 263/300, Valid Loss: 2.3275, Acc: 0.2791, F1: 0.4364, AUC: 0.4286\n",
      "--Start Epoch 264/300--\n",
      "Epoch 264/300, Train Loss: 0.6184, Acc: 0.5988, F1: 0.4103, AUC: 0.5617\n",
      "Epoch 264/300, Valid Loss: 3.1788, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 265/300--\n",
      "Epoch 265/300, Train Loss: 0.4320, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 265/300, Valid Loss: 2.5586, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "--Start Epoch 266/300--\n",
      "Epoch 266/300, Train Loss: 0.3841, Acc: 0.6860, F1: 0.2703, AUC: 0.5443\n",
      "Epoch 266/300, Valid Loss: 3.2355, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 267/300--\n",
      "Epoch 267/300, Train Loss: 0.4390, Acc: 0.7151, F1: 0.6475, AUC: 0.7635\n",
      "Epoch 267/300, Valid Loss: 3.5032, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "--Start Epoch 268/300--\n",
      "Epoch 268/300, Train Loss: 0.3544, Acc: 0.8081, F1: 0.7402, AUC: 0.8409\n",
      "Epoch 268/300, Valid Loss: 3.1617, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "--Start Epoch 269/300--\n",
      "Epoch 269/300, Train Loss: 0.3615, Acc: 0.8256, F1: 0.7656, AUC: 0.8647\n",
      "Epoch 269/300, Valid Loss: 3.5124, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "--Start Epoch 270/300--\n",
      "Epoch 270/300, Train Loss: 0.3830, Acc: 0.8140, F1: 0.7419, AUC: 0.8394\n",
      "Epoch 270/300, Valid Loss: 2.4664, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "--Start Epoch 271/300--\n",
      "Epoch 271/300, Train Loss: 0.3425, Acc: 0.8081, F1: 0.7317, AUC: 0.8296\n",
      "Epoch 271/300, Valid Loss: 2.9761, Acc: 0.7209, F1: 0.4000, AUC: 0.6084\n",
      "--Start Epoch 272/300--\n",
      "Epoch 272/300, Train Loss: 0.3815, Acc: 0.7907, F1: 0.7231, AUC: 0.8286\n",
      "Epoch 272/300, Valid Loss: 3.8900, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "--Start Epoch 273/300--\n",
      "Epoch 273/300, Train Loss: 0.4161, Acc: 0.7733, F1: 0.6880, AUC: 0.7935\n",
      "Epoch 273/300, Valid Loss: 2.3639, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "--Start Epoch 274/300--\n",
      "Epoch 274/300, Train Loss: 0.3758, Acc: 0.7616, F1: 0.6870, AUC: 0.7965\n",
      "Epoch 274/300, Valid Loss: 2.3432, Acc: 0.3023, F1: 0.4231, AUC: 0.4273\n",
      "--Start Epoch 275/300--\n",
      "Epoch 275/300, Train Loss: 0.4020, Acc: 0.7616, F1: 0.6822, AUC: 0.7909\n",
      "Epoch 275/300, Valid Loss: 2.6008, Acc: 0.5814, F1: 0.5000, AUC: 0.5973\n",
      "--Start Epoch 276/300--\n",
      "Epoch 276/300, Train Loss: 0.3475, Acc: 0.8081, F1: 0.7442, AUC: 0.8466\n",
      "Epoch 276/300, Valid Loss: 3.0224, Acc: 0.5814, F1: 0.4000, AUC: 0.5419\n",
      "--Start Epoch 277/300--\n",
      "Epoch 277/300, Train Loss: 0.3591, Acc: 0.7965, F1: 0.7328, AUC: 0.8384\n",
      "Epoch 277/300, Valid Loss: 4.0032, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 278/300--\n",
      "Epoch 278/300, Train Loss: 0.4855, Acc: 0.7151, F1: 0.6475, AUC: 0.7635\n",
      "Epoch 278/300, Valid Loss: 2.6404, Acc: 0.5581, F1: 0.4242, AUC: 0.5431\n",
      "--Start Epoch 279/300--\n",
      "Epoch 279/300, Train Loss: 0.3835, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
      "Epoch 279/300, Valid Loss: 2.6371, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 280/300--\n",
      "Epoch 280/300, Train Loss: 0.4169, Acc: 0.7442, F1: 0.6812, AUC: 0.7955\n",
      "Epoch 280/300, Valid Loss: 2.7561, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "--Start Epoch 281/300--\n",
      "Epoch 281/300, Train Loss: 0.3972, Acc: 0.7500, F1: 0.6906, AUC: 0.8053\n",
      "Epoch 281/300, Valid Loss: 3.4106, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "--Start Epoch 282/300--\n",
      "Epoch 282/300, Train Loss: 0.4253, Acc: 0.7442, F1: 0.6716, AUC: 0.7842\n",
      "Epoch 282/300, Valid Loss: 3.0518, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 283/300--\n",
      "Epoch 283/300, Train Loss: 0.3714, Acc: 0.7907, F1: 0.7188, AUC: 0.8229\n",
      "Epoch 283/300, Valid Loss: 4.1848, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "--Start Epoch 284/300--\n",
      "Epoch 284/300, Train Loss: 0.4088, Acc: 0.8023, F1: 0.7119, AUC: 0.8085\n",
      "Epoch 284/300, Valid Loss: 2.6326, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 285/300--\n",
      "Epoch 285/300, Train Loss: 0.3490, Acc: 0.8256, F1: 0.7581, AUC: 0.8533\n",
      "Epoch 285/300, Valid Loss: 3.8706, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "--Start Epoch 286/300--\n",
      "Epoch 286/300, Train Loss: 0.4386, Acc: 0.7558, F1: 0.6667, AUC: 0.7754\n",
      "Epoch 286/300, Valid Loss: 2.3679, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 287/300--\n",
      "Epoch 287/300, Train Loss: 0.3750, Acc: 0.7733, F1: 0.6929, AUC: 0.7991\n",
      "Epoch 287/300, Valid Loss: 2.3178, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "--Start Epoch 288/300--\n",
      "Epoch 288/300, Train Loss: 0.4098, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 288/300, Valid Loss: 2.5211, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "--Start Epoch 289/300--\n",
      "Epoch 289/300, Train Loss: 0.3568, Acc: 0.8023, F1: 0.7302, AUC: 0.8311\n",
      "Epoch 289/300, Valid Loss: 2.9983, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "--Start Epoch 290/300--\n",
      "Epoch 290/300, Train Loss: 0.4144, Acc: 0.7384, F1: 0.6715, AUC: 0.7857\n",
      "Epoch 290/300, Valid Loss: 3.1389, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "--Start Epoch 291/300--\n",
      "Epoch 291/300, Train Loss: 0.3849, Acc: 0.7791, F1: 0.6935, AUC: 0.7976\n",
      "Epoch 291/300, Valid Loss: 2.5800, Acc: 0.5814, F1: 0.4000, AUC: 0.5419\n",
      "--Start Epoch 292/300--\n",
      "Epoch 292/300, Train Loss: 0.3871, Acc: 0.7791, F1: 0.6984, AUC: 0.8033\n",
      "Epoch 292/300, Valid Loss: 2.8393, Acc: 0.6047, F1: 0.3200, AUC: 0.5222\n",
      "--Start Epoch 293/300--\n",
      "Epoch 293/300, Train Loss: 0.3343, Acc: 0.8140, F1: 0.7500, AUC: 0.8508\n",
      "Epoch 293/300, Valid Loss: 4.1110, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "--Start Epoch 294/300--\n",
      "Epoch 294/300, Train Loss: 0.3433, Acc: 0.8256, F1: 0.7541, AUC: 0.8477\n",
      "Epoch 294/300, Valid Loss: 3.0463, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "--Start Epoch 295/300--\n",
      "Epoch 295/300, Train Loss: 0.3537, Acc: 0.8023, F1: 0.7344, AUC: 0.8368\n",
      "Epoch 295/300, Valid Loss: 3.4203, Acc: 0.6279, F1: 0.3333, AUC: 0.5394\n",
      "--Start Epoch 296/300--\n",
      "Epoch 296/300, Train Loss: 0.4186, Acc: 0.7267, F1: 0.6569, AUC: 0.7718\n",
      "Epoch 296/300, Valid Loss: 3.0975, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "--Start Epoch 297/300--\n",
      "Epoch 297/300, Train Loss: 0.3816, Acc: 0.7733, F1: 0.7068, AUC: 0.8162\n",
      "Epoch 297/300, Valid Loss: 3.0635, Acc: 0.6047, F1: 0.4138, AUC: 0.5591\n",
      "--Start Epoch 298/300--\n",
      "Epoch 298/300, Train Loss: 0.4125, Acc: 0.7558, F1: 0.6818, AUC: 0.7924\n",
      "Epoch 298/300, Valid Loss: 2.9941, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 299/300--\n",
      "Epoch 299/300, Train Loss: 0.3375, Acc: 0.8256, F1: 0.7581, AUC: 0.8533\n",
      "Epoch 299/300, Valid Loss: 3.3670, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "--Start Epoch 300/300--\n",
      "Epoch 300/300, Train Loss: 0.3664, Acc: 0.7849, F1: 0.7218, AUC: 0.8301\n",
      "Epoch 300/300, Valid Loss: 3.7700, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLPModel(input_dim=768, hidden_dims=[512, 256, 128], output_dim=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss combines Sigmoid and BCELoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'--Start Epoch {epoch+1}/{num_epochs}--')\n",
    "    train_loss, train_acc, train_f1, train_auc = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1, val_auc = validate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Valid Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.train_utils import plot_cross_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Fold 1/5 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 0.7374, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 1/300, Valid Loss: 0.6583, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Train Loss: 0.6591, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Valid Loss: 0.6411, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Train Loss: 0.6304, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6401, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Train Loss: 0.6295, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Valid Loss: 0.6637, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Train Loss: 0.6205, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.6373, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Train Loss: 0.6147, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.6465, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Train Loss: 0.6299, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.6428, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Train Loss: 0.6236, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.6420, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Train Loss: 0.6163, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.6516, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Train Loss: 0.6234, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.6677, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Train Loss: 0.6114, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.6671, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Train Loss: 0.6149, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.6790, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Train Loss: 0.6193, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.6856, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Train Loss: 0.6240, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.6750, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Train Loss: 0.6061, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.6865, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Train Loss: 0.6063, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.6696, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Train Loss: 0.6111, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.6863, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Train Loss: 0.6028, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Valid Loss: 0.7406, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Train Loss: 0.6084, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Valid Loss: 0.6911, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Train Loss: 0.5989, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Valid Loss: 0.7159, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Train Loss: 0.5985, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Valid Loss: 0.7308, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Train Loss: 0.6106, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Valid Loss: 0.7255, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Train Loss: 0.5892, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Valid Loss: 0.7041, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Train Loss: 0.6174, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Valid Loss: 0.6768, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Train Loss: 0.5988, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Valid Loss: 0.7214, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Train Loss: 0.6136, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Valid Loss: 0.6816, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Train Loss: 0.5856, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Valid Loss: 0.7052, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Train Loss: 0.5804, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Valid Loss: 0.7199, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Train Loss: 0.5901, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Valid Loss: 0.7034, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Train Loss: 0.5872, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Valid Loss: 0.7499, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Train Loss: 0.5938, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Valid Loss: 0.7431, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Train Loss: 0.6027, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Valid Loss: 0.7161, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Train Loss: 0.6101, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Valid Loss: 0.7488, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Train Loss: 0.5978, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Valid Loss: 0.7531, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Train Loss: 0.6215, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Valid Loss: 0.6958, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Train Loss: 0.5998, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Valid Loss: 0.6729, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Train Loss: 0.5895, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Valid Loss: 0.6837, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Train Loss: 0.5940, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Valid Loss: 0.7884, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Train Loss: 0.5885, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Valid Loss: 0.6873, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Train Loss: 0.5828, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Valid Loss: 0.7007, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Train Loss: 0.5800, Acc: 0.7093, F1: 0.1071, AUC: 0.5211\n",
      "Epoch 41/300, Valid Loss: 0.8023, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Train Loss: 0.6135, Acc: 0.7093, F1: 0.0741, AUC: 0.5155\n",
      "Epoch 42/300, Valid Loss: 0.6752, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 43/300, Train Loss: 0.5836, Acc: 0.7151, F1: 0.0755, AUC: 0.5196\n",
      "Epoch 43/300, Valid Loss: 0.7544, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Train Loss: 0.5781, Acc: 0.7093, F1: 0.0741, AUC: 0.5155\n",
      "Epoch 44/300, Valid Loss: 0.8342, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Train Loss: 0.5685, Acc: 0.7151, F1: 0.0755, AUC: 0.5196\n",
      "Epoch 45/300, Valid Loss: 0.8202, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Train Loss: 0.5722, Acc: 0.7326, F1: 0.1786, AUC: 0.5490\n",
      "Epoch 46/300, Valid Loss: 0.7443, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 47/300, Train Loss: 0.5848, Acc: 0.7035, F1: 0.1356, AUC: 0.5227\n",
      "Epoch 47/300, Valid Loss: 0.7409, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 48/300, Train Loss: 0.5678, Acc: 0.6977, F1: 0.0370, AUC: 0.5015\n",
      "Epoch 48/300, Valid Loss: 0.7657, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Train Loss: 0.5742, Acc: 0.7093, F1: 0.0385, AUC: 0.5098\n",
      "Epoch 49/300, Valid Loss: 0.8088, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Train Loss: 0.5917, Acc: 0.6860, F1: 0.1000, AUC: 0.5046\n",
      "Epoch 50/300, Valid Loss: 0.7812, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Train Loss: 0.5968, Acc: 0.6977, F1: 0.1034, AUC: 0.5129\n",
      "Epoch 51/300, Valid Loss: 0.8437, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Train Loss: 0.5668, Acc: 0.7093, F1: 0.0741, AUC: 0.5155\n",
      "Epoch 52/300, Valid Loss: 0.8083, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "Epoch 53/300, Train Loss: 0.5909, Acc: 0.6919, F1: 0.1017, AUC: 0.5088\n",
      "Epoch 53/300, Valid Loss: 0.8136, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 54/300, Train Loss: 0.5657, Acc: 0.7267, F1: 0.1455, AUC: 0.5392\n",
      "Epoch 54/300, Valid Loss: 0.9007, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 55/300, Train Loss: 0.5877, Acc: 0.6919, F1: 0.0702, AUC: 0.5031\n",
      "Epoch 55/300, Valid Loss: 0.8391, Acc: 0.7209, F1: 0.2500, AUC: 0.5714\n",
      "Epoch 56/300, Train Loss: 0.5736, Acc: 0.7035, F1: 0.1905, AUC: 0.5340\n",
      "Epoch 56/300, Valid Loss: 0.8928, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Train Loss: 0.5678, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Valid Loss: 0.9385, Acc: 0.6279, F1: 0.2727, AUC: 0.5209\n",
      "Epoch 58/300, Train Loss: 0.5797, Acc: 0.7151, F1: 0.2462, AUC: 0.5536\n",
      "Epoch 58/300, Valid Loss: 0.9560, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 59/300, Train Loss: 0.5969, Acc: 0.7035, F1: 0.1053, AUC: 0.5170\n",
      "Epoch 59/300, Valid Loss: 0.8664, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 60/300, Train Loss: 0.5692, Acc: 0.7267, F1: 0.2540, AUC: 0.5619\n",
      "Epoch 60/300, Valid Loss: 0.8574, Acc: 0.6512, F1: 0.4000, AUC: 0.5751\n",
      "Epoch 61/300, Train Loss: 0.5886, Acc: 0.6860, F1: 0.2059, AUC: 0.5273\n",
      "Epoch 61/300, Valid Loss: 0.8233, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 62/300, Train Loss: 0.5846, Acc: 0.6977, F1: 0.1333, AUC: 0.5186\n",
      "Epoch 62/300, Valid Loss: 0.8529, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 63/300, Train Loss: 0.5415, Acc: 0.7093, F1: 0.1071, AUC: 0.5211\n",
      "Epoch 63/300, Valid Loss: 0.9609, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 64/300, Train Loss: 0.5303, Acc: 0.7326, F1: 0.2333, AUC: 0.5604\n",
      "Epoch 64/300, Valid Loss: 1.0111, Acc: 0.6977, F1: 0.4348, AUC: 0.6096\n",
      "Epoch 65/300, Train Loss: 0.5638, Acc: 0.7093, F1: 0.2857, AUC: 0.5608\n",
      "Epoch 65/300, Valid Loss: 1.0789, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 66/300, Train Loss: 0.5528, Acc: 0.6919, F1: 0.1846, AUC: 0.5258\n",
      "Epoch 66/300, Valid Loss: 1.0838, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 67/300, Train Loss: 0.5476, Acc: 0.7267, F1: 0.3562, AUC: 0.5903\n",
      "Epoch 67/300, Valid Loss: 1.2006, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "Epoch 68/300, Train Loss: 0.5542, Acc: 0.7267, F1: 0.2985, AUC: 0.5732\n",
      "Epoch 68/300, Valid Loss: 1.1185, Acc: 0.4884, F1: 0.3529, AUC: 0.4729\n",
      "Epoch 69/300, Train Loss: 0.5522, Acc: 0.6919, F1: 0.2090, AUC: 0.5314\n",
      "Epoch 69/300, Valid Loss: 1.1076, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 70/300, Train Loss: 0.5717, Acc: 0.7035, F1: 0.2609, AUC: 0.5510\n",
      "Epoch 70/300, Valid Loss: 1.0074, Acc: 0.6512, F1: 0.1176, AUC: 0.5012\n",
      "Epoch 71/300, Train Loss: 0.5571, Acc: 0.7209, F1: 0.4286, AUC: 0.6145\n",
      "Epoch 71/300, Valid Loss: 0.9925, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Train Loss: 0.5356, Acc: 0.7209, F1: 0.3333, AUC: 0.5805\n",
      "Epoch 72/300, Valid Loss: 1.0036, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 73/300, Train Loss: 0.5062, Acc: 0.7733, F1: 0.4658, AUC: 0.6460\n",
      "Epoch 73/300, Valid Loss: 1.0722, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 74/300, Train Loss: 0.5451, Acc: 0.7384, F1: 0.4304, AUC: 0.6212\n",
      "Epoch 74/300, Valid Loss: 1.2344, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 75/300, Train Loss: 0.5061, Acc: 0.7674, F1: 0.4737, AUC: 0.6475\n",
      "Epoch 75/300, Valid Loss: 1.2408, Acc: 0.6744, F1: 0.4167, AUC: 0.5924\n",
      "Epoch 76/300, Train Loss: 0.5261, Acc: 0.7093, F1: 0.3590, AUC: 0.5835\n",
      "Epoch 76/300, Valid Loss: 1.1846, Acc: 0.6744, F1: 0.5000, AUC: 0.6293\n",
      "Epoch 77/300, Train Loss: 0.5077, Acc: 0.7384, F1: 0.4944, AUC: 0.6496\n",
      "Epoch 77/300, Valid Loss: 1.3170, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 78/300, Train Loss: 0.6034, Acc: 0.6570, F1: 0.4957, AUC: 0.6314\n",
      "Epoch 78/300, Valid Loss: 1.5016, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Train Loss: 0.5985, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Valid Loss: 0.9802, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Train Loss: 0.5077, Acc: 0.7151, F1: 0.2222, AUC: 0.5480\n",
      "Epoch 80/300, Valid Loss: 1.1158, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 81/300, Train Loss: 0.4969, Acc: 0.7558, F1: 0.4878, AUC: 0.6506\n",
      "Epoch 81/300, Valid Loss: 1.1284, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 82/300, Train Loss: 0.5207, Acc: 0.7384, F1: 0.5055, AUC: 0.6552\n",
      "Epoch 82/300, Valid Loss: 1.1447, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "Epoch 83/300, Train Loss: 0.5557, Acc: 0.6860, F1: 0.4000, AUC: 0.5897\n",
      "Epoch 83/300, Valid Loss: 1.3627, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 84/300, Train Loss: 0.5295, Acc: 0.6977, F1: 0.3810, AUC: 0.5866\n",
      "Epoch 84/300, Valid Loss: 1.4883, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Train Loss: 0.5259, Acc: 0.7267, F1: 0.3562, AUC: 0.5903\n",
      "Epoch 85/300, Valid Loss: 1.2499, Acc: 0.6047, F1: 0.4138, AUC: 0.5591\n",
      "Epoch 86/300, Train Loss: 0.5259, Acc: 0.7326, F1: 0.5490, AUC: 0.6795\n",
      "Epoch 86/300, Valid Loss: 1.1646, Acc: 0.7209, F1: 0.2500, AUC: 0.5714\n",
      "Epoch 87/300, Train Loss: 0.4979, Acc: 0.7558, F1: 0.4750, AUC: 0.6450\n",
      "Epoch 87/300, Valid Loss: 1.2763, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 88/300, Train Loss: 0.4898, Acc: 0.7326, F1: 0.4889, AUC: 0.6454\n",
      "Epoch 88/300, Valid Loss: 1.3584, Acc: 0.6047, F1: 0.4138, AUC: 0.5591\n",
      "Epoch 89/300, Train Loss: 0.5173, Acc: 0.7442, F1: 0.5319, AUC: 0.6707\n",
      "Epoch 89/300, Valid Loss: 1.4204, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 90/300, Train Loss: 0.5057, Acc: 0.7733, F1: 0.5714, AUC: 0.6971\n",
      "Epoch 90/300, Valid Loss: 1.2684, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 91/300, Train Loss: 0.5203, Acc: 0.7267, F1: 0.4598, AUC: 0.6300\n",
      "Epoch 91/300, Valid Loss: 1.2006, Acc: 0.6512, F1: 0.2105, AUC: 0.5197\n",
      "Epoch 92/300, Train Loss: 0.4843, Acc: 0.7326, F1: 0.5741, AUC: 0.6965\n",
      "Epoch 92/300, Valid Loss: 1.7940, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 93/300, Train Loss: 0.5167, Acc: 0.7093, F1: 0.5192, AUC: 0.6573\n",
      "Epoch 93/300, Valid Loss: 1.5928, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 94/300, Train Loss: 0.5309, Acc: 0.6744, F1: 0.5172, AUC: 0.6495\n",
      "Epoch 94/300, Valid Loss: 1.5286, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 95/300, Train Loss: 0.4879, Acc: 0.7442, F1: 0.4884, AUC: 0.6480\n",
      "Epoch 95/300, Valid Loss: 2.0199, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Train Loss: 0.5093, Acc: 0.7500, F1: 0.4941, AUC: 0.6522\n",
      "Epoch 96/300, Valid Loss: 1.4982, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 97/300, Train Loss: 0.5039, Acc: 0.7093, F1: 0.4792, AUC: 0.6346\n",
      "Epoch 97/300, Valid Loss: 1.8866, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 98/300, Train Loss: 0.4730, Acc: 0.7616, F1: 0.5941, AUC: 0.7115\n",
      "Epoch 98/300, Valid Loss: 2.1369, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 99/300, Train Loss: 0.5567, Acc: 0.6860, F1: 0.5091, AUC: 0.6464\n",
      "Epoch 99/300, Valid Loss: 1.5282, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "Epoch 100/300, Train Loss: 0.4871, Acc: 0.7500, F1: 0.5474, AUC: 0.6805\n",
      "Epoch 100/300, Valid Loss: 1.7930, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 101/300, Train Loss: 0.4585, Acc: 0.7791, F1: 0.6042, AUC: 0.7182\n",
      "Epoch 101/300, Valid Loss: 1.5162, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "Epoch 102/300, Train Loss: 0.5015, Acc: 0.7151, F1: 0.5243, AUC: 0.6614\n",
      "Epoch 102/300, Valid Loss: 1.6291, Acc: 0.6512, F1: 0.4444, AUC: 0.5936\n",
      "Epoch 103/300, Train Loss: 0.4738, Acc: 0.7558, F1: 0.5532, AUC: 0.6847\n",
      "Epoch 103/300, Valid Loss: 1.7040, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "Epoch 104/300, Train Loss: 0.4771, Acc: 0.7616, F1: 0.5773, AUC: 0.7001\n",
      "Epoch 104/300, Valid Loss: 1.7948, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "Epoch 105/300, Train Loss: 0.4314, Acc: 0.7733, F1: 0.6355, AUC: 0.7424\n",
      "Epoch 105/300, Valid Loss: 2.2905, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 106/300, Train Loss: 0.4840, Acc: 0.7558, F1: 0.5227, AUC: 0.6676\n",
      "Epoch 106/300, Valid Loss: 1.9006, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 107/300, Train Loss: 0.4425, Acc: 0.7849, F1: 0.6186, AUC: 0.7280\n",
      "Epoch 107/300, Valid Loss: 1.8796, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 108/300, Train Loss: 0.5098, Acc: 0.7267, F1: 0.5053, AUC: 0.6526\n",
      "Epoch 108/300, Valid Loss: 1.5787, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 109/300, Train Loss: 0.5095, Acc: 0.7209, F1: 0.4000, AUC: 0.6031\n",
      "Epoch 109/300, Valid Loss: 1.7386, Acc: 0.7209, F1: 0.2500, AUC: 0.5714\n",
      "Epoch 110/300, Train Loss: 0.4467, Acc: 0.7674, F1: 0.6078, AUC: 0.7213\n",
      "Epoch 110/300, Valid Loss: 2.0935, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 111/300, Train Loss: 0.4994, Acc: 0.7093, F1: 0.5968, AUC: 0.7140\n",
      "Epoch 111/300, Valid Loss: 2.4646, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 112/300, Train Loss: 0.4791, Acc: 0.7674, F1: 0.5745, AUC: 0.6986\n",
      "Epoch 112/300, Valid Loss: 2.0203, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 113/300, Train Loss: 0.4279, Acc: 0.8198, F1: 0.6990, AUC: 0.7868\n",
      "Epoch 113/300, Valid Loss: 1.8500, Acc: 0.6977, F1: 0.4348, AUC: 0.6096\n",
      "Epoch 114/300, Train Loss: 0.4704, Acc: 0.7558, F1: 0.5625, AUC: 0.6903\n",
      "Epoch 114/300, Valid Loss: 1.7286, Acc: 0.6279, F1: 0.2727, AUC: 0.5209\n",
      "Epoch 115/300, Train Loss: 0.4501, Acc: 0.7791, F1: 0.6545, AUC: 0.7579\n",
      "Epoch 115/300, Valid Loss: 1.9357, Acc: 0.6279, F1: 0.2727, AUC: 0.5209\n",
      "Epoch 116/300, Train Loss: 0.4345, Acc: 0.7907, F1: 0.6786, AUC: 0.7775\n",
      "Epoch 116/300, Valid Loss: 2.1847, Acc: 0.6512, F1: 0.2105, AUC: 0.5197\n",
      "Epoch 117/300, Train Loss: 0.4681, Acc: 0.7558, F1: 0.6379, AUC: 0.7470\n",
      "Epoch 117/300, Valid Loss: 2.2685, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 118/300, Train Loss: 0.4220, Acc: 0.7791, F1: 0.6667, AUC: 0.7692\n",
      "Epoch 118/300, Valid Loss: 2.6412, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 119/300, Train Loss: 0.5404, Acc: 0.6512, F1: 0.5312, AUC: 0.6556\n",
      "Epoch 119/300, Valid Loss: 2.1434, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 120/300, Train Loss: 0.4406, Acc: 0.7674, F1: 0.6429, AUC: 0.7496\n",
      "Epoch 120/300, Valid Loss: 2.0734, Acc: 0.5116, F1: 0.4000, AUC: 0.5086\n",
      "Epoch 121/300, Train Loss: 0.4451, Acc: 0.7616, F1: 0.6496, AUC: 0.7568\n",
      "Epoch 121/300, Valid Loss: 2.3001, Acc: 0.6512, F1: 0.2105, AUC: 0.5197\n",
      "Epoch 122/300, Train Loss: 0.4450, Acc: 0.7442, F1: 0.6333, AUC: 0.7444\n",
      "Epoch 122/300, Valid Loss: 2.7589, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 123/300, Train Loss: 0.4312, Acc: 0.7965, F1: 0.6903, AUC: 0.7873\n",
      "Epoch 123/300, Valid Loss: 3.1689, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 124/300, Train Loss: 0.4626, Acc: 0.7733, F1: 0.6422, AUC: 0.7481\n",
      "Epoch 124/300, Valid Loss: 2.0683, Acc: 0.6744, F1: 0.2222, AUC: 0.5369\n",
      "Epoch 125/300, Train Loss: 0.4469, Acc: 0.7558, F1: 0.6441, AUC: 0.7527\n",
      "Epoch 125/300, Valid Loss: 2.2872, Acc: 0.5349, F1: 0.3750, AUC: 0.5074\n",
      "Epoch 126/300, Train Loss: 0.4606, Acc: 0.7616, F1: 0.6612, AUC: 0.7682\n",
      "Epoch 126/300, Valid Loss: 2.2162, Acc: 0.5349, F1: 0.3750, AUC: 0.5074\n",
      "Epoch 127/300, Train Loss: 0.4938, Acc: 0.6919, F1: 0.6015, AUC: 0.7186\n",
      "Epoch 127/300, Valid Loss: 2.3626, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 128/300, Train Loss: 0.4557, Acc: 0.7384, F1: 0.5794, AUC: 0.7006\n",
      "Epoch 128/300, Valid Loss: 2.2822, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 129/300, Train Loss: 0.4194, Acc: 0.7791, F1: 0.6607, AUC: 0.7636\n",
      "Epoch 129/300, Valid Loss: 2.7461, Acc: 0.7209, F1: 0.2500, AUC: 0.5714\n",
      "Epoch 130/300, Train Loss: 0.4192, Acc: 0.7965, F1: 0.6903, AUC: 0.7873\n",
      "Epoch 130/300, Valid Loss: 2.4862, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 131/300, Train Loss: 0.4196, Acc: 0.7791, F1: 0.6545, AUC: 0.7579\n",
      "Epoch 131/300, Valid Loss: 2.7704, Acc: 0.6279, F1: 0.2000, AUC: 0.5025\n",
      "Epoch 132/300, Train Loss: 0.4756, Acc: 0.7500, F1: 0.6387, AUC: 0.7486\n",
      "Epoch 132/300, Valid Loss: 2.1719, Acc: 0.6047, F1: 0.2609, AUC: 0.5037\n",
      "Epoch 133/300, Train Loss: 0.4530, Acc: 0.7442, F1: 0.6140, AUC: 0.7274\n",
      "Epoch 133/300, Valid Loss: 2.1625, Acc: 0.5814, F1: 0.3077, AUC: 0.5049\n",
      "Epoch 134/300, Train Loss: 0.4086, Acc: 0.7849, F1: 0.6726, AUC: 0.7734\n",
      "Epoch 134/300, Valid Loss: 2.4772, Acc: 0.5581, F1: 0.4571, AUC: 0.5616\n",
      "Epoch 135/300, Train Loss: 0.4719, Acc: 0.6860, F1: 0.5781, AUC: 0.6975\n",
      "Epoch 135/300, Valid Loss: 2.8302, Acc: 0.6279, F1: 0.2727, AUC: 0.5209\n",
      "Epoch 136/300, Train Loss: 0.4602, Acc: 0.7674, F1: 0.6296, AUC: 0.7383\n",
      "Epoch 136/300, Valid Loss: 2.5353, Acc: 0.6512, F1: 0.3478, AUC: 0.5567\n",
      "Epoch 137/300, Train Loss: 0.4121, Acc: 0.7907, F1: 0.6897, AUC: 0.7889\n",
      "Epoch 137/300, Valid Loss: 2.8990, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 138/300, Train Loss: 0.4827, Acc: 0.7442, F1: 0.6271, AUC: 0.7388\n",
      "Epoch 138/300, Valid Loss: 2.3294, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "Epoch 139/300, Train Loss: 0.4969, Acc: 0.8023, F1: 0.6792, AUC: 0.7744\n",
      "Epoch 139/300, Valid Loss: 2.0620, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 140/300, Train Loss: 0.4264, Acc: 0.7442, F1: 0.6207, AUC: 0.7331\n",
      "Epoch 140/300, Valid Loss: 2.2552, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 141/300, Train Loss: 0.4145, Acc: 0.7965, F1: 0.6667, AUC: 0.7646\n",
      "Epoch 141/300, Valid Loss: 2.4178, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 142/300, Train Loss: 0.4252, Acc: 0.7558, F1: 0.6667, AUC: 0.7754\n",
      "Epoch 142/300, Valid Loss: 2.6876, Acc: 0.5349, F1: 0.3333, AUC: 0.4889\n",
      "Epoch 143/300, Train Loss: 0.4143, Acc: 0.7965, F1: 0.7107, AUC: 0.8100\n",
      "Epoch 143/300, Valid Loss: 2.7564, Acc: 0.6512, F1: 0.2857, AUC: 0.5382\n",
      "Epoch 144/300, Train Loss: 0.4723, Acc: 0.7616, F1: 0.6667, AUC: 0.7739\n",
      "Epoch 144/300, Valid Loss: 2.1835, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "Epoch 145/300, Train Loss: 0.4588, Acc: 0.7558, F1: 0.6182, AUC: 0.7300\n",
      "Epoch 145/300, Valid Loss: 2.0039, Acc: 0.6279, F1: 0.5000, AUC: 0.6133\n",
      "Epoch 146/300, Train Loss: 0.4329, Acc: 0.7558, F1: 0.6441, AUC: 0.7527\n",
      "Epoch 146/300, Valid Loss: 2.2522, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 147/300, Train Loss: 0.4400, Acc: 0.7791, F1: 0.6833, AUC: 0.7863\n",
      "Epoch 147/300, Valid Loss: 2.2374, Acc: 0.6512, F1: 0.3478, AUC: 0.5567\n",
      "Epoch 148/300, Train Loss: 0.4078, Acc: 0.7674, F1: 0.6667, AUC: 0.7723\n",
      "Epoch 148/300, Valid Loss: 2.4154, Acc: 0.5349, F1: 0.5000, AUC: 0.5813\n",
      "Epoch 149/300, Train Loss: 0.4126, Acc: 0.7965, F1: 0.6847, AUC: 0.7816\n",
      "Epoch 149/300, Valid Loss: 3.0367, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 150/300, Train Loss: 0.4082, Acc: 0.8023, F1: 0.7069, AUC: 0.8028\n",
      "Epoch 150/300, Valid Loss: 3.0636, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 151/300, Train Loss: 0.4875, Acc: 0.7442, F1: 0.6207, AUC: 0.7331\n",
      "Epoch 151/300, Valid Loss: 1.9785, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 152/300, Train Loss: 0.4112, Acc: 0.7616, F1: 0.6372, AUC: 0.7455\n",
      "Epoch 152/300, Valid Loss: 2.3417, Acc: 0.5349, F1: 0.4118, AUC: 0.5259\n",
      "Epoch 153/300, Train Loss: 0.4125, Acc: 0.7733, F1: 0.6929, AUC: 0.7991\n",
      "Epoch 153/300, Valid Loss: 2.9747, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 154/300, Train Loss: 0.4295, Acc: 0.7849, F1: 0.6891, AUC: 0.7904\n",
      "Epoch 154/300, Valid Loss: 2.5953, Acc: 0.5814, F1: 0.3077, AUC: 0.5049\n",
      "Epoch 155/300, Train Loss: 0.4211, Acc: 0.8256, F1: 0.7222, AUC: 0.8080\n",
      "Epoch 155/300, Valid Loss: 2.2304, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "Epoch 156/300, Train Loss: 0.5009, Acc: 0.6570, F1: 0.5755, AUC: 0.6938\n",
      "Epoch 156/300, Valid Loss: 2.1062, Acc: 0.2558, F1: 0.3333, AUC: 0.3374\n",
      "Epoch 157/300, Train Loss: 0.4658, Acc: 0.7442, F1: 0.6071, AUC: 0.7218\n",
      "Epoch 157/300, Valid Loss: 2.2830, Acc: 0.5349, F1: 0.3333, AUC: 0.4889\n",
      "Epoch 158/300, Train Loss: 0.3755, Acc: 0.8023, F1: 0.6909, AUC: 0.7858\n",
      "Epoch 158/300, Valid Loss: 2.9778, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 159/300, Train Loss: 0.3972, Acc: 0.7965, F1: 0.6847, AUC: 0.7816\n",
      "Epoch 159/300, Valid Loss: 2.5861, Acc: 0.5814, F1: 0.4706, AUC: 0.5788\n",
      "Epoch 160/300, Train Loss: 0.4278, Acc: 0.7558, F1: 0.6667, AUC: 0.7754\n",
      "Epoch 160/300, Valid Loss: 2.8579, Acc: 0.6512, F1: 0.5161, AUC: 0.6305\n",
      "Epoch 161/300, Train Loss: 0.4361, Acc: 0.7384, F1: 0.6457, AUC: 0.7573\n",
      "Epoch 161/300, Valid Loss: 2.8881, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 162/300, Train Loss: 0.4157, Acc: 0.7442, F1: 0.6393, AUC: 0.7501\n",
      "Epoch 162/300, Valid Loss: 2.9912, Acc: 0.2558, F1: 0.3846, AUC: 0.3744\n",
      "Epoch 163/300, Train Loss: 0.4688, Acc: 0.6919, F1: 0.5954, AUC: 0.7129\n",
      "Epoch 163/300, Valid Loss: 3.4006, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 164/300, Train Loss: 0.4363, Acc: 0.7500, F1: 0.6325, AUC: 0.7429\n",
      "Epoch 164/300, Valid Loss: 2.9883, Acc: 0.7209, F1: 0.4000, AUC: 0.6084\n",
      "Epoch 165/300, Train Loss: 0.4312, Acc: 0.7384, F1: 0.6154, AUC: 0.7290\n",
      "Epoch 165/300, Valid Loss: 3.2925, Acc: 0.7209, F1: 0.4545, AUC: 0.6268\n",
      "Epoch 166/300, Train Loss: 0.4107, Acc: 0.7965, F1: 0.6957, AUC: 0.7930\n",
      "Epoch 166/300, Valid Loss: 3.2848, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 167/300, Train Loss: 0.4180, Acc: 0.7674, F1: 0.6667, AUC: 0.7723\n",
      "Epoch 167/300, Valid Loss: 3.3630, Acc: 0.5581, F1: 0.3871, AUC: 0.5246\n",
      "Epoch 168/300, Train Loss: 0.4126, Acc: 0.8256, F1: 0.7458, AUC: 0.8363\n",
      "Epoch 168/300, Valid Loss: 3.6141, Acc: 0.6744, F1: 0.1250, AUC: 0.5185\n",
      "Epoch 169/300, Train Loss: 0.3858, Acc: 0.8372, F1: 0.7500, AUC: 0.8333\n",
      "Epoch 169/300, Valid Loss: 4.2497, Acc: 0.6977, F1: 0.1333, AUC: 0.5357\n",
      "Epoch 170/300, Train Loss: 0.4800, Acc: 0.7209, F1: 0.6250, AUC: 0.7393\n",
      "Epoch 170/300, Valid Loss: 2.6376, Acc: 0.6512, F1: 0.4828, AUC: 0.6121\n",
      "Epoch 171/300, Train Loss: 0.3670, Acc: 0.8198, F1: 0.7350, AUC: 0.8265\n",
      "Epoch 171/300, Valid Loss: 3.0217, Acc: 0.6047, F1: 0.4516, AUC: 0.5776\n",
      "Epoch 172/300, Train Loss: 0.3552, Acc: 0.8372, F1: 0.7627, AUC: 0.8503\n",
      "Epoch 172/300, Valid Loss: 3.6338, Acc: 0.6279, F1: 0.3846, AUC: 0.5579\n",
      "Epoch 173/300, Train Loss: 0.4479, Acc: 0.7558, F1: 0.6818, AUC: 0.7924\n",
      "Epoch 173/300, Valid Loss: 3.2575, Acc: 0.7674, F1: 0.5000, AUC: 0.6613\n",
      "Epoch 174/300, Train Loss: 0.3813, Acc: 0.8140, F1: 0.7241, AUC: 0.8167\n",
      "Epoch 174/300, Valid Loss: 3.1551, Acc: 0.6047, F1: 0.4138, AUC: 0.5591\n",
      "Epoch 175/300, Train Loss: 0.4329, Acc: 0.7384, F1: 0.6667, AUC: 0.7800\n",
      "Epoch 175/300, Valid Loss: 3.1819, Acc: 0.4651, F1: 0.3784, AUC: 0.4741\n",
      "Epoch 176/300, Train Loss: 0.4080, Acc: 0.8140, F1: 0.7333, AUC: 0.8281\n",
      "Epoch 176/300, Valid Loss: 3.1520, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 177/300, Train Loss: 0.3787, Acc: 0.7791, F1: 0.7031, AUC: 0.8089\n",
      "Epoch 177/300, Valid Loss: 3.5621, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 178/300, Train Loss: 0.4528, Acc: 0.7733, F1: 0.6667, AUC: 0.7708\n",
      "Epoch 178/300, Valid Loss: 2.7177, Acc: 0.5814, F1: 0.3571, AUC: 0.5234\n",
      "Epoch 179/300, Train Loss: 0.3719, Acc: 0.8140, F1: 0.7193, AUC: 0.8111\n",
      "Epoch 179/300, Valid Loss: 2.8114, Acc: 0.5349, F1: 0.3750, AUC: 0.5074\n",
      "Epoch 180/300, Train Loss: 0.3585, Acc: 0.8256, F1: 0.7500, AUC: 0.8420\n",
      "Epoch 180/300, Valid Loss: 3.1431, Acc: 0.5349, F1: 0.3750, AUC: 0.5074\n",
      "Epoch 181/300, Train Loss: 0.3430, Acc: 0.8081, F1: 0.7317, AUC: 0.8296\n",
      "Epoch 181/300, Valid Loss: 3.7678, Acc: 0.6047, F1: 0.3200, AUC: 0.5222\n",
      "Epoch 182/300, Train Loss: 0.4584, Acc: 0.7733, F1: 0.6777, AUC: 0.7821\n",
      "Epoch 182/300, Valid Loss: 2.5419, Acc: 0.6744, F1: 0.4615, AUC: 0.6108\n",
      "Epoch 183/300, Train Loss: 0.3799, Acc: 0.8081, F1: 0.7360, AUC: 0.8353\n",
      "Epoch 183/300, Valid Loss: 2.9514, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "Epoch 184/300, Train Loss: 0.4079, Acc: 0.7965, F1: 0.6789, AUC: 0.7760\n",
      "Epoch 184/300, Valid Loss: 2.5799, Acc: 0.7442, F1: 0.5217, AUC: 0.6626\n",
      "Epoch 185/300, Train Loss: 0.4375, Acc: 0.7674, F1: 0.6610, AUC: 0.7667\n",
      "Epoch 185/300, Valid Loss: 2.5453, Acc: 0.7209, F1: 0.4000, AUC: 0.6084\n",
      "Epoch 186/300, Train Loss: 0.3825, Acc: 0.7791, F1: 0.6607, AUC: 0.7636\n",
      "Epoch 186/300, Valid Loss: 2.8720, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 187/300, Train Loss: 0.3604, Acc: 0.8314, F1: 0.7521, AUC: 0.8405\n",
      "Epoch 187/300, Valid Loss: 3.1291, Acc: 0.7209, F1: 0.4545, AUC: 0.6268\n",
      "Epoch 188/300, Train Loss: 0.4205, Acc: 0.8023, F1: 0.7167, AUC: 0.8141\n",
      "Epoch 188/300, Valid Loss: 2.6530, Acc: 0.4651, F1: 0.2581, AUC: 0.4187\n",
      "Epoch 189/300, Train Loss: 0.3995, Acc: 0.7791, F1: 0.6724, AUC: 0.7749\n",
      "Epoch 189/300, Valid Loss: 2.8266, Acc: 0.6047, F1: 0.3704, AUC: 0.5406\n",
      "Epoch 190/300, Train Loss: 0.3507, Acc: 0.8198, F1: 0.7350, AUC: 0.8265\n",
      "Epoch 190/300, Valid Loss: 3.1565, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 191/300, Train Loss: 0.3810, Acc: 0.8256, F1: 0.7368, AUC: 0.8250\n",
      "Epoch 191/300, Valid Loss: 2.8495, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 192/300, Train Loss: 0.3888, Acc: 0.8023, F1: 0.7069, AUC: 0.8028\n",
      "Epoch 192/300, Valid Loss: 2.9178, Acc: 0.6744, F1: 0.3636, AUC: 0.5739\n",
      "Epoch 193/300, Train Loss: 0.3632, Acc: 0.8023, F1: 0.7302, AUC: 0.8311\n",
      "Epoch 193/300, Valid Loss: 3.7587, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "Epoch 194/300, Train Loss: 0.4635, Acc: 0.7442, F1: 0.6615, AUC: 0.7728\n",
      "Epoch 194/300, Valid Loss: 3.2082, Acc: 0.6977, F1: 0.2353, AUC: 0.5542\n",
      "Epoch 195/300, Train Loss: 0.4096, Acc: 0.7558, F1: 0.6769, AUC: 0.7867\n",
      "Epoch 195/300, Valid Loss: 3.1927, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 196/300, Train Loss: 0.3958, Acc: 0.8081, F1: 0.7317, AUC: 0.8296\n",
      "Epoch 196/300, Valid Loss: 2.6135, Acc: 0.6512, F1: 0.4828, AUC: 0.6121\n",
      "Epoch 197/300, Train Loss: 0.3720, Acc: 0.8081, F1: 0.7227, AUC: 0.8183\n",
      "Epoch 197/300, Valid Loss: 2.8410, Acc: 0.7209, F1: 0.5000, AUC: 0.6453\n",
      "Epoch 198/300, Train Loss: 0.3778, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
      "Epoch 198/300, Valid Loss: 3.1493, Acc: 0.6279, F1: 0.5556, AUC: 0.6502\n",
      "Epoch 199/300, Train Loss: 0.4435, Acc: 0.7558, F1: 0.6613, AUC: 0.7697\n",
      "Epoch 199/300, Valid Loss: 3.1358, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 200/300, Train Loss: 0.4077, Acc: 0.7849, F1: 0.6992, AUC: 0.8017\n",
      "Epoch 200/300, Valid Loss: 3.3745, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 201/300, Train Loss: 0.4438, Acc: 0.7267, F1: 0.6357, AUC: 0.7491\n",
      "Epoch 201/300, Valid Loss: 3.1195, Acc: 0.7209, F1: 0.4545, AUC: 0.6268\n",
      "Epoch 202/300, Train Loss: 0.4007, Acc: 0.7791, F1: 0.7121, AUC: 0.8203\n",
      "Epoch 202/300, Valid Loss: 3.3230, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 203/300, Train Loss: 0.3942, Acc: 0.7674, F1: 0.6721, AUC: 0.7780\n",
      "Epoch 203/300, Valid Loss: 3.1193, Acc: 0.6512, F1: 0.5455, AUC: 0.6490\n",
      "Epoch 204/300, Train Loss: 0.4118, Acc: 0.7733, F1: 0.6880, AUC: 0.7935\n",
      "Epoch 204/300, Valid Loss: 3.0285, Acc: 0.6744, F1: 0.5000, AUC: 0.6293\n",
      "Epoch 205/300, Train Loss: 0.3734, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 205/300, Valid Loss: 3.5887, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 206/300, Train Loss: 0.3644, Acc: 0.8140, F1: 0.7288, AUC: 0.8224\n",
      "Epoch 206/300, Valid Loss: 3.4470, Acc: 0.6512, F1: 0.4444, AUC: 0.5936\n",
      "Epoch 207/300, Train Loss: 0.4262, Acc: 0.8023, F1: 0.7258, AUC: 0.8255\n",
      "Epoch 207/300, Valid Loss: 2.6370, Acc: 0.6047, F1: 0.5143, AUC: 0.6145\n",
      "Epoch 208/300, Train Loss: 0.3600, Acc: 0.8140, F1: 0.7241, AUC: 0.8167\n",
      "Epoch 208/300, Valid Loss: 3.0346, Acc: 0.6279, F1: 0.5000, AUC: 0.6133\n",
      "Epoch 209/300, Train Loss: 0.4467, Acc: 0.7384, F1: 0.6715, AUC: 0.7857\n",
      "Epoch 209/300, Valid Loss: 3.1435, Acc: 0.6744, F1: 0.4615, AUC: 0.6108\n",
      "Epoch 210/300, Train Loss: 0.3756, Acc: 0.8081, F1: 0.7179, AUC: 0.8126\n",
      "Epoch 210/300, Valid Loss: 3.4007, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 211/300, Train Loss: 0.3820, Acc: 0.7791, F1: 0.6935, AUC: 0.7976\n",
      "Epoch 211/300, Valid Loss: 3.3494, Acc: 0.6744, F1: 0.5625, AUC: 0.6663\n",
      "Epoch 212/300, Train Loss: 0.4016, Acc: 0.7500, F1: 0.6718, AUC: 0.7826\n",
      "Epoch 212/300, Valid Loss: 3.9160, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 213/300, Train Loss: 0.3470, Acc: 0.8372, F1: 0.7544, AUC: 0.8389\n",
      "Epoch 213/300, Valid Loss: 3.8767, Acc: 0.7209, F1: 0.5000, AUC: 0.6453\n",
      "Epoch 214/300, Train Loss: 0.4487, Acc: 0.7791, F1: 0.6935, AUC: 0.7976\n",
      "Epoch 214/300, Valid Loss: 3.1052, Acc: 0.6744, F1: 0.3000, AUC: 0.5554\n",
      "Epoch 215/300, Train Loss: 0.3973, Acc: 0.7616, F1: 0.6720, AUC: 0.7795\n",
      "Epoch 215/300, Valid Loss: 2.9716, Acc: 0.6977, F1: 0.4800, AUC: 0.6281\n",
      "Epoch 216/300, Train Loss: 0.4120, Acc: 0.7674, F1: 0.6825, AUC: 0.7893\n",
      "Epoch 216/300, Valid Loss: 3.2004, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 217/300, Train Loss: 0.3835, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 217/300, Valid Loss: 3.6131, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 218/300, Train Loss: 0.3741, Acc: 0.7674, F1: 0.6774, AUC: 0.7837\n",
      "Epoch 218/300, Valid Loss: 3.4878, Acc: 0.6512, F1: 0.4828, AUC: 0.6121\n",
      "Epoch 219/300, Train Loss: 0.4704, Acc: 0.7500, F1: 0.6560, AUC: 0.7656\n",
      "Epoch 219/300, Valid Loss: 3.4008, Acc: 0.6977, F1: 0.5185, AUC: 0.6466\n",
      "Epoch 220/300, Train Loss: 0.3920, Acc: 0.8023, F1: 0.7213, AUC: 0.8198\n",
      "Epoch 220/300, Valid Loss: 3.0499, Acc: 0.7209, F1: 0.4545, AUC: 0.6268\n",
      "Epoch 221/300, Train Loss: 0.3650, Acc: 0.7965, F1: 0.7009, AUC: 0.7987\n",
      "Epoch 221/300, Valid Loss: 3.1949, Acc: 0.6744, F1: 0.5333, AUC: 0.6478\n",
      "Epoch 222/300, Train Loss: 0.4098, Acc: 0.7500, F1: 0.6861, AUC: 0.7996\n",
      "Epoch 222/300, Valid Loss: 3.9071, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 223/300, Train Loss: 0.4504, Acc: 0.7209, F1: 0.6364, AUC: 0.7506\n",
      "Epoch 223/300, Valid Loss: 3.5708, Acc: 0.7209, F1: 0.5000, AUC: 0.6453\n",
      "Epoch 224/300, Train Loss: 0.3667, Acc: 0.8023, F1: 0.7167, AUC: 0.8141\n",
      "Epoch 224/300, Valid Loss: 3.6923, Acc: 0.6744, F1: 0.5625, AUC: 0.6663\n",
      "Epoch 225/300, Train Loss: 0.3878, Acc: 0.8023, F1: 0.7069, AUC: 0.8028\n",
      "Epoch 225/300, Valid Loss: 3.7263, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "Epoch 226/300, Train Loss: 0.4113, Acc: 0.7791, F1: 0.6885, AUC: 0.7919\n",
      "Epoch 226/300, Valid Loss: 3.3362, Acc: 0.6744, F1: 0.4167, AUC: 0.5924\n",
      "Epoch 227/300, Train Loss: 0.3722, Acc: 0.8081, F1: 0.7273, AUC: 0.8239\n",
      "Epoch 227/300, Valid Loss: 3.8737, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 228/300, Train Loss: 0.3572, Acc: 0.8256, F1: 0.7500, AUC: 0.8420\n",
      "Epoch 228/300, Valid Loss: 3.6582, Acc: 0.6512, F1: 0.4444, AUC: 0.5936\n",
      "Epoch 229/300, Train Loss: 0.4302, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 229/300, Valid Loss: 3.1064, Acc: 0.6512, F1: 0.4828, AUC: 0.6121\n",
      "Epoch 230/300, Train Loss: 0.3472, Acc: 0.8372, F1: 0.7627, AUC: 0.8503\n",
      "Epoch 230/300, Valid Loss: 3.2117, Acc: 0.6047, F1: 0.5143, AUC: 0.6145\n",
      "Epoch 231/300, Train Loss: 0.4187, Acc: 0.7500, F1: 0.6614, AUC: 0.7713\n",
      "Epoch 231/300, Valid Loss: 3.4207, Acc: 0.6744, F1: 0.5000, AUC: 0.6293\n",
      "Epoch 232/300, Train Loss: 0.3856, Acc: 0.8081, F1: 0.7227, AUC: 0.8183\n",
      "Epoch 232/300, Valid Loss: 3.2490, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 233/300, Train Loss: 0.4085, Acc: 0.7674, F1: 0.6923, AUC: 0.8007\n",
      "Epoch 233/300, Valid Loss: 3.9218, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 234/300, Train Loss: 0.3890, Acc: 0.7791, F1: 0.6885, AUC: 0.7919\n",
      "Epoch 234/300, Valid Loss: 3.6273, Acc: 0.6279, F1: 0.4667, AUC: 0.5948\n",
      "Epoch 235/300, Train Loss: 0.3768, Acc: 0.7965, F1: 0.7200, AUC: 0.8213\n",
      "Epoch 235/300, Valid Loss: 3.8271, Acc: 0.6977, F1: 0.4800, AUC: 0.6281\n",
      "Epoch 236/300, Train Loss: 0.3617, Acc: 0.8314, F1: 0.7521, AUC: 0.8405\n",
      "Epoch 236/300, Valid Loss: 3.3725, Acc: 0.5814, F1: 0.4375, AUC: 0.5603\n",
      "Epoch 237/300, Train Loss: 0.4696, Acc: 0.7267, F1: 0.6569, AUC: 0.7718\n",
      "Epoch 237/300, Valid Loss: 3.1766, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 238/300, Train Loss: 0.3334, Acc: 0.8488, F1: 0.7719, AUC: 0.8529\n",
      "Epoch 238/300, Valid Loss: 3.5868, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "Epoch 239/300, Train Loss: 0.3664, Acc: 0.7907, F1: 0.7000, AUC: 0.8002\n",
      "Epoch 239/300, Valid Loss: 4.0313, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 240/300, Train Loss: 0.3736, Acc: 0.7965, F1: 0.7059, AUC: 0.8043\n",
      "Epoch 240/300, Valid Loss: 3.8878, Acc: 0.7209, F1: 0.5000, AUC: 0.6453\n",
      "Epoch 241/300, Train Loss: 0.3529, Acc: 0.7965, F1: 0.7154, AUC: 0.8157\n",
      "Epoch 241/300, Valid Loss: 4.8473, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 242/300, Train Loss: 0.3878, Acc: 0.7907, F1: 0.6897, AUC: 0.7889\n",
      "Epoch 242/300, Valid Loss: 3.9413, Acc: 0.6977, F1: 0.6061, AUC: 0.7020\n",
      "Epoch 243/300, Train Loss: 0.4089, Acc: 0.7558, F1: 0.6667, AUC: 0.7754\n",
      "Epoch 243/300, Valid Loss: 4.4856, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 244/300, Train Loss: 0.4579, Acc: 0.6977, F1: 0.6232, AUC: 0.7398\n",
      "Epoch 244/300, Valid Loss: 4.4291, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 245/300, Train Loss: 0.3921, Acc: 0.7500, F1: 0.6718, AUC: 0.7826\n",
      "Epoch 245/300, Valid Loss: 4.1256, Acc: 0.6512, F1: 0.5714, AUC: 0.6675\n",
      "Epoch 246/300, Train Loss: 0.3344, Acc: 0.8314, F1: 0.7563, AUC: 0.8461\n",
      "Epoch 246/300, Valid Loss: 5.5921, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 247/300, Train Loss: 0.3195, Acc: 0.8488, F1: 0.7797, AUC: 0.8642\n",
      "Epoch 247/300, Valid Loss: 6.1744, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 248/300, Train Loss: 0.4380, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
      "Epoch 248/300, Valid Loss: 4.2069, Acc: 0.6744, F1: 0.5882, AUC: 0.6847\n",
      "Epoch 249/300, Train Loss: 0.4358, Acc: 0.7326, F1: 0.6462, AUC: 0.7589\n",
      "Epoch 249/300, Valid Loss: 4.1409, Acc: 0.6512, F1: 0.4828, AUC: 0.6121\n",
      "Epoch 250/300, Train Loss: 0.3569, Acc: 0.8256, F1: 0.7541, AUC: 0.8477\n",
      "Epoch 250/300, Valid Loss: 5.3387, Acc: 0.7209, F1: 0.3333, AUC: 0.5899\n",
      "Epoch 251/300, Train Loss: 0.3658, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
      "Epoch 251/300, Valid Loss: 5.1454, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 252/300, Train Loss: 0.3363, Acc: 0.8256, F1: 0.7541, AUC: 0.8477\n",
      "Epoch 252/300, Valid Loss: 5.3624, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 253/300, Train Loss: 0.5947, Acc: 0.5291, F1: 0.4706, AUC: 0.5802\n",
      "Epoch 253/300, Valid Loss: 4.7883, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 254/300, Train Loss: 0.3376, Acc: 0.7733, F1: 0.6214, AUC: 0.7311\n",
      "Epoch 254/300, Valid Loss: 5.5514, Acc: 0.6977, F1: 0.4348, AUC: 0.6096\n",
      "Epoch 255/300, Train Loss: 0.3620, Acc: 0.7907, F1: 0.7049, AUC: 0.8059\n",
      "Epoch 255/300, Valid Loss: 5.0669, Acc: 0.6279, F1: 0.5294, AUC: 0.6318\n",
      "Epoch 256/300, Train Loss: 0.4484, Acc: 0.7674, F1: 0.6970, AUC: 0.8064\n",
      "Epoch 256/300, Valid Loss: 3.9504, Acc: 0.6744, F1: 0.5333, AUC: 0.6478\n",
      "Epoch 257/300, Train Loss: 0.3938, Acc: 0.7849, F1: 0.7040, AUC: 0.8074\n",
      "Epoch 257/300, Valid Loss: 4.0264, Acc: 0.6279, F1: 0.5000, AUC: 0.6133\n",
      "Epoch 258/300, Train Loss: 0.3525, Acc: 0.7965, F1: 0.7154, AUC: 0.8157\n",
      "Epoch 258/300, Valid Loss: 4.7268, Acc: 0.6977, F1: 0.3810, AUC: 0.5911\n",
      "Epoch 259/300, Train Loss: 0.3523, Acc: 0.8256, F1: 0.7541, AUC: 0.8477\n",
      "Epoch 259/300, Valid Loss: 4.7952, Acc: 0.6977, F1: 0.3158, AUC: 0.5727\n",
      "Epoch 260/300, Train Loss: 0.3477, Acc: 0.8140, F1: 0.7419, AUC: 0.8394\n",
      "Epoch 260/300, Valid Loss: 5.1199, Acc: 0.7209, F1: 0.4000, AUC: 0.6084\n",
      "Epoch 261/300, Train Loss: 0.4391, Acc: 0.7151, F1: 0.6525, AUC: 0.7692\n",
      "Epoch 261/300, Valid Loss: 3.8424, Acc: 0.3023, F1: 0.4444, AUC: 0.4458\n",
      "Epoch 262/300, Train Loss: 0.6782, Acc: 0.5523, F1: 0.3937, AUC: 0.5344\n",
      "Epoch 262/300, Valid Loss: 3.7673, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 263/300, Train Loss: 0.6301, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 263/300, Valid Loss: 3.7998, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 264/300, Train Loss: 0.6111, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 264/300, Valid Loss: 3.8775, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 265/300, Train Loss: 0.6053, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 265/300, Valid Loss: 3.9355, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 266/300, Train Loss: 0.6060, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 266/300, Valid Loss: 3.9427, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 267/300, Train Loss: 0.6013, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 267/300, Valid Loss: 3.9677, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 268/300, Train Loss: 0.6051, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 268/300, Valid Loss: 3.9550, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 269/300, Train Loss: 0.6006, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 269/300, Valid Loss: 3.9719, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 270/300, Train Loss: 0.6037, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 270/300, Valid Loss: 3.9496, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 271/300, Train Loss: 0.5981, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 271/300, Valid Loss: 3.9802, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 272/300, Train Loss: 0.6010, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 272/300, Valid Loss: 3.9605, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 273/300, Train Loss: 0.5993, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 273/300, Valid Loss: 3.9861, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 274/300, Train Loss: 0.5954, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 274/300, Valid Loss: 4.0188, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 275/300, Train Loss: 0.6010, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 275/300, Valid Loss: 4.0150, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 276/300, Train Loss: 0.6046, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 276/300, Valid Loss: 3.9933, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 277/300, Train Loss: 0.6007, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 277/300, Valid Loss: 3.9549, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 278/300, Train Loss: 0.6055, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 278/300, Valid Loss: 3.9278, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 279/300, Train Loss: 0.6014, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 279/300, Valid Loss: 3.9177, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 280/300, Train Loss: 0.6052, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 280/300, Valid Loss: 3.9293, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 281/300, Train Loss: 0.6013, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 281/300, Valid Loss: 3.9369, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 282/300, Train Loss: 0.5998, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 282/300, Valid Loss: 3.9231, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 283/300, Train Loss: 0.6003, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 283/300, Valid Loss: 3.9065, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 284/300, Train Loss: 0.6024, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 284/300, Valid Loss: 3.8835, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 285/300, Train Loss: 0.6061, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 285/300, Valid Loss: 3.8636, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 286/300, Train Loss: 0.6048, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 286/300, Valid Loss: 3.8796, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 287/300, Train Loss: 0.5965, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 287/300, Valid Loss: 3.8696, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 288/300, Train Loss: 0.6026, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 288/300, Valid Loss: 3.8616, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 289/300, Train Loss: 0.6015, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 289/300, Valid Loss: 3.8619, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 290/300, Train Loss: 0.6009, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 290/300, Valid Loss: 3.8393, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 291/300, Train Loss: 0.6051, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 291/300, Valid Loss: 3.8349, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 292/300, Train Loss: 0.6015, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 292/300, Valid Loss: 3.8100, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 293/300, Train Loss: 0.6022, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 293/300, Valid Loss: 3.8251, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 294/300, Train Loss: 0.6035, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 294/300, Valid Loss: 3.7964, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 295/300, Train Loss: 0.6021, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 295/300, Valid Loss: 3.7834, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 296/300, Train Loss: 0.6031, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 296/300, Valid Loss: 3.7945, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 297/300, Train Loss: 0.6230, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 297/300, Valid Loss: 2.3475, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 298/300, Train Loss: 0.6022, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 298/300, Valid Loss: 2.4320, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 299/300, Train Loss: 0.5977, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 299/300, Valid Loss: 2.5190, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 300/300, Train Loss: 0.5961, Acc: 0.7035, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 300/300, Valid Loss: 2.5672, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "-- Fold 2/5 --\n",
      "Epoch 1/300, Train Loss: 0.6607, Acc: 0.6860, F1: 0.1000, AUC: 0.5080\n",
      "Epoch 1/300, Valid Loss: 0.6518, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Train Loss: 0.6696, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Valid Loss: 0.6456, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Train Loss: 0.6392, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6443, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Train Loss: 0.6446, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Valid Loss: 0.6311, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Train Loss: 0.6356, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.6394, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Train Loss: 0.6433, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.6332, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Train Loss: 0.6236, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.6682, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Train Loss: 0.6266, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.6571, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Train Loss: 0.6109, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.6556, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Train Loss: 0.6113, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.6602, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Train Loss: 0.6143, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.6639, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Train Loss: 0.6085, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.7094, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Train Loss: 0.6120, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.6799, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Train Loss: 0.6105, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.6600, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Train Loss: 0.6033, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.6557, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Train Loss: 0.6051, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.6555, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Train Loss: 0.6041, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.6719, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Train Loss: 0.5994, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Valid Loss: 0.8040, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Train Loss: 0.6085, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Valid Loss: 0.7246, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 20/300, Train Loss: 0.6132, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 20/300, Valid Loss: 0.6947, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 21/300, Train Loss: 0.5964, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 21/300, Valid Loss: 0.7602, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 22/300, Train Loss: 0.5828, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 22/300, Valid Loss: 0.9342, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 23/300, Train Loss: 0.6015, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 23/300, Valid Loss: 0.8321, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 24/300, Train Loss: 0.5871, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 24/300, Valid Loss: 0.7020, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Train Loss: 0.6085, Acc: 0.6802, F1: 0.1270, AUC: 0.5093\n",
      "Epoch 25/300, Valid Loss: 0.9060, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 26/300, Train Loss: 0.5796, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 26/300, Valid Loss: 0.9079, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 27/300, Train Loss: 0.5806, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 27/300, Valid Loss: 0.9197, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 28/300, Train Loss: 0.5969, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 28/300, Valid Loss: 0.9032, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 29/300, Train Loss: 0.5799, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 29/300, Valid Loss: 0.8963, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 30/300, Train Loss: 0.5760, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 30/300, Valid Loss: 0.9531, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 31/300, Train Loss: 0.5916, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 31/300, Valid Loss: 0.9440, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 32/300, Train Loss: 0.5849, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 32/300, Valid Loss: 1.0179, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 33/300, Train Loss: 0.5810, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 33/300, Valid Loss: 0.9894, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 34/300, Train Loss: 0.5813, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 34/300, Valid Loss: 0.9471, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 35/300, Train Loss: 0.5778, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 35/300, Valid Loss: 1.0278, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 36/300, Train Loss: 0.5787, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 36/300, Valid Loss: 0.9728, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 37/300, Train Loss: 0.5720, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 37/300, Valid Loss: 1.0167, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 38/300, Train Loss: 0.5593, Acc: 0.6802, F1: 0.0984, AUC: 0.5038\n",
      "Epoch 38/300, Valid Loss: 1.1566, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 39/300, Train Loss: 0.5783, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 39/300, Valid Loss: 1.0086, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 40/300, Train Loss: 0.5744, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 40/300, Valid Loss: 1.0302, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 41/300, Train Loss: 0.5754, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 41/300, Valid Loss: 1.0031, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 42/300, Train Loss: 0.5542, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 42/300, Valid Loss: 1.0738, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 43/300, Train Loss: 0.5660, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 43/300, Valid Loss: 1.0740, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 44/300, Train Loss: 0.5590, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 44/300, Valid Loss: 1.0476, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 45/300, Train Loss: 0.5657, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Valid Loss: 1.0768, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 46/300, Train Loss: 0.5627, Acc: 0.7035, F1: 0.0727, AUC: 0.5151\n",
      "Epoch 46/300, Valid Loss: 1.0711, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 47/300, Train Loss: 0.5500, Acc: 0.6802, F1: 0.0678, AUC: 0.4984\n",
      "Epoch 47/300, Valid Loss: 1.1054, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 48/300, Train Loss: 0.5449, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 48/300, Valid Loss: 1.1409, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 49/300, Train Loss: 0.5645, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 49/300, Valid Loss: 1.0308, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 50/300, Train Loss: 0.5656, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 50/300, Valid Loss: 1.0255, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 51/300, Train Loss: 0.5674, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Valid Loss: 1.0713, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 52/300, Train Loss: 0.5491, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 52/300, Valid Loss: 1.0914, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 53/300, Train Loss: 0.5492, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 53/300, Valid Loss: 1.1632, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 54/300, Train Loss: 0.5650, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 54/300, Valid Loss: 1.1214, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 55/300, Train Loss: 0.5530, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 55/300, Valid Loss: 1.1394, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 56/300, Train Loss: 0.5351, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 56/300, Valid Loss: 1.1747, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 57/300, Train Loss: 0.5410, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 57/300, Valid Loss: 1.2209, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 58/300, Train Loss: 0.5411, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 58/300, Valid Loss: 1.1324, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 59/300, Train Loss: 0.5455, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 59/300, Valid Loss: 1.1311, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 60/300, Train Loss: 0.5484, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 60/300, Valid Loss: 1.1023, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 61/300, Train Loss: 0.5332, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 61/300, Valid Loss: 1.0556, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 62/300, Train Loss: 0.5551, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 62/300, Valid Loss: 1.0861, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 63/300, Train Loss: 0.5331, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 63/300, Valid Loss: 1.0686, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 64/300, Train Loss: 0.5346, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 64/300, Valid Loss: 1.1201, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 65/300, Train Loss: 0.5458, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 65/300, Valid Loss: 1.0702, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 66/300, Train Loss: 0.5379, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 66/300, Valid Loss: 1.2034, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 67/300, Train Loss: 0.5349, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 67/300, Valid Loss: 1.1705, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 68/300, Train Loss: 0.5228, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 68/300, Valid Loss: 1.1152, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 69/300, Train Loss: 0.5535, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 69/300, Valid Loss: 1.3147, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 70/300, Train Loss: 0.5228, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 70/300, Valid Loss: 1.3664, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 71/300, Train Loss: 0.5192, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 71/300, Valid Loss: 1.1984, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 72/300, Train Loss: 0.5160, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 72/300, Valid Loss: 1.1940, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 73/300, Train Loss: 0.4843, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 73/300, Valid Loss: 1.5424, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 74/300, Train Loss: 0.5323, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 74/300, Valid Loss: 1.2633, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 75/300, Train Loss: 0.4923, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 75/300, Valid Loss: 1.3167, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 76/300, Train Loss: 0.5225, Acc: 0.7035, F1: 0.0377, AUC: 0.5096\n",
      "Epoch 76/300, Valid Loss: 1.2535, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 77/300, Train Loss: 0.5145, Acc: 0.7035, F1: 0.0727, AUC: 0.5151\n",
      "Epoch 77/300, Valid Loss: 1.2691, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 78/300, Train Loss: 0.5162, Acc: 0.7674, F1: 0.4444, AUC: 0.6372\n",
      "Epoch 78/300, Valid Loss: 1.5746, Acc: 0.6279, F1: 0.0000, AUC: 0.4500\n",
      "Epoch 79/300, Train Loss: 0.5593, Acc: 0.7151, F1: 0.3951, AUC: 0.5997\n",
      "Epoch 79/300, Valid Loss: 1.2265, Acc: 0.5581, F1: 0.2963, AUC: 0.4872\n",
      "Epoch 80/300, Train Loss: 0.5109, Acc: 0.7093, F1: 0.4792, AUC: 0.6337\n",
      "Epoch 80/300, Valid Loss: 1.2123, Acc: 0.6512, F1: 0.1176, AUC: 0.4885\n",
      "Epoch 81/300, Train Loss: 0.5309, Acc: 0.7267, F1: 0.4198, AUC: 0.6135\n",
      "Epoch 81/300, Valid Loss: 1.3657, Acc: 0.6047, F1: 0.0000, AUC: 0.4333\n",
      "Epoch 82/300, Train Loss: 0.5095, Acc: 0.6977, F1: 0.4222, AUC: 0.6035\n",
      "Epoch 82/300, Valid Loss: 1.2118, Acc: 0.5814, F1: 0.1818, AUC: 0.4603\n",
      "Epoch 83/300, Train Loss: 0.5089, Acc: 0.7151, F1: 0.4731, AUC: 0.6324\n",
      "Epoch 83/300, Valid Loss: 1.2546, Acc: 0.4884, F1: 0.0833, AUC: 0.3718\n",
      "Epoch 84/300, Train Loss: 0.5132, Acc: 0.7093, F1: 0.3750, AUC: 0.5901\n",
      "Epoch 84/300, Valid Loss: 1.2539, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 85/300, Train Loss: 0.4933, Acc: 0.7326, F1: 0.5106, AUC: 0.6558\n",
      "Epoch 85/300, Valid Loss: 1.2442, Acc: 0.6047, F1: 0.1905, AUC: 0.4769\n",
      "Epoch 86/300, Train Loss: 0.5146, Acc: 0.7093, F1: 0.5000, AUC: 0.6446\n",
      "Epoch 86/300, Valid Loss: 1.2396, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 87/300, Train Loss: 0.5359, Acc: 0.7151, F1: 0.4731, AUC: 0.6324\n",
      "Epoch 87/300, Valid Loss: 1.1983, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 88/300, Train Loss: 0.4937, Acc: 0.7151, F1: 0.3288, AUC: 0.5779\n",
      "Epoch 88/300, Valid Loss: 1.2275, Acc: 0.5349, F1: 0.0909, AUC: 0.4051\n",
      "Epoch 89/300, Train Loss: 0.4934, Acc: 0.7267, F1: 0.4598, AUC: 0.6298\n",
      "Epoch 89/300, Valid Loss: 1.3085, Acc: 0.5116, F1: 0.2222, AUC: 0.4321\n",
      "Epoch 90/300, Train Loss: 0.5015, Acc: 0.7558, F1: 0.5333, AUC: 0.6724\n",
      "Epoch 90/300, Valid Loss: 1.3425, Acc: 0.4884, F1: 0.2667, AUC: 0.4372\n",
      "Epoch 91/300, Train Loss: 0.5072, Acc: 0.7733, F1: 0.5618, AUC: 0.6904\n",
      "Epoch 91/300, Valid Loss: 1.3606, Acc: 0.4884, F1: 0.0833, AUC: 0.3718\n",
      "Epoch 92/300, Train Loss: 0.5083, Acc: 0.7442, F1: 0.5510, AUC: 0.6804\n",
      "Epoch 92/300, Valid Loss: 1.2373, Acc: 0.5581, F1: 0.1739, AUC: 0.4436\n",
      "Epoch 93/300, Train Loss: 0.5156, Acc: 0.7151, F1: 0.3467, AUC: 0.5833\n",
      "Epoch 93/300, Valid Loss: 1.1254, Acc: 0.5814, F1: 0.0000, AUC: 0.4167\n",
      "Epoch 94/300, Train Loss: 0.4720, Acc: 0.7326, F1: 0.4524, AUC: 0.6285\n",
      "Epoch 94/300, Valid Loss: 1.2233, Acc: 0.5349, F1: 0.3333, AUC: 0.4923\n",
      "Epoch 95/300, Train Loss: 0.4733, Acc: 0.7384, F1: 0.4304, AUC: 0.6218\n",
      "Epoch 95/300, Valid Loss: 1.1943, Acc: 0.5116, F1: 0.0870, AUC: 0.3885\n",
      "Epoch 96/300, Train Loss: 0.4670, Acc: 0.7558, F1: 0.5227, AUC: 0.6670\n",
      "Epoch 96/300, Valid Loss: 1.1104, Acc: 0.6977, F1: 0.3810, AUC: 0.5872\n",
      "Epoch 97/300, Train Loss: 0.5354, Acc: 0.6628, F1: 0.3256, AUC: 0.5513\n",
      "Epoch 97/300, Valid Loss: 1.1058, Acc: 0.5814, F1: 0.1000, AUC: 0.4385\n",
      "Epoch 98/300, Train Loss: 0.4638, Acc: 0.7500, F1: 0.3944, AUC: 0.6138\n",
      "Epoch 98/300, Valid Loss: 1.2683, Acc: 0.4419, F1: 0.4000, AUC: 0.4910\n",
      "Epoch 99/300, Train Loss: 0.4625, Acc: 0.7558, F1: 0.5714, AUC: 0.6942\n",
      "Epoch 99/300, Valid Loss: 1.4780, Acc: 0.6047, F1: 0.2609, AUC: 0.4987\n",
      "Epoch 100/300, Train Loss: 0.5042, Acc: 0.7267, F1: 0.5347, AUC: 0.6679\n",
      "Epoch 100/300, Valid Loss: 1.5030, Acc: 0.4651, F1: 0.3030, AUC: 0.4423\n",
      "Epoch 101/300, Train Loss: 0.4789, Acc: 0.7267, F1: 0.5524, AUC: 0.6788\n",
      "Epoch 101/300, Valid Loss: 1.3645, Acc: 0.6744, F1: 0.1250, AUC: 0.5051\n",
      "Epoch 102/300, Train Loss: 0.4447, Acc: 0.7616, F1: 0.5684, AUC: 0.6929\n",
      "Epoch 102/300, Valid Loss: 1.3266, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 103/300, Train Loss: 0.4823, Acc: 0.7616, F1: 0.5941, AUC: 0.7093\n",
      "Epoch 103/300, Valid Loss: 1.1837, Acc: 0.6512, F1: 0.3478, AUC: 0.5538\n",
      "Epoch 104/300, Train Loss: 0.4599, Acc: 0.7151, F1: 0.5739, AUC: 0.6923\n",
      "Epoch 104/300, Valid Loss: 1.5036, Acc: 0.5116, F1: 0.2222, AUC: 0.4321\n",
      "Epoch 105/300, Train Loss: 0.4857, Acc: 0.7791, F1: 0.6042, AUC: 0.7163\n",
      "Epoch 105/300, Valid Loss: 1.1944, Acc: 0.6047, F1: 0.3704, AUC: 0.5423\n",
      "Epoch 106/300, Train Loss: 0.4641, Acc: 0.7384, F1: 0.5055, AUC: 0.6545\n",
      "Epoch 106/300, Valid Loss: 1.2705, Acc: 0.5814, F1: 0.1818, AUC: 0.4603\n",
      "Epoch 107/300, Train Loss: 0.4643, Acc: 0.7442, F1: 0.5417, AUC: 0.6750\n",
      "Epoch 107/300, Valid Loss: 1.3006, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 108/300, Train Loss: 0.4653, Acc: 0.7558, F1: 0.5962, AUC: 0.7106\n",
      "Epoch 108/300, Valid Loss: 1.3086, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 109/300, Train Loss: 0.4857, Acc: 0.7384, F1: 0.4444, AUC: 0.6272\n",
      "Epoch 109/300, Valid Loss: 1.5172, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 110/300, Train Loss: 0.4791, Acc: 0.7326, F1: 0.4889, AUC: 0.6449\n",
      "Epoch 110/300, Valid Loss: 1.2981, Acc: 0.6047, F1: 0.1053, AUC: 0.4551\n",
      "Epoch 111/300, Train Loss: 0.4294, Acc: 0.7674, F1: 0.5349, AUC: 0.6753\n",
      "Epoch 111/300, Valid Loss: 1.8930, Acc: 0.6744, F1: 0.0000, AUC: 0.4833\n",
      "Epoch 112/300, Train Loss: 0.5190, Acc: 0.7616, F1: 0.5684, AUC: 0.6929\n",
      "Epoch 112/300, Valid Loss: 1.1356, Acc: 0.6977, F1: 0.1333, AUC: 0.5218\n",
      "Epoch 113/300, Train Loss: 0.4663, Acc: 0.7500, F1: 0.4941, AUC: 0.6519\n",
      "Epoch 113/300, Valid Loss: 1.2664, Acc: 0.4186, F1: 0.3590, AUC: 0.4526\n",
      "Epoch 114/300, Train Loss: 0.4580, Acc: 0.7500, F1: 0.5567, AUC: 0.6846\n",
      "Epoch 114/300, Valid Loss: 1.2902, Acc: 0.6977, F1: 0.1333, AUC: 0.5218\n",
      "Epoch 115/300, Train Loss: 0.4333, Acc: 0.7616, F1: 0.5591, AUC: 0.6875\n",
      "Epoch 115/300, Valid Loss: 1.3607, Acc: 0.4884, F1: 0.0833, AUC: 0.3718\n",
      "Epoch 116/300, Train Loss: 0.4211, Acc: 0.7674, F1: 0.5918, AUC: 0.7080\n",
      "Epoch 116/300, Valid Loss: 1.2776, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 117/300, Train Loss: 0.4830, Acc: 0.7558, F1: 0.5116, AUC: 0.6615\n",
      "Epoch 117/300, Valid Loss: 1.3662, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 118/300, Train Loss: 0.4568, Acc: 0.7500, F1: 0.5275, AUC: 0.6683\n",
      "Epoch 118/300, Valid Loss: 1.2988, Acc: 0.6744, F1: 0.1250, AUC: 0.5051\n",
      "Epoch 119/300, Train Loss: 0.4447, Acc: 0.7267, F1: 0.4051, AUC: 0.6080\n",
      "Epoch 119/300, Valid Loss: 1.3427, Acc: 0.5581, F1: 0.2963, AUC: 0.4872\n",
      "Epoch 120/300, Train Loss: 0.4680, Acc: 0.7500, F1: 0.5657, AUC: 0.6901\n",
      "Epoch 120/300, Valid Loss: 1.1897, Acc: 0.6512, F1: 0.0000, AUC: 0.4667\n",
      "Epoch 121/300, Train Loss: 0.4579, Acc: 0.7151, F1: 0.4096, AUC: 0.6051\n",
      "Epoch 121/300, Valid Loss: 1.2111, Acc: 0.6047, F1: 0.1905, AUC: 0.4769\n",
      "Epoch 122/300, Train Loss: 0.4498, Acc: 0.7965, F1: 0.6465, AUC: 0.7452\n",
      "Epoch 122/300, Valid Loss: 1.2366, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 123/300, Train Loss: 0.4740, Acc: 0.7267, F1: 0.4198, AUC: 0.6135\n",
      "Epoch 123/300, Valid Loss: 1.2377, Acc: 0.7209, F1: 0.4000, AUC: 0.6038\n",
      "Epoch 124/300, Train Loss: 0.4240, Acc: 0.7674, F1: 0.5122, AUC: 0.6644\n",
      "Epoch 124/300, Valid Loss: 1.3493, Acc: 0.6047, F1: 0.3200, AUC: 0.5205\n",
      "Epoch 125/300, Train Loss: 0.4346, Acc: 0.7907, F1: 0.6327, AUC: 0.7356\n",
      "Epoch 125/300, Valid Loss: 1.2898, Acc: 0.7209, F1: 0.3333, AUC: 0.5821\n",
      "Epoch 126/300, Train Loss: 0.4493, Acc: 0.8023, F1: 0.6458, AUC: 0.7439\n",
      "Epoch 126/300, Valid Loss: 1.2910, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 127/300, Train Loss: 0.4569, Acc: 0.7500, F1: 0.5057, AUC: 0.6574\n",
      "Epoch 127/300, Valid Loss: 1.4405, Acc: 0.5814, F1: 0.3571, AUC: 0.5256\n",
      "Epoch 128/300, Train Loss: 0.4622, Acc: 0.7442, F1: 0.6000, AUC: 0.7131\n",
      "Epoch 128/300, Valid Loss: 1.5626, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 129/300, Train Loss: 0.4480, Acc: 0.7558, F1: 0.5800, AUC: 0.6997\n",
      "Epoch 129/300, Valid Loss: 1.3043, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 130/300, Train Loss: 0.4608, Acc: 0.7616, F1: 0.5060, AUC: 0.6603\n",
      "Epoch 130/300, Valid Loss: 1.4556, Acc: 0.5116, F1: 0.3226, AUC: 0.4756\n",
      "Epoch 131/300, Train Loss: 0.4266, Acc: 0.7674, F1: 0.5455, AUC: 0.6808\n",
      "Epoch 131/300, Valid Loss: 1.6036, Acc: 0.6744, F1: 0.1250, AUC: 0.5051\n",
      "Epoch 132/300, Train Loss: 0.4211, Acc: 0.7733, F1: 0.6486, AUC: 0.7503\n",
      "Epoch 132/300, Valid Loss: 1.6007, Acc: 0.4186, F1: 0.3243, AUC: 0.4308\n",
      "Epoch 133/300, Train Loss: 0.4696, Acc: 0.7965, F1: 0.6847, AUC: 0.7779\n",
      "Epoch 133/300, Valid Loss: 1.3860, Acc: 0.4651, F1: 0.4103, AUC: 0.5077\n",
      "Epoch 134/300, Train Loss: 0.4581, Acc: 0.7674, F1: 0.6154, AUC: 0.7244\n",
      "Epoch 134/300, Valid Loss: 1.6076, Acc: 0.6977, F1: 0.3810, AUC: 0.5872\n",
      "Epoch 135/300, Train Loss: 0.3950, Acc: 0.8023, F1: 0.6458, AUC: 0.7439\n",
      "Epoch 135/300, Valid Loss: 1.4513, Acc: 0.6512, F1: 0.3478, AUC: 0.5538\n",
      "Epoch 136/300, Train Loss: 0.3833, Acc: 0.8081, F1: 0.6598, AUC: 0.7535\n",
      "Epoch 136/300, Valid Loss: 1.5808, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 137/300, Train Loss: 0.4032, Acc: 0.7907, F1: 0.6250, AUC: 0.7301\n",
      "Epoch 137/300, Valid Loss: 1.5002, Acc: 0.5349, F1: 0.3333, AUC: 0.4923\n",
      "Epoch 138/300, Train Loss: 0.4849, Acc: 0.7791, F1: 0.6481, AUC: 0.7490\n",
      "Epoch 138/300, Valid Loss: 1.5279, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 139/300, Train Loss: 0.4337, Acc: 0.7791, F1: 0.5870, AUC: 0.7054\n",
      "Epoch 139/300, Valid Loss: 1.7390, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 140/300, Train Loss: 0.4015, Acc: 0.8140, F1: 0.6923, AUC: 0.7795\n",
      "Epoch 140/300, Valid Loss: 1.5723, Acc: 0.6512, F1: 0.1176, AUC: 0.4885\n",
      "Epoch 141/300, Train Loss: 0.4122, Acc: 0.8023, F1: 0.6731, AUC: 0.7657\n",
      "Epoch 141/300, Valid Loss: 1.7643, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 142/300, Train Loss: 0.4250, Acc: 0.7965, F1: 0.6667, AUC: 0.7615\n",
      "Epoch 142/300, Valid Loss: 1.5916, Acc: 0.3953, F1: 0.3500, AUC: 0.4359\n",
      "Epoch 143/300, Train Loss: 0.4103, Acc: 0.8023, F1: 0.6667, AUC: 0.7603\n",
      "Epoch 143/300, Valid Loss: 1.6593, Acc: 0.6047, F1: 0.1053, AUC: 0.4551\n",
      "Epoch 144/300, Train Loss: 0.4303, Acc: 0.7616, F1: 0.5287, AUC: 0.6712\n",
      "Epoch 144/300, Valid Loss: 1.6357, Acc: 0.6047, F1: 0.2609, AUC: 0.4987\n",
      "Epoch 145/300, Train Loss: 0.4991, Acc: 0.7267, F1: 0.5607, AUC: 0.6843\n",
      "Epoch 145/300, Valid Loss: 1.5292, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 146/300, Train Loss: 0.4040, Acc: 0.7907, F1: 0.6471, AUC: 0.7465\n",
      "Epoch 146/300, Valid Loss: 1.6136, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 147/300, Train Loss: 0.3935, Acc: 0.7849, F1: 0.6542, AUC: 0.7532\n",
      "Epoch 147/300, Valid Loss: 1.6585, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 148/300, Train Loss: 0.3740, Acc: 0.7965, F1: 0.6392, AUC: 0.7397\n",
      "Epoch 148/300, Valid Loss: 1.8512, Acc: 0.4419, F1: 0.3333, AUC: 0.4474\n",
      "Epoch 149/300, Train Loss: 0.4174, Acc: 0.7616, F1: 0.6095, AUC: 0.7202\n",
      "Epoch 149/300, Valid Loss: 1.7020, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 150/300, Train Loss: 0.3758, Acc: 0.8314, F1: 0.7434, AUC: 0.8247\n",
      "Epoch 150/300, Valid Loss: 1.6343, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 151/300, Train Loss: 0.4297, Acc: 0.7907, F1: 0.5814, AUC: 0.7029\n",
      "Epoch 151/300, Valid Loss: 1.6062, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 152/300, Train Loss: 0.3790, Acc: 0.7907, F1: 0.6400, AUC: 0.7410\n",
      "Epoch 152/300, Valid Loss: 1.5853, Acc: 0.6279, F1: 0.1111, AUC: 0.4718\n",
      "Epoch 153/300, Train Loss: 0.3926, Acc: 0.8198, F1: 0.6737, AUC: 0.7619\n",
      "Epoch 153/300, Valid Loss: 1.6233, Acc: 0.3721, F1: 0.4000, AUC: 0.4628\n",
      "Epoch 154/300, Train Loss: 0.4239, Acc: 0.7558, F1: 0.5714, AUC: 0.6942\n",
      "Epoch 154/300, Valid Loss: 1.4436, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 155/300, Train Loss: 0.4088, Acc: 0.7733, F1: 0.6214, AUC: 0.7285\n",
      "Epoch 155/300, Valid Loss: 1.6744, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 156/300, Train Loss: 0.4480, Acc: 0.7791, F1: 0.6481, AUC: 0.7490\n",
      "Epoch 156/300, Valid Loss: 1.6896, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 157/300, Train Loss: 0.4058, Acc: 0.7674, F1: 0.5745, AUC: 0.6971\n",
      "Epoch 157/300, Valid Loss: 1.6476, Acc: 0.6047, F1: 0.2609, AUC: 0.4987\n",
      "Epoch 158/300, Train Loss: 0.3679, Acc: 0.7907, F1: 0.6250, AUC: 0.7301\n",
      "Epoch 158/300, Valid Loss: 1.8898, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 159/300, Train Loss: 0.4522, Acc: 0.8081, F1: 0.6374, AUC: 0.7372\n",
      "Epoch 159/300, Valid Loss: 1.7628, Acc: 0.5581, F1: 0.3871, AUC: 0.5308\n",
      "Epoch 160/300, Train Loss: 0.4218, Acc: 0.7558, F1: 0.6250, AUC: 0.7324\n",
      "Epoch 160/300, Valid Loss: 2.0877, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 161/300, Train Loss: 0.3627, Acc: 0.8140, F1: 0.6735, AUC: 0.7631\n",
      "Epoch 161/300, Valid Loss: 2.1922, Acc: 0.6977, F1: 0.3158, AUC: 0.5654\n",
      "Epoch 162/300, Train Loss: 0.4069, Acc: 0.7791, F1: 0.6346, AUC: 0.7381\n",
      "Epoch 162/300, Valid Loss: 1.8904, Acc: 0.4186, F1: 0.2857, AUC: 0.4090\n",
      "Epoch 163/300, Train Loss: 0.4275, Acc: 0.7733, F1: 0.6061, AUC: 0.7176\n",
      "Epoch 163/300, Valid Loss: 1.3395, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 164/300, Train Loss: 0.4064, Acc: 0.8081, F1: 0.6598, AUC: 0.7535\n",
      "Epoch 164/300, Valid Loss: 1.4786, Acc: 0.5116, F1: 0.3636, AUC: 0.4974\n",
      "Epoch 165/300, Train Loss: 0.3937, Acc: 0.7907, F1: 0.6327, AUC: 0.7356\n",
      "Epoch 165/300, Valid Loss: 1.4052, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 166/300, Train Loss: 0.4120, Acc: 0.8023, F1: 0.6852, AUC: 0.7766\n",
      "Epoch 166/300, Valid Loss: 1.4118, Acc: 0.5581, F1: 0.3448, AUC: 0.5090\n",
      "Epoch 167/300, Train Loss: 0.4139, Acc: 0.8256, F1: 0.7273, AUC: 0.8096\n",
      "Epoch 167/300, Valid Loss: 1.4832, Acc: 0.7209, F1: 0.2500, AUC: 0.5603\n",
      "Epoch 168/300, Train Loss: 0.3906, Acc: 0.8198, F1: 0.6804, AUC: 0.7673\n",
      "Epoch 168/300, Valid Loss: 1.5026, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 169/300, Train Loss: 0.3213, Acc: 0.8430, F1: 0.7379, AUC: 0.8112\n",
      "Epoch 169/300, Valid Loss: 1.8506, Acc: 0.5581, F1: 0.2963, AUC: 0.4872\n",
      "Epoch 170/300, Train Loss: 0.3657, Acc: 0.8663, F1: 0.7723, AUC: 0.8333\n",
      "Epoch 170/300, Valid Loss: 1.5492, Acc: 0.6279, F1: 0.4667, AUC: 0.6026\n",
      "Epoch 171/300, Train Loss: 0.4416, Acc: 0.8023, F1: 0.6531, AUC: 0.7494\n",
      "Epoch 171/300, Valid Loss: 1.3276, Acc: 0.6977, F1: 0.3810, AUC: 0.5872\n",
      "Epoch 172/300, Train Loss: 0.3485, Acc: 0.7849, F1: 0.5934, AUC: 0.7096\n",
      "Epoch 172/300, Valid Loss: 1.5975, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 173/300, Train Loss: 0.3226, Acc: 0.8663, F1: 0.7850, AUC: 0.8497\n",
      "Epoch 173/300, Valid Loss: 2.0235, Acc: 0.6977, F1: 0.1333, AUC: 0.5218\n",
      "Epoch 174/300, Train Loss: 0.3829, Acc: 0.7965, F1: 0.6602, AUC: 0.7561\n",
      "Epoch 174/300, Valid Loss: 1.7227, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 175/300, Train Loss: 0.3971, Acc: 0.7965, F1: 0.6789, AUC: 0.7724\n",
      "Epoch 175/300, Valid Loss: 1.7089, Acc: 0.4884, F1: 0.3529, AUC: 0.4808\n",
      "Epoch 176/300, Train Loss: 0.4412, Acc: 0.8081, F1: 0.6972, AUC: 0.7862\n",
      "Epoch 176/300, Valid Loss: 1.9655, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 177/300, Train Loss: 0.3607, Acc: 0.8314, F1: 0.7290, AUC: 0.8083\n",
      "Epoch 177/300, Valid Loss: 1.9785, Acc: 0.6512, F1: 0.2105, AUC: 0.5103\n",
      "Epoch 178/300, Train Loss: 0.3568, Acc: 0.8605, F1: 0.7647, AUC: 0.8292\n",
      "Epoch 178/300, Valid Loss: 1.7984, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 179/300, Train Loss: 0.3636, Acc: 0.8663, F1: 0.7928, AUC: 0.8606\n",
      "Epoch 179/300, Valid Loss: 2.2709, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 180/300, Train Loss: 0.3955, Acc: 0.8372, F1: 0.7500, AUC: 0.8288\n",
      "Epoch 180/300, Valid Loss: 1.8775, Acc: 0.5814, F1: 0.3571, AUC: 0.5256\n",
      "Epoch 181/300, Train Loss: 0.3155, Acc: 0.8605, F1: 0.7736, AUC: 0.8401\n",
      "Epoch 181/300, Valid Loss: 2.1071, Acc: 0.5116, F1: 0.3636, AUC: 0.4974\n",
      "Epoch 182/300, Train Loss: 0.3417, Acc: 0.8605, F1: 0.7736, AUC: 0.8401\n",
      "Epoch 182/300, Valid Loss: 1.8972, Acc: 0.5116, F1: 0.3226, AUC: 0.4756\n",
      "Epoch 183/300, Train Loss: 0.3303, Acc: 0.8198, F1: 0.7103, AUC: 0.7946\n",
      "Epoch 183/300, Valid Loss: 1.8640, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 184/300, Train Loss: 0.3330, Acc: 0.8372, F1: 0.7407, AUC: 0.8179\n",
      "Epoch 184/300, Valid Loss: 2.1714, Acc: 0.5581, F1: 0.1739, AUC: 0.4436\n",
      "Epoch 185/300, Train Loss: 0.4290, Acc: 0.8198, F1: 0.7156, AUC: 0.8000\n",
      "Epoch 185/300, Valid Loss: 1.6231, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 186/300, Train Loss: 0.3469, Acc: 0.8256, F1: 0.7170, AUC: 0.7987\n",
      "Epoch 186/300, Valid Loss: 1.6523, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 187/300, Train Loss: 0.2900, Acc: 0.8779, F1: 0.8073, AUC: 0.8689\n",
      "Epoch 187/300, Valid Loss: 1.9344, Acc: 0.6279, F1: 0.2000, AUC: 0.4936\n",
      "Epoch 188/300, Train Loss: 0.3679, Acc: 0.8721, F1: 0.7925, AUC: 0.8538\n",
      "Epoch 188/300, Valid Loss: 1.7836, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 189/300, Train Loss: 0.3488, Acc: 0.8488, F1: 0.7400, AUC: 0.8099\n",
      "Epoch 189/300, Valid Loss: 1.7458, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 190/300, Train Loss: 0.3528, Acc: 0.8023, F1: 0.6600, AUC: 0.7548\n",
      "Epoch 190/300, Valid Loss: 1.7866, Acc: 0.7209, F1: 0.2500, AUC: 0.5603\n",
      "Epoch 191/300, Train Loss: 0.3554, Acc: 0.8140, F1: 0.6596, AUC: 0.7522\n",
      "Epoch 191/300, Valid Loss: 1.6124, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 192/300, Train Loss: 0.3145, Acc: 0.8430, F1: 0.7429, AUC: 0.8167\n",
      "Epoch 192/300, Valid Loss: 1.5859, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 193/300, Train Loss: 0.3693, Acc: 0.8372, F1: 0.7407, AUC: 0.8179\n",
      "Epoch 193/300, Valid Loss: 1.4317, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 194/300, Train Loss: 0.3275, Acc: 0.8547, F1: 0.7525, AUC: 0.8196\n",
      "Epoch 194/300, Valid Loss: 1.6254, Acc: 0.6047, F1: 0.3200, AUC: 0.5205\n",
      "Epoch 195/300, Train Loss: 0.3239, Acc: 0.8837, F1: 0.8148, AUC: 0.8731\n",
      "Epoch 195/300, Valid Loss: 1.8115, Acc: 0.7209, F1: 0.3333, AUC: 0.5821\n",
      "Epoch 196/300, Train Loss: 0.3055, Acc: 0.8663, F1: 0.7850, AUC: 0.8497\n",
      "Epoch 196/300, Valid Loss: 1.7121, Acc: 0.5814, F1: 0.3077, AUC: 0.5038\n",
      "Epoch 197/300, Train Loss: 0.3127, Acc: 0.8372, F1: 0.7255, AUC: 0.8016\n",
      "Epoch 197/300, Valid Loss: 1.6987, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 198/300, Train Loss: 0.4566, Acc: 0.7500, F1: 0.5905, AUC: 0.7064\n",
      "Epoch 198/300, Valid Loss: 1.5673, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 199/300, Train Loss: 0.3453, Acc: 0.8256, F1: 0.7273, AUC: 0.8096\n",
      "Epoch 199/300, Valid Loss: 1.5989, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 200/300, Train Loss: 0.3490, Acc: 0.8140, F1: 0.7037, AUC: 0.7904\n",
      "Epoch 200/300, Valid Loss: 1.8708, Acc: 0.6744, F1: 0.4167, AUC: 0.5923\n",
      "Epoch 201/300, Train Loss: 0.3423, Acc: 0.8953, F1: 0.8364, AUC: 0.8923\n",
      "Epoch 201/300, Valid Loss: 1.6741, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 202/300, Train Loss: 0.3648, Acc: 0.8547, F1: 0.7664, AUC: 0.8359\n",
      "Epoch 202/300, Valid Loss: 1.7799, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 203/300, Train Loss: 0.3173, Acc: 0.8198, F1: 0.6931, AUC: 0.7782\n",
      "Epoch 203/300, Valid Loss: 1.6928, Acc: 0.5814, F1: 0.2500, AUC: 0.4821\n",
      "Epoch 204/300, Train Loss: 0.3292, Acc: 0.8547, F1: 0.7706, AUC: 0.8413\n",
      "Epoch 204/300, Valid Loss: 2.0166, Acc: 0.6279, F1: 0.1111, AUC: 0.4718\n",
      "Epoch 205/300, Train Loss: 0.4729, Acc: 0.8314, F1: 0.7129, AUC: 0.7920\n",
      "Epoch 205/300, Valid Loss: 1.6636, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 206/300, Train Loss: 0.2972, Acc: 0.8605, F1: 0.7600, AUC: 0.8237\n",
      "Epoch 206/300, Valid Loss: 1.6409, Acc: 0.6279, F1: 0.3846, AUC: 0.5590\n",
      "Epoch 207/300, Train Loss: 0.2694, Acc: 0.9012, F1: 0.8440, AUC: 0.8965\n",
      "Epoch 207/300, Valid Loss: 1.6518, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 208/300, Train Loss: 0.4189, Acc: 0.7965, F1: 0.6847, AUC: 0.7779\n",
      "Epoch 208/300, Valid Loss: 1.6560, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 209/300, Train Loss: 0.3312, Acc: 0.8430, F1: 0.7652, AUC: 0.8439\n",
      "Epoch 209/300, Valid Loss: 1.7259, Acc: 0.4884, F1: 0.3529, AUC: 0.4808\n",
      "Epoch 210/300, Train Loss: 0.3227, Acc: 0.8430, F1: 0.7652, AUC: 0.8439\n",
      "Epoch 210/300, Valid Loss: 1.8167, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 211/300, Train Loss: 0.3873, Acc: 0.7965, F1: 0.7107, AUC: 0.8051\n",
      "Epoch 211/300, Valid Loss: 1.3391, Acc: 0.6512, F1: 0.4828, AUC: 0.6192\n",
      "Epoch 212/300, Train Loss: 0.3763, Acc: 0.8081, F1: 0.7179, AUC: 0.8080\n",
      "Epoch 212/300, Valid Loss: 1.7668, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 213/300, Train Loss: 0.3617, Acc: 0.8198, F1: 0.7350, AUC: 0.8218\n",
      "Epoch 213/300, Valid Loss: 1.7512, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 214/300, Train Loss: 0.3002, Acc: 0.8605, F1: 0.7931, AUC: 0.8673\n",
      "Epoch 214/300, Valid Loss: 2.0423, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 215/300, Train Loss: 0.3134, Acc: 0.8488, F1: 0.7719, AUC: 0.8481\n",
      "Epoch 215/300, Valid Loss: 2.1042, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 216/300, Train Loss: 0.3460, Acc: 0.8488, F1: 0.7500, AUC: 0.8208\n",
      "Epoch 216/300, Valid Loss: 1.8005, Acc: 0.7209, F1: 0.3333, AUC: 0.5821\n",
      "Epoch 217/300, Train Loss: 0.3745, Acc: 0.8140, F1: 0.7091, AUC: 0.7958\n",
      "Epoch 217/300, Valid Loss: 1.4393, Acc: 0.6512, F1: 0.2105, AUC: 0.5103\n",
      "Epoch 218/300, Train Loss: 0.3744, Acc: 0.8023, F1: 0.6964, AUC: 0.7875\n",
      "Epoch 218/300, Valid Loss: 1.7376, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 219/300, Train Loss: 0.3326, Acc: 0.8314, F1: 0.7339, AUC: 0.8138\n",
      "Epoch 219/300, Valid Loss: 1.1045, Acc: 0.7442, F1: 0.5600, AUC: 0.6859\n",
      "Epoch 220/300, Train Loss: 0.3834, Acc: 0.8081, F1: 0.7130, AUC: 0.8026\n",
      "Epoch 220/300, Valid Loss: 1.1330, Acc: 0.7442, F1: 0.4762, AUC: 0.6423\n",
      "Epoch 221/300, Train Loss: 0.3393, Acc: 0.8198, F1: 0.6931, AUC: 0.7782\n",
      "Epoch 221/300, Valid Loss: 0.9747, Acc: 0.6512, F1: 0.3478, AUC: 0.5538\n",
      "Epoch 222/300, Train Loss: 0.3397, Acc: 0.8605, F1: 0.7895, AUC: 0.8619\n",
      "Epoch 222/300, Valid Loss: 1.1255, Acc: 0.6279, F1: 0.4667, AUC: 0.6026\n",
      "Epoch 223/300, Train Loss: 0.3533, Acc: 0.8430, F1: 0.7429, AUC: 0.8167\n",
      "Epoch 223/300, Valid Loss: 0.9975, Acc: 0.6977, F1: 0.4348, AUC: 0.6090\n",
      "Epoch 224/300, Train Loss: 0.3043, Acc: 0.8547, F1: 0.7664, AUC: 0.8359\n",
      "Epoch 224/300, Valid Loss: 1.1145, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 225/300, Train Loss: 0.3346, Acc: 0.8721, F1: 0.8070, AUC: 0.8756\n",
      "Epoch 225/300, Valid Loss: 1.4828, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 226/300, Train Loss: 0.3141, Acc: 0.8488, F1: 0.7636, AUC: 0.8372\n",
      "Epoch 226/300, Valid Loss: 1.4690, Acc: 0.7209, F1: 0.2500, AUC: 0.5603\n",
      "Epoch 227/300, Train Loss: 0.3338, Acc: 0.8488, F1: 0.7636, AUC: 0.8372\n",
      "Epoch 227/300, Valid Loss: 1.3706, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 228/300, Train Loss: 0.3215, Acc: 0.8663, F1: 0.8000, AUC: 0.8715\n",
      "Epoch 228/300, Valid Loss: 1.2786, Acc: 0.6977, F1: 0.3158, AUC: 0.5654\n",
      "Epoch 229/300, Train Loss: 0.3758, Acc: 0.8663, F1: 0.7928, AUC: 0.8606\n",
      "Epoch 229/300, Valid Loss: 1.1757, Acc: 0.5581, F1: 0.3871, AUC: 0.5308\n",
      "Epoch 230/300, Train Loss: 0.3425, Acc: 0.8547, F1: 0.7788, AUC: 0.8522\n",
      "Epoch 230/300, Valid Loss: 1.4062, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 231/300, Train Loss: 0.3006, Acc: 0.8372, F1: 0.7358, AUC: 0.8125\n",
      "Epoch 231/300, Valid Loss: 1.2706, Acc: 0.5349, F1: 0.4444, AUC: 0.5577\n",
      "Epoch 232/300, Train Loss: 0.3308, Acc: 0.8430, F1: 0.7611, AUC: 0.8385\n",
      "Epoch 232/300, Valid Loss: 1.1620, Acc: 0.6977, F1: 0.4348, AUC: 0.6090\n",
      "Epoch 233/300, Train Loss: 0.4252, Acc: 0.8081, F1: 0.7080, AUC: 0.7971\n",
      "Epoch 233/300, Valid Loss: 0.8211, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 234/300, Train Loss: 0.4051, Acc: 0.7849, F1: 0.6838, AUC: 0.7804\n",
      "Epoch 234/300, Valid Loss: 1.3098, Acc: 0.5349, F1: 0.2857, AUC: 0.4705\n",
      "Epoch 235/300, Train Loss: 0.2839, Acc: 0.8779, F1: 0.8037, AUC: 0.8635\n",
      "Epoch 235/300, Valid Loss: 1.5581, Acc: 0.5349, F1: 0.2857, AUC: 0.4705\n",
      "Epoch 236/300, Train Loss: 0.2767, Acc: 0.8953, F1: 0.8393, AUC: 0.8978\n",
      "Epoch 236/300, Valid Loss: 1.3613, Acc: 0.6512, F1: 0.3478, AUC: 0.5538\n",
      "Epoch 237/300, Train Loss: 0.3051, Acc: 0.8430, F1: 0.7568, AUC: 0.8330\n",
      "Epoch 237/300, Valid Loss: 1.2617, Acc: 0.6744, F1: 0.4167, AUC: 0.5923\n",
      "Epoch 238/300, Train Loss: 0.2583, Acc: 0.8837, F1: 0.8246, AUC: 0.8894\n",
      "Epoch 238/300, Valid Loss: 2.5756, Acc: 0.7442, F1: 0.2667, AUC: 0.5769\n",
      "Epoch 239/300, Train Loss: 0.3642, Acc: 0.8547, F1: 0.7826, AUC: 0.8577\n",
      "Epoch 239/300, Valid Loss: 1.6300, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 240/300, Train Loss: 0.2766, Acc: 0.8721, F1: 0.7925, AUC: 0.8538\n",
      "Epoch 240/300, Valid Loss: 1.3863, Acc: 0.6512, F1: 0.2105, AUC: 0.5103\n",
      "Epoch 241/300, Train Loss: 0.3005, Acc: 0.8779, F1: 0.8073, AUC: 0.8689\n",
      "Epoch 241/300, Valid Loss: 1.6196, Acc: 0.6279, F1: 0.2727, AUC: 0.5154\n",
      "Epoch 242/300, Train Loss: 0.3277, Acc: 0.8663, F1: 0.7723, AUC: 0.8333\n",
      "Epoch 242/300, Valid Loss: 1.2622, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 243/300, Train Loss: 0.3721, Acc: 0.8198, F1: 0.7207, AUC: 0.8054\n",
      "Epoch 243/300, Valid Loss: 1.2777, Acc: 0.6744, F1: 0.4167, AUC: 0.5923\n",
      "Epoch 244/300, Train Loss: 0.3034, Acc: 0.8663, F1: 0.7928, AUC: 0.8606\n",
      "Epoch 244/300, Valid Loss: 1.4640, Acc: 0.4419, F1: 0.4000, AUC: 0.4910\n",
      "Epoch 245/300, Train Loss: 0.2909, Acc: 0.8663, F1: 0.7965, AUC: 0.8660\n",
      "Epoch 245/300, Valid Loss: 1.3725, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 246/300, Train Loss: 0.2546, Acc: 0.9244, F1: 0.8807, AUC: 0.9240\n",
      "Epoch 246/300, Valid Loss: 1.7944, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 247/300, Train Loss: 0.3114, Acc: 0.8721, F1: 0.8000, AUC: 0.8647\n",
      "Epoch 247/300, Valid Loss: 1.3175, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 248/300, Train Loss: 0.2939, Acc: 0.8779, F1: 0.8000, AUC: 0.8580\n",
      "Epoch 248/300, Valid Loss: 1.5699, Acc: 0.5814, F1: 0.4375, AUC: 0.5692\n",
      "Epoch 249/300, Train Loss: 0.2829, Acc: 0.8895, F1: 0.8224, AUC: 0.8772\n",
      "Epoch 249/300, Valid Loss: 1.7185, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 250/300, Train Loss: 0.2310, Acc: 0.9012, F1: 0.8350, AUC: 0.8801\n",
      "Epoch 250/300, Valid Loss: 2.1847, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 251/300, Train Loss: 0.3254, Acc: 0.8547, F1: 0.7664, AUC: 0.8359\n",
      "Epoch 251/300, Valid Loss: 1.5613, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 252/300, Train Loss: 0.2887, Acc: 0.8663, F1: 0.7810, AUC: 0.8442\n",
      "Epoch 252/300, Valid Loss: 1.3883, Acc: 0.6744, F1: 0.4167, AUC: 0.5923\n",
      "Epoch 253/300, Train Loss: 0.3179, Acc: 0.8430, F1: 0.7652, AUC: 0.8439\n",
      "Epoch 253/300, Valid Loss: 1.7096, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 254/300, Train Loss: 0.3491, Acc: 0.8256, F1: 0.7368, AUC: 0.8205\n",
      "Epoch 254/300, Valid Loss: 1.4632, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 255/300, Train Loss: 0.2758, Acc: 0.9128, F1: 0.8598, AUC: 0.9048\n",
      "Epoch 255/300, Valid Loss: 1.3601, Acc: 0.6279, F1: 0.3846, AUC: 0.5590\n",
      "Epoch 256/300, Train Loss: 0.3173, Acc: 0.8721, F1: 0.8000, AUC: 0.8647\n",
      "Epoch 256/300, Valid Loss: 1.2503, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 257/300, Train Loss: 0.2598, Acc: 0.8837, F1: 0.8182, AUC: 0.8785\n",
      "Epoch 257/300, Valid Loss: 1.7826, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 258/300, Train Loss: 0.1877, Acc: 0.9302, F1: 0.8929, AUC: 0.9391\n",
      "Epoch 258/300, Valid Loss: 2.2255, Acc: 0.6977, F1: 0.3158, AUC: 0.5654\n",
      "Epoch 259/300, Train Loss: 0.2880, Acc: 0.8663, F1: 0.8000, AUC: 0.8715\n",
      "Epoch 259/300, Valid Loss: 2.7293, Acc: 0.7209, F1: 0.2500, AUC: 0.5603\n",
      "Epoch 260/300, Train Loss: 0.3422, Acc: 0.8314, F1: 0.7478, AUC: 0.8301\n",
      "Epoch 260/300, Valid Loss: 1.9455, Acc: 0.6279, F1: 0.2727, AUC: 0.5154\n",
      "Epoch 261/300, Train Loss: 0.2498, Acc: 0.8895, F1: 0.8257, AUC: 0.8827\n",
      "Epoch 261/300, Valid Loss: 1.6268, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 262/300, Train Loss: 0.2750, Acc: 0.8605, F1: 0.7895, AUC: 0.8619\n",
      "Epoch 262/300, Valid Loss: 1.4849, Acc: 0.7209, F1: 0.4545, AUC: 0.6256\n",
      "Epoch 263/300, Train Loss: 0.3195, Acc: 0.8314, F1: 0.7434, AUC: 0.8247\n",
      "Epoch 263/300, Valid Loss: 1.7435, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 264/300, Train Loss: 0.2972, Acc: 0.8779, F1: 0.8142, AUC: 0.8798\n",
      "Epoch 264/300, Valid Loss: 1.5796, Acc: 0.4186, F1: 0.3902, AUC: 0.4744\n",
      "Epoch 265/300, Train Loss: 0.3871, Acc: 0.8430, F1: 0.7477, AUC: 0.8221\n",
      "Epoch 265/300, Valid Loss: 1.6218, Acc: 0.6744, F1: 0.2222, AUC: 0.5269\n",
      "Epoch 266/300, Train Loss: 0.3955, Acc: 0.8023, F1: 0.6792, AUC: 0.7712\n",
      "Epoch 266/300, Valid Loss: 1.5012, Acc: 0.4651, F1: 0.3784, AUC: 0.4859\n",
      "Epoch 267/300, Train Loss: 0.3404, Acc: 0.8488, F1: 0.7679, AUC: 0.8426\n",
      "Epoch 267/300, Valid Loss: 1.9955, Acc: 0.6977, F1: 0.3810, AUC: 0.5872\n",
      "Epoch 268/300, Train Loss: 0.3010, Acc: 0.8605, F1: 0.7857, AUC: 0.8564\n",
      "Epoch 268/300, Valid Loss: 2.3181, Acc: 0.7442, F1: 0.3529, AUC: 0.5987\n",
      "Epoch 269/300, Train Loss: 0.3159, Acc: 0.8663, F1: 0.7928, AUC: 0.8606\n",
      "Epoch 269/300, Valid Loss: 1.5457, Acc: 0.5116, F1: 0.2222, AUC: 0.4321\n",
      "Epoch 270/300, Train Loss: 0.3084, Acc: 0.8895, F1: 0.8257, AUC: 0.8827\n",
      "Epoch 270/300, Valid Loss: 1.8268, Acc: 0.6047, F1: 0.2609, AUC: 0.4987\n",
      "Epoch 271/300, Train Loss: 0.3216, Acc: 0.8372, F1: 0.7627, AUC: 0.8452\n",
      "Epoch 271/300, Valid Loss: 1.9528, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 272/300, Train Loss: 0.2810, Acc: 0.8953, F1: 0.8393, AUC: 0.8978\n",
      "Epoch 272/300, Valid Loss: 1.8363, Acc: 0.6512, F1: 0.3478, AUC: 0.5538\n",
      "Epoch 273/300, Train Loss: 0.2823, Acc: 0.8663, F1: 0.7965, AUC: 0.8660\n",
      "Epoch 273/300, Valid Loss: 1.6225, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 274/300, Train Loss: 0.2639, Acc: 0.8779, F1: 0.8108, AUC: 0.8744\n",
      "Epoch 274/300, Valid Loss: 1.9260, Acc: 0.4419, F1: 0.4000, AUC: 0.4910\n",
      "Epoch 275/300, Train Loss: 0.4232, Acc: 0.7791, F1: 0.6481, AUC: 0.7490\n",
      "Epoch 275/300, Valid Loss: 1.4304, Acc: 0.6047, F1: 0.4516, AUC: 0.5859\n",
      "Epoch 276/300, Train Loss: 0.3214, Acc: 0.8721, F1: 0.7885, AUC: 0.8484\n",
      "Epoch 276/300, Valid Loss: 1.7325, Acc: 0.6047, F1: 0.3704, AUC: 0.5423\n",
      "Epoch 277/300, Train Loss: 0.3635, Acc: 0.8023, F1: 0.7069, AUC: 0.7984\n",
      "Epoch 277/300, Valid Loss: 1.5351, Acc: 0.7209, F1: 0.5000, AUC: 0.6474\n",
      "Epoch 278/300, Train Loss: 0.3806, Acc: 0.8198, F1: 0.7103, AUC: 0.7946\n",
      "Epoch 278/300, Valid Loss: 1.3990, Acc: 0.7209, F1: 0.4000, AUC: 0.6038\n",
      "Epoch 279/300, Train Loss: 0.3589, Acc: 0.8314, F1: 0.7434, AUC: 0.8247\n",
      "Epoch 279/300, Valid Loss: 1.3286, Acc: 0.6744, F1: 0.4615, AUC: 0.6141\n",
      "Epoch 280/300, Train Loss: 0.3412, Acc: 0.8256, F1: 0.7368, AUC: 0.8205\n",
      "Epoch 280/300, Valid Loss: 1.3621, Acc: 0.6977, F1: 0.4800, AUC: 0.6308\n",
      "Epoch 281/300, Train Loss: 0.3046, Acc: 0.8605, F1: 0.7818, AUC: 0.8510\n",
      "Epoch 281/300, Valid Loss: 1.9493, Acc: 0.5116, F1: 0.2759, AUC: 0.4538\n",
      "Epoch 282/300, Train Loss: 0.3381, Acc: 0.8488, F1: 0.7797, AUC: 0.8590\n",
      "Epoch 282/300, Valid Loss: 2.0815, Acc: 0.7209, F1: 0.3333, AUC: 0.5821\n",
      "Epoch 283/300, Train Loss: 0.4097, Acc: 0.7907, F1: 0.7188, AUC: 0.8173\n",
      "Epoch 283/300, Valid Loss: 2.1598, Acc: 0.5581, F1: 0.3448, AUC: 0.5090\n",
      "Epoch 284/300, Train Loss: 0.3510, Acc: 0.8256, F1: 0.7458, AUC: 0.8314\n",
      "Epoch 284/300, Valid Loss: 2.0417, Acc: 0.7442, F1: 0.5217, AUC: 0.6641\n",
      "Epoch 285/300, Train Loss: 0.4046, Acc: 0.7791, F1: 0.6984, AUC: 0.7981\n",
      "Epoch 285/300, Valid Loss: 1.9119, Acc: 0.4419, F1: 0.2941, AUC: 0.4256\n",
      "Epoch 286/300, Train Loss: 0.4276, Acc: 0.7442, F1: 0.6857, AUC: 0.7949\n",
      "Epoch 286/300, Valid Loss: 1.8957, Acc: 0.4884, F1: 0.3125, AUC: 0.4590\n",
      "Epoch 287/300, Train Loss: 0.4266, Acc: 0.7558, F1: 0.6912, AUC: 0.7978\n",
      "Epoch 287/300, Valid Loss: 1.6152, Acc: 0.4419, F1: 0.3684, AUC: 0.4692\n",
      "Epoch 288/300, Train Loss: 0.4481, Acc: 0.6919, F1: 0.6395, AUC: 0.7519\n",
      "Epoch 288/300, Valid Loss: 1.4929, Acc: 0.3488, F1: 0.3636, AUC: 0.4244\n",
      "Epoch 289/300, Train Loss: 0.4923, Acc: 0.6512, F1: 0.6154, AUC: 0.7282\n",
      "Epoch 289/300, Valid Loss: 1.7047, Acc: 0.4651, F1: 0.3030, AUC: 0.4423\n",
      "Epoch 290/300, Train Loss: 0.4595, Acc: 0.7326, F1: 0.6806, AUC: 0.7920\n",
      "Epoch 290/300, Valid Loss: 1.4563, Acc: 0.4884, F1: 0.3529, AUC: 0.4808\n",
      "Epoch 291/300, Train Loss: 0.4137, Acc: 0.7209, F1: 0.6522, AUC: 0.7619\n",
      "Epoch 291/300, Valid Loss: 1.4235, Acc: 0.4419, F1: 0.4286, AUC: 0.5128\n",
      "Epoch 292/300, Train Loss: 0.4220, Acc: 0.7384, F1: 0.6565, AUC: 0.7635\n",
      "Epoch 292/300, Valid Loss: 1.4932, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 293/300, Train Loss: 0.3618, Acc: 0.8314, F1: 0.7521, AUC: 0.8356\n",
      "Epoch 293/300, Valid Loss: 1.7443, Acc: 0.4651, F1: 0.3030, AUC: 0.4423\n",
      "Epoch 294/300, Train Loss: 0.3600, Acc: 0.8198, F1: 0.7350, AUC: 0.8218\n",
      "Epoch 294/300, Valid Loss: 1.6251, Acc: 0.4419, F1: 0.4000, AUC: 0.4910\n",
      "Epoch 295/300, Train Loss: 0.3722, Acc: 0.8430, F1: 0.7652, AUC: 0.8439\n",
      "Epoch 295/300, Valid Loss: 1.7781, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 296/300, Train Loss: 0.3417, Acc: 0.8430, F1: 0.7523, AUC: 0.8276\n",
      "Epoch 296/300, Valid Loss: 1.6724, Acc: 0.4419, F1: 0.4000, AUC: 0.4910\n",
      "Epoch 297/300, Train Loss: 0.3312, Acc: 0.8663, F1: 0.7810, AUC: 0.8442\n",
      "Epoch 297/300, Valid Loss: 1.4908, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 298/300, Train Loss: 0.3549, Acc: 0.8198, F1: 0.7257, AUC: 0.8109\n",
      "Epoch 298/300, Valid Loss: 1.8065, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 299/300, Train Loss: 0.3296, Acc: 0.8721, F1: 0.8000, AUC: 0.8647\n",
      "Epoch 299/300, Valid Loss: 1.4816, Acc: 0.6977, F1: 0.3810, AUC: 0.5872\n",
      "Epoch 300/300, Train Loss: 0.3123, Acc: 0.8547, F1: 0.7664, AUC: 0.8359\n",
      "Epoch 300/300, Valid Loss: 1.7060, Acc: 0.7209, F1: 0.4000, AUC: 0.6038\n",
      "-- Fold 3/5 --\n",
      "Epoch 1/300, Train Loss: 0.6690, Acc: 0.7035, F1: 0.1053, AUC: 0.5103\n",
      "Epoch 1/300, Valid Loss: 0.7044, Acc: 0.4186, F1: 0.4681, AUC: 0.4734\n",
      "Epoch 2/300, Train Loss: 0.6364, Acc: 0.6744, F1: 0.1250, AUC: 0.4961\n",
      "Epoch 2/300, Valid Loss: 0.6807, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Train Loss: 0.6459, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6689, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Train Loss: 0.6140, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Valid Loss: 0.7516, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Train Loss: 0.6130, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.7501, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Train Loss: 0.6109, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.6788, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Train Loss: 0.6128, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.7369, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Train Loss: 0.6021, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.6842, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Train Loss: 0.6051, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.7096, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Train Loss: 0.5958, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.6807, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Train Loss: 0.5922, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.7062, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Train Loss: 0.6067, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.6844, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Train Loss: 0.5993, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.7246, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Train Loss: 0.5863, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.6935, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Train Loss: 0.5944, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.7436, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Train Loss: 0.5916, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.6945, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Train Loss: 0.6063, Acc: 0.7151, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.6898, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Train Loss: 0.5866, Acc: 0.7267, F1: 0.0784, AUC: 0.5204\n",
      "Epoch 18/300, Valid Loss: 0.7326, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Train Loss: 0.5967, Acc: 0.7326, F1: 0.1154, AUC: 0.5306\n",
      "Epoch 19/300, Valid Loss: 0.7121, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 20/300, Train Loss: 0.5982, Acc: 0.7151, F1: 0.0392, AUC: 0.5061\n",
      "Epoch 20/300, Valid Loss: 0.6748, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Train Loss: 0.5903, Acc: 0.7267, F1: 0.0784, AUC: 0.5204\n",
      "Epoch 21/300, Valid Loss: 0.7374, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Train Loss: 0.5736, Acc: 0.7151, F1: 0.0755, AUC: 0.5123\n",
      "Epoch 22/300, Valid Loss: 0.6807, Acc: 0.6279, F1: 0.1111, AUC: 0.5127\n",
      "Epoch 23/300, Train Loss: 0.5778, Acc: 0.7035, F1: 0.1356, AUC: 0.5164\n",
      "Epoch 23/300, Valid Loss: 0.7077, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Train Loss: 0.5783, Acc: 0.7209, F1: 0.0769, AUC: 0.5163\n",
      "Epoch 24/300, Valid Loss: 0.7332, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Train Loss: 0.5817, Acc: 0.7209, F1: 0.0400, AUC: 0.5102\n",
      "Epoch 25/300, Valid Loss: 0.7047, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 26/300, Train Loss: 0.5946, Acc: 0.7151, F1: 0.1695, AUC: 0.5307\n",
      "Epoch 26/300, Valid Loss: 0.6710, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 27/300, Train Loss: 0.5972, Acc: 0.7035, F1: 0.0377, AUC: 0.4980\n",
      "Epoch 27/300, Valid Loss: 0.7049, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Train Loss: 0.5662, Acc: 0.7267, F1: 0.1132, AUC: 0.5265\n",
      "Epoch 28/300, Valid Loss: 0.7292, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Train Loss: 0.5714, Acc: 0.7209, F1: 0.2258, AUC: 0.5470\n",
      "Epoch 29/300, Valid Loss: 0.8162, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Train Loss: 0.5976, Acc: 0.7035, F1: 0.0727, AUC: 0.5041\n",
      "Epoch 30/300, Valid Loss: 0.6867, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Train Loss: 0.5840, Acc: 0.7209, F1: 0.1111, AUC: 0.5225\n",
      "Epoch 31/300, Valid Loss: 0.6986, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Train Loss: 0.5701, Acc: 0.7267, F1: 0.1455, AUC: 0.5327\n",
      "Epoch 32/300, Valid Loss: 0.7177, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Train Loss: 0.5816, Acc: 0.7093, F1: 0.0741, AUC: 0.5082\n",
      "Epoch 33/300, Valid Loss: 0.6943, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Train Loss: 0.5591, Acc: 0.7326, F1: 0.1481, AUC: 0.5368\n",
      "Epoch 34/300, Valid Loss: 0.7241, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Train Loss: 0.5709, Acc: 0.7151, F1: 0.1091, AUC: 0.5184\n",
      "Epoch 35/300, Valid Loss: 0.6974, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Train Loss: 0.5843, Acc: 0.7151, F1: 0.1404, AUC: 0.5246\n",
      "Epoch 36/300, Valid Loss: 0.7315, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Train Loss: 0.5596, Acc: 0.7384, F1: 0.2623, AUC: 0.5654\n",
      "Epoch 37/300, Valid Loss: 0.6761, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Train Loss: 0.5939, Acc: 0.7326, F1: 0.1786, AUC: 0.5429\n",
      "Epoch 38/300, Valid Loss: 0.7003, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 39/300, Train Loss: 0.5794, Acc: 0.7035, F1: 0.1356, AUC: 0.5164\n",
      "Epoch 39/300, Valid Loss: 0.7104, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 40/300, Train Loss: 0.5815, Acc: 0.7093, F1: 0.1379, AUC: 0.5205\n",
      "Epoch 40/300, Valid Loss: 0.6986, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 41/300, Train Loss: 0.5551, Acc: 0.7267, F1: 0.1132, AUC: 0.5265\n",
      "Epoch 41/300, Valid Loss: 0.7964, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Train Loss: 0.5689, Acc: 0.7267, F1: 0.2034, AUC: 0.5450\n",
      "Epoch 42/300, Valid Loss: 0.7421, Acc: 0.5581, F1: 0.0000, AUC: 0.4444\n",
      "Epoch 43/300, Train Loss: 0.5601, Acc: 0.7326, F1: 0.2333, AUC: 0.5552\n",
      "Epoch 43/300, Valid Loss: 0.7135, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 44/300, Train Loss: 0.5573, Acc: 0.7151, F1: 0.2222, AUC: 0.5430\n",
      "Epoch 44/300, Valid Loss: 0.6859, Acc: 0.6279, F1: 0.1111, AUC: 0.5127\n",
      "Epoch 45/300, Train Loss: 0.5574, Acc: 0.6977, F1: 0.1613, AUC: 0.5185\n",
      "Epoch 45/300, Valid Loss: 0.6953, Acc: 0.5349, F1: 0.0000, AUC: 0.4259\n",
      "Epoch 46/300, Train Loss: 0.5656, Acc: 0.7384, F1: 0.3077, AUC: 0.5777\n",
      "Epoch 46/300, Valid Loss: 0.7447, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Train Loss: 0.5711, Acc: 0.7209, F1: 0.2500, AUC: 0.5532\n",
      "Epoch 47/300, Valid Loss: 0.7165, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Train Loss: 0.5696, Acc: 0.7035, F1: 0.1905, AUC: 0.5287\n",
      "Epoch 48/300, Valid Loss: 0.7708, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 49/300, Train Loss: 0.5256, Acc: 0.7500, F1: 0.2712, AUC: 0.5735\n",
      "Epoch 49/300, Valid Loss: 0.7048, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 50/300, Train Loss: 0.5824, Acc: 0.7035, F1: 0.2609, AUC: 0.5471\n",
      "Epoch 50/300, Valid Loss: 0.7281, Acc: 0.5814, F1: 0.1000, AUC: 0.4757\n",
      "Epoch 51/300, Train Loss: 0.5737, Acc: 0.7384, F1: 0.2857, AUC: 0.5715\n",
      "Epoch 51/300, Valid Loss: 0.7407, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 52/300, Train Loss: 0.5552, Acc: 0.7384, F1: 0.3478, AUC: 0.5899\n",
      "Epoch 52/300, Valid Loss: 0.7211, Acc: 0.4884, F1: 0.0833, AUC: 0.4016\n",
      "Epoch 53/300, Train Loss: 0.5461, Acc: 0.7558, F1: 0.4324, AUC: 0.6267\n",
      "Epoch 53/300, Valid Loss: 0.7819, Acc: 0.5814, F1: 0.1000, AUC: 0.4757\n",
      "Epoch 54/300, Train Loss: 0.5240, Acc: 0.7384, F1: 0.3478, AUC: 0.5899\n",
      "Epoch 54/300, Valid Loss: 0.7799, Acc: 0.5814, F1: 0.1000, AUC: 0.4757\n",
      "Epoch 55/300, Train Loss: 0.5358, Acc: 0.7442, F1: 0.2667, AUC: 0.5694\n",
      "Epoch 55/300, Valid Loss: 0.7292, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Train Loss: 0.5722, Acc: 0.7209, F1: 0.2500, AUC: 0.5532\n",
      "Epoch 56/300, Valid Loss: 0.7045, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Train Loss: 0.5321, Acc: 0.7267, F1: 0.4051, AUC: 0.6064\n",
      "Epoch 57/300, Valid Loss: 0.7472, Acc: 0.3953, F1: 0.2778, AUC: 0.3785\n",
      "Epoch 58/300, Train Loss: 0.5581, Acc: 0.7267, F1: 0.4471, AUC: 0.6248\n",
      "Epoch 58/300, Valid Loss: 0.7259, Acc: 0.5349, F1: 0.0909, AUC: 0.4387\n",
      "Epoch 59/300, Train Loss: 0.5522, Acc: 0.7442, F1: 0.4634, AUC: 0.6370\n",
      "Epoch 59/300, Valid Loss: 0.7679, Acc: 0.3953, F1: 0.2778, AUC: 0.3785\n",
      "Epoch 60/300, Train Loss: 0.5485, Acc: 0.7326, F1: 0.4651, AUC: 0.6350\n",
      "Epoch 60/300, Valid Loss: 0.7041, Acc: 0.4651, F1: 0.0800, AUC: 0.3831\n",
      "Epoch 61/300, Train Loss: 0.5374, Acc: 0.7616, F1: 0.4384, AUC: 0.6307\n",
      "Epoch 61/300, Valid Loss: 0.7429, Acc: 0.3256, F1: 0.2162, AUC: 0.3102\n",
      "Epoch 62/300, Train Loss: 0.5389, Acc: 0.7267, F1: 0.4719, AUC: 0.6370\n",
      "Epoch 62/300, Valid Loss: 0.7526, Acc: 0.4884, F1: 0.0833, AUC: 0.4016\n",
      "Epoch 63/300, Train Loss: 0.5320, Acc: 0.7384, F1: 0.5161, AUC: 0.6636\n",
      "Epoch 63/300, Valid Loss: 0.8017, Acc: 0.5349, F1: 0.0909, AUC: 0.4387\n",
      "Epoch 64/300, Train Loss: 0.5763, Acc: 0.6977, F1: 0.4091, AUC: 0.5983\n",
      "Epoch 64/300, Valid Loss: 0.7046, Acc: 0.4651, F1: 0.0000, AUC: 0.3704\n",
      "Epoch 65/300, Train Loss: 0.5236, Acc: 0.7907, F1: 0.5135, AUC: 0.6695\n",
      "Epoch 65/300, Valid Loss: 0.7666, Acc: 0.3953, F1: 0.1875, AUC: 0.3530\n",
      "Epoch 66/300, Train Loss: 0.5588, Acc: 0.7442, F1: 0.4500, AUC: 0.6308\n",
      "Epoch 66/300, Valid Loss: 0.7128, Acc: 0.4651, F1: 0.2069, AUC: 0.4086\n",
      "Epoch 67/300, Train Loss: 0.5513, Acc: 0.7326, F1: 0.3030, AUC: 0.5736\n",
      "Epoch 67/300, Valid Loss: 0.6854, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 68/300, Train Loss: 0.5319, Acc: 0.7326, F1: 0.4250, AUC: 0.6166\n",
      "Epoch 68/300, Valid Loss: 0.7498, Acc: 0.4651, F1: 0.0800, AUC: 0.3831\n",
      "Epoch 69/300, Train Loss: 0.5397, Acc: 0.7442, F1: 0.3333, AUC: 0.5879\n",
      "Epoch 69/300, Valid Loss: 0.7268, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 70/300, Train Loss: 0.5347, Acc: 0.7326, F1: 0.3429, AUC: 0.5859\n",
      "Epoch 70/300, Valid Loss: 0.8026, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 71/300, Train Loss: 0.5299, Acc: 0.7558, F1: 0.4000, AUC: 0.6144\n",
      "Epoch 71/300, Valid Loss: 0.7421, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Train Loss: 0.5261, Acc: 0.7733, F1: 0.4658, AUC: 0.6450\n",
      "Epoch 72/300, Valid Loss: 0.8718, Acc: 0.5116, F1: 0.0870, AUC: 0.4201\n",
      "Epoch 73/300, Train Loss: 0.5165, Acc: 0.7500, F1: 0.4557, AUC: 0.6349\n",
      "Epoch 73/300, Valid Loss: 0.8010, Acc: 0.4419, F1: 0.0769, AUC: 0.3646\n",
      "Epoch 74/300, Train Loss: 0.5368, Acc: 0.7616, F1: 0.4384, AUC: 0.6307\n",
      "Epoch 74/300, Valid Loss: 0.7391, Acc: 0.4884, F1: 0.0000, AUC: 0.3889\n",
      "Epoch 75/300, Train Loss: 0.5381, Acc: 0.7384, F1: 0.4000, AUC: 0.6083\n",
      "Epoch 75/300, Valid Loss: 0.7574, Acc: 0.4186, F1: 0.2857, AUC: 0.3970\n",
      "Epoch 76/300, Train Loss: 0.5008, Acc: 0.7733, F1: 0.5185, AUC: 0.6696\n",
      "Epoch 76/300, Valid Loss: 0.7951, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Train Loss: 0.5265, Acc: 0.7616, F1: 0.4384, AUC: 0.6307\n",
      "Epoch 77/300, Valid Loss: 0.7559, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 78/300, Train Loss: 0.5154, Acc: 0.7384, F1: 0.3478, AUC: 0.5899\n",
      "Epoch 78/300, Valid Loss: 0.7407, Acc: 0.5116, F1: 0.2222, AUC: 0.4456\n",
      "Epoch 79/300, Train Loss: 0.5047, Acc: 0.7733, F1: 0.4348, AUC: 0.6327\n",
      "Epoch 79/300, Valid Loss: 0.7665, Acc: 0.5349, F1: 0.0909, AUC: 0.4387\n",
      "Epoch 80/300, Train Loss: 0.5184, Acc: 0.7616, F1: 0.4384, AUC: 0.6307\n",
      "Epoch 80/300, Valid Loss: 0.7485, Acc: 0.4651, F1: 0.1481, AUC: 0.3958\n",
      "Epoch 81/300, Train Loss: 0.5176, Acc: 0.7558, F1: 0.5116, AUC: 0.6635\n",
      "Epoch 81/300, Valid Loss: 0.8282, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 82/300, Train Loss: 0.5192, Acc: 0.7326, F1: 0.3947, AUC: 0.6043\n",
      "Epoch 82/300, Valid Loss: 0.7802, Acc: 0.4186, F1: 0.2857, AUC: 0.3970\n",
      "Epoch 83/300, Train Loss: 0.4912, Acc: 0.7674, F1: 0.5349, AUC: 0.6778\n",
      "Epoch 83/300, Valid Loss: 0.7377, Acc: 0.4651, F1: 0.3030, AUC: 0.4340\n",
      "Epoch 84/300, Train Loss: 0.5587, Acc: 0.7209, F1: 0.5000, AUC: 0.6514\n",
      "Epoch 84/300, Valid Loss: 0.7233, Acc: 0.5814, F1: 0.1000, AUC: 0.4757\n",
      "Epoch 85/300, Train Loss: 0.4758, Acc: 0.7791, F1: 0.5476, AUC: 0.6859\n",
      "Epoch 85/300, Valid Loss: 0.7602, Acc: 0.5814, F1: 0.1000, AUC: 0.4757\n",
      "Epoch 86/300, Train Loss: 0.5029, Acc: 0.7326, F1: 0.5000, AUC: 0.6534\n",
      "Epoch 86/300, Valid Loss: 0.8558, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 87/300, Train Loss: 0.5102, Acc: 0.7326, F1: 0.5208, AUC: 0.6657\n",
      "Epoch 87/300, Valid Loss: 0.7034, Acc: 0.4186, F1: 0.1379, AUC: 0.3588\n",
      "Epoch 88/300, Train Loss: 0.4700, Acc: 0.7791, F1: 0.5250, AUC: 0.6736\n",
      "Epoch 88/300, Valid Loss: 0.7983, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 89/300, Train Loss: 0.5005, Acc: 0.7616, F1: 0.5176, AUC: 0.6676\n",
      "Epoch 89/300, Valid Loss: 0.6401, Acc: 0.5814, F1: 0.5714, AUC: 0.6157\n",
      "Epoch 90/300, Train Loss: 0.5053, Acc: 0.7733, F1: 0.5301, AUC: 0.6757\n",
      "Epoch 90/300, Valid Loss: 0.7491, Acc: 0.4884, F1: 0.0833, AUC: 0.4016\n",
      "Epoch 91/300, Train Loss: 0.4828, Acc: 0.7558, F1: 0.4878, AUC: 0.6512\n",
      "Epoch 91/300, Valid Loss: 0.7258, Acc: 0.4884, F1: 0.2667, AUC: 0.4398\n",
      "Epoch 92/300, Train Loss: 0.4615, Acc: 0.8081, F1: 0.6118, AUC: 0.7247\n",
      "Epoch 92/300, Valid Loss: 0.7628, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 93/300, Train Loss: 0.5122, Acc: 0.7674, F1: 0.5833, AUC: 0.7085\n",
      "Epoch 93/300, Valid Loss: 0.7844, Acc: 0.3721, F1: 0.1818, AUC: 0.3345\n",
      "Epoch 94/300, Train Loss: 0.4742, Acc: 0.7849, F1: 0.5542, AUC: 0.6900\n",
      "Epoch 94/300, Valid Loss: 0.8392, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 95/300, Train Loss: 0.5073, Acc: 0.7442, F1: 0.5000, AUC: 0.6554\n",
      "Epoch 95/300, Valid Loss: 0.7761, Acc: 0.5116, F1: 0.2222, AUC: 0.4456\n",
      "Epoch 96/300, Train Loss: 0.4870, Acc: 0.7907, F1: 0.5814, AUC: 0.7063\n",
      "Epoch 96/300, Valid Loss: 0.8755, Acc: 0.5581, F1: 0.0000, AUC: 0.4444\n",
      "Epoch 97/300, Train Loss: 0.5305, Acc: 0.7558, F1: 0.4474, AUC: 0.6328\n",
      "Epoch 97/300, Valid Loss: 0.7245, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 98/300, Train Loss: 0.4501, Acc: 0.7849, F1: 0.6022, AUC: 0.7207\n",
      "Epoch 98/300, Valid Loss: 0.7965, Acc: 0.4186, F1: 0.2857, AUC: 0.3970\n",
      "Epoch 99/300, Train Loss: 0.4967, Acc: 0.7616, F1: 0.5393, AUC: 0.6799\n",
      "Epoch 99/300, Valid Loss: 0.7507, Acc: 0.4186, F1: 0.1935, AUC: 0.3715\n",
      "Epoch 100/300, Train Loss: 0.4933, Acc: 0.7384, F1: 0.5263, AUC: 0.6697\n",
      "Epoch 100/300, Valid Loss: 0.7631, Acc: 0.5581, F1: 0.0952, AUC: 0.4572\n",
      "Epoch 101/300, Train Loss: 0.4819, Acc: 0.7791, F1: 0.5778, AUC: 0.7043\n",
      "Epoch 101/300, Valid Loss: 0.6867, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 102/300, Train Loss: 0.4836, Acc: 0.7907, F1: 0.6087, AUC: 0.7247\n",
      "Epoch 102/300, Valid Loss: 0.6965, Acc: 0.5116, F1: 0.5333, AUC: 0.5602\n",
      "Epoch 103/300, Train Loss: 0.4714, Acc: 0.7674, F1: 0.5833, AUC: 0.7085\n",
      "Epoch 103/300, Valid Loss: 0.8655, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 104/300, Train Loss: 0.4340, Acc: 0.8198, F1: 0.5974, AUC: 0.7144\n",
      "Epoch 104/300, Valid Loss: 0.8365, Acc: 0.4186, F1: 0.2857, AUC: 0.3970\n",
      "Epoch 105/300, Train Loss: 0.4755, Acc: 0.7907, F1: 0.6000, AUC: 0.7186\n",
      "Epoch 105/300, Valid Loss: 1.0444, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 106/300, Train Loss: 0.4496, Acc: 0.7965, F1: 0.6535, AUC: 0.7595\n",
      "Epoch 106/300, Valid Loss: 0.7945, Acc: 0.5349, F1: 0.2857, AUC: 0.4769\n",
      "Epoch 107/300, Train Loss: 0.4977, Acc: 0.7791, F1: 0.6346, AUC: 0.7473\n",
      "Epoch 107/300, Valid Loss: 0.9385, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 108/300, Train Loss: 0.4597, Acc: 0.7674, F1: 0.4872, AUC: 0.6532\n",
      "Epoch 108/300, Valid Loss: 0.7229, Acc: 0.4186, F1: 0.1935, AUC: 0.3715\n",
      "Epoch 109/300, Train Loss: 0.4463, Acc: 0.8140, F1: 0.6444, AUC: 0.7471\n",
      "Epoch 109/300, Valid Loss: 0.7562, Acc: 0.5349, F1: 0.1667, AUC: 0.4514\n",
      "Epoch 110/300, Train Loss: 0.4550, Acc: 0.8081, F1: 0.6452, AUC: 0.7492\n",
      "Epoch 110/300, Valid Loss: 0.7005, Acc: 0.5349, F1: 0.4444, AUC: 0.5278\n",
      "Epoch 111/300, Train Loss: 0.4776, Acc: 0.7616, F1: 0.4675, AUC: 0.6430\n",
      "Epoch 111/300, Valid Loss: 0.7032, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 112/300, Train Loss: 0.4117, Acc: 0.8256, F1: 0.6429, AUC: 0.7430\n",
      "Epoch 112/300, Valid Loss: 0.7191, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 113/300, Train Loss: 0.4700, Acc: 0.7849, F1: 0.5647, AUC: 0.6961\n",
      "Epoch 113/300, Valid Loss: 0.7198, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 114/300, Train Loss: 0.4701, Acc: 0.7965, F1: 0.5783, AUC: 0.7042\n",
      "Epoch 114/300, Valid Loss: 0.7488, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 115/300, Train Loss: 0.4157, Acc: 0.8140, F1: 0.6522, AUC: 0.7533\n",
      "Epoch 115/300, Valid Loss: 0.8480, Acc: 0.5116, F1: 0.2759, AUC: 0.4583\n",
      "Epoch 116/300, Train Loss: 0.4542, Acc: 0.7616, F1: 0.5591, AUC: 0.6921\n",
      "Epoch 116/300, Valid Loss: 0.7362, Acc: 0.4651, F1: 0.2069, AUC: 0.4086\n",
      "Epoch 117/300, Train Loss: 0.4670, Acc: 0.7907, F1: 0.6327, AUC: 0.7432\n",
      "Epoch 117/300, Valid Loss: 0.7094, Acc: 0.4884, F1: 0.4211, AUC: 0.4907\n",
      "Epoch 118/300, Train Loss: 0.4416, Acc: 0.7791, F1: 0.5870, AUC: 0.7105\n",
      "Epoch 118/300, Valid Loss: 0.9094, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 119/300, Train Loss: 0.4256, Acc: 0.8256, F1: 0.6512, AUC: 0.7491\n",
      "Epoch 119/300, Valid Loss: 0.6476, Acc: 0.6279, F1: 0.6000, AUC: 0.6528\n",
      "Epoch 120/300, Train Loss: 0.4191, Acc: 0.8140, F1: 0.6522, AUC: 0.7533\n",
      "Epoch 120/300, Valid Loss: 0.8977, Acc: 0.5116, F1: 0.2759, AUC: 0.4583\n",
      "Epoch 121/300, Train Loss: 0.4084, Acc: 0.8314, F1: 0.6947, AUC: 0.7839\n",
      "Epoch 121/300, Valid Loss: 0.7067, Acc: 0.5581, F1: 0.3448, AUC: 0.5081\n",
      "Epoch 122/300, Train Loss: 0.4728, Acc: 0.7558, F1: 0.6182, AUC: 0.7372\n",
      "Epoch 122/300, Valid Loss: 0.6459, Acc: 0.6047, F1: 0.4516, AUC: 0.5706\n",
      "Epoch 123/300, Train Loss: 0.4290, Acc: 0.7907, F1: 0.5814, AUC: 0.7063\n",
      "Epoch 123/300, Valid Loss: 0.8067, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 124/300, Train Loss: 0.4197, Acc: 0.8081, F1: 0.6292, AUC: 0.7369\n",
      "Epoch 124/300, Valid Loss: 1.4079, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 125/300, Train Loss: 0.4157, Acc: 0.8081, F1: 0.6598, AUC: 0.7615\n",
      "Epoch 125/300, Valid Loss: 0.8479, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 126/300, Train Loss: 0.4419, Acc: 0.7674, F1: 0.6000, AUC: 0.7208\n",
      "Epoch 126/300, Valid Loss: 0.6576, Acc: 0.4884, F1: 0.3529, AUC: 0.4653\n",
      "Epoch 127/300, Train Loss: 0.4291, Acc: 0.7791, F1: 0.6200, AUC: 0.7350\n",
      "Epoch 127/300, Valid Loss: 0.7228, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 128/300, Train Loss: 0.4363, Acc: 0.8081, F1: 0.6526, AUC: 0.7554\n",
      "Epoch 128/300, Valid Loss: 0.7461, Acc: 0.5116, F1: 0.2759, AUC: 0.4583\n",
      "Epoch 129/300, Train Loss: 0.4060, Acc: 0.7965, F1: 0.6316, AUC: 0.7411\n",
      "Epoch 129/300, Valid Loss: 0.7478, Acc: 0.5349, F1: 0.2308, AUC: 0.4641\n",
      "Epoch 130/300, Train Loss: 0.4237, Acc: 0.8314, F1: 0.6813, AUC: 0.7716\n",
      "Epoch 130/300, Valid Loss: 1.0021, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 131/300, Train Loss: 0.4228, Acc: 0.8023, F1: 0.6792, AUC: 0.7820\n",
      "Epoch 131/300, Valid Loss: 1.0609, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 132/300, Train Loss: 0.4392, Acc: 0.8081, F1: 0.6292, AUC: 0.7369\n",
      "Epoch 132/300, Valid Loss: 0.9266, Acc: 0.6047, F1: 0.1905, AUC: 0.5069\n",
      "Epoch 133/300, Train Loss: 0.4079, Acc: 0.8023, F1: 0.6600, AUC: 0.7636\n",
      "Epoch 133/300, Valid Loss: 0.7957, Acc: 0.6279, F1: 0.2727, AUC: 0.5382\n",
      "Epoch 134/300, Train Loss: 0.4533, Acc: 0.7849, F1: 0.6263, AUC: 0.7391\n",
      "Epoch 134/300, Valid Loss: 0.7693, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 135/300, Train Loss: 0.4098, Acc: 0.8256, F1: 0.6739, AUC: 0.7675\n",
      "Epoch 135/300, Valid Loss: 0.7710, Acc: 0.5814, F1: 0.3077, AUC: 0.5139\n",
      "Epoch 136/300, Train Loss: 0.4198, Acc: 0.8256, F1: 0.7000, AUC: 0.7921\n",
      "Epoch 136/300, Valid Loss: 0.6759, Acc: 0.5349, F1: 0.2308, AUC: 0.4641\n",
      "Epoch 137/300, Train Loss: 0.3631, Acc: 0.8488, F1: 0.7174, AUC: 0.7961\n",
      "Epoch 137/300, Valid Loss: 0.8168, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 138/300, Train Loss: 0.4501, Acc: 0.8198, F1: 0.6593, AUC: 0.7573\n",
      "Epoch 138/300, Valid Loss: 0.7190, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 139/300, Train Loss: 0.3805, Acc: 0.8256, F1: 0.6591, AUC: 0.7553\n",
      "Epoch 139/300, Valid Loss: 0.7576, Acc: 0.5116, F1: 0.2759, AUC: 0.4583\n",
      "Epoch 140/300, Train Loss: 0.3914, Acc: 0.8140, F1: 0.6522, AUC: 0.7533\n",
      "Epoch 140/300, Valid Loss: 0.6553, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 141/300, Train Loss: 0.4859, Acc: 0.8023, F1: 0.6383, AUC: 0.7451\n",
      "Epoch 141/300, Valid Loss: 0.9223, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 142/300, Train Loss: 0.4487, Acc: 0.8140, F1: 0.6364, AUC: 0.7410\n",
      "Epoch 142/300, Valid Loss: 0.7506, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 143/300, Train Loss: 0.3816, Acc: 0.8314, F1: 0.6813, AUC: 0.7716\n",
      "Epoch 143/300, Valid Loss: 0.9453, Acc: 0.5116, F1: 0.3226, AUC: 0.4711\n",
      "Epoch 144/300, Train Loss: 0.4480, Acc: 0.7907, F1: 0.6170, AUC: 0.7309\n",
      "Epoch 144/300, Valid Loss: 0.6881, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 145/300, Train Loss: 0.4057, Acc: 0.8256, F1: 0.7059, AUC: 0.7982\n",
      "Epoch 145/300, Valid Loss: 0.7231, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 146/300, Train Loss: 0.3670, Acc: 0.8488, F1: 0.7234, AUC: 0.8022\n",
      "Epoch 146/300, Valid Loss: 0.6365, Acc: 0.6279, F1: 0.6000, AUC: 0.6528\n",
      "Epoch 147/300, Train Loss: 0.3854, Acc: 0.8140, F1: 0.6596, AUC: 0.7594\n",
      "Epoch 147/300, Valid Loss: 0.7587, Acc: 0.4884, F1: 0.2667, AUC: 0.4398\n",
      "Epoch 148/300, Train Loss: 0.4095, Acc: 0.8140, F1: 0.6444, AUC: 0.7471\n",
      "Epoch 148/300, Valid Loss: 0.7701, Acc: 0.5814, F1: 0.1818, AUC: 0.4884\n",
      "Epoch 149/300, Train Loss: 0.3840, Acc: 0.8372, F1: 0.7021, AUC: 0.7880\n",
      "Epoch 149/300, Valid Loss: 0.8433, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 150/300, Train Loss: 0.3963, Acc: 0.8198, F1: 0.6804, AUC: 0.7758\n",
      "Epoch 150/300, Valid Loss: 0.7566, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 151/300, Train Loss: 0.3567, Acc: 0.8372, F1: 0.7021, AUC: 0.7880\n",
      "Epoch 151/300, Valid Loss: 0.8436, Acc: 0.4884, F1: 0.4211, AUC: 0.4907\n",
      "Epoch 152/300, Train Loss: 0.3910, Acc: 0.7907, F1: 0.6786, AUC: 0.7861\n",
      "Epoch 152/300, Valid Loss: 0.7461, Acc: 0.4884, F1: 0.4211, AUC: 0.4907\n",
      "Epoch 153/300, Train Loss: 0.3904, Acc: 0.8198, F1: 0.6990, AUC: 0.7942\n",
      "Epoch 153/300, Valid Loss: 0.8627, Acc: 0.3953, F1: 0.3500, AUC: 0.4039\n",
      "Epoch 154/300, Train Loss: 0.4083, Acc: 0.7907, F1: 0.6471, AUC: 0.7554\n",
      "Epoch 154/300, Valid Loss: 0.7793, Acc: 0.4651, F1: 0.5106, AUC: 0.5231\n",
      "Epoch 155/300, Train Loss: 0.3443, Acc: 0.8779, F1: 0.7586, AUC: 0.8164\n",
      "Epoch 155/300, Valid Loss: 0.6318, Acc: 0.6279, F1: 0.5556, AUC: 0.6273\n",
      "Epoch 156/300, Train Loss: 0.3721, Acc: 0.8488, F1: 0.7400, AUC: 0.8206\n",
      "Epoch 156/300, Valid Loss: 0.9268, Acc: 0.5349, F1: 0.3333, AUC: 0.4896\n",
      "Epoch 157/300, Train Loss: 0.4812, Acc: 0.7791, F1: 0.5250, AUC: 0.6736\n",
      "Epoch 157/300, Valid Loss: 0.6375, Acc: 0.6047, F1: 0.3200, AUC: 0.5324\n",
      "Epoch 158/300, Train Loss: 0.3936, Acc: 0.8140, F1: 0.6981, AUC: 0.7963\n",
      "Epoch 158/300, Valid Loss: 0.7799, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 159/300, Train Loss: 0.3763, Acc: 0.8256, F1: 0.6429, AUC: 0.7430\n",
      "Epoch 159/300, Valid Loss: 0.9229, Acc: 0.6047, F1: 0.1905, AUC: 0.5069\n",
      "Epoch 160/300, Train Loss: 0.3875, Acc: 0.8547, F1: 0.7253, AUC: 0.8001\n",
      "Epoch 160/300, Valid Loss: 0.8790, Acc: 0.5116, F1: 0.3226, AUC: 0.4711\n",
      "Epoch 161/300, Train Loss: 0.3902, Acc: 0.8256, F1: 0.6809, AUC: 0.7737\n",
      "Epoch 161/300, Valid Loss: 0.9956, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 162/300, Train Loss: 0.3212, Acc: 0.8547, F1: 0.7126, AUC: 0.7879\n",
      "Epoch 162/300, Valid Loss: 0.7284, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 163/300, Train Loss: 0.4399, Acc: 0.7965, F1: 0.6154, AUC: 0.7288\n",
      "Epoch 163/300, Valid Loss: 1.0963, Acc: 0.5814, F1: 0.0000, AUC: 0.4630\n",
      "Epoch 164/300, Train Loss: 0.4044, Acc: 0.8256, F1: 0.6667, AUC: 0.7614\n",
      "Epoch 164/300, Valid Loss: 0.8239, Acc: 0.6047, F1: 0.1905, AUC: 0.5069\n",
      "Epoch 165/300, Train Loss: 0.3192, Acc: 0.8721, F1: 0.7660, AUC: 0.8308\n",
      "Epoch 165/300, Valid Loss: 1.0919, Acc: 0.6279, F1: 0.1111, AUC: 0.5127\n",
      "Epoch 166/300, Train Loss: 0.3686, Acc: 0.8256, F1: 0.6739, AUC: 0.7675\n",
      "Epoch 166/300, Valid Loss: 0.6465, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 167/300, Train Loss: 0.3854, Acc: 0.8547, F1: 0.7423, AUC: 0.8186\n",
      "Epoch 167/300, Valid Loss: 0.7754, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 168/300, Train Loss: 0.3513, Acc: 0.8430, F1: 0.7429, AUC: 0.8289\n",
      "Epoch 168/300, Valid Loss: 1.0496, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 169/300, Train Loss: 0.3498, Acc: 0.8430, F1: 0.7097, AUC: 0.7920\n",
      "Epoch 169/300, Valid Loss: 0.8051, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 170/300, Train Loss: 0.4020, Acc: 0.8256, F1: 0.6667, AUC: 0.7614\n",
      "Epoch 170/300, Valid Loss: 0.7910, Acc: 0.6512, F1: 0.2857, AUC: 0.5567\n",
      "Epoch 171/300, Train Loss: 0.3724, Acc: 0.8547, F1: 0.7253, AUC: 0.8001\n",
      "Epoch 171/300, Valid Loss: 0.8100, Acc: 0.5349, F1: 0.0909, AUC: 0.4387\n",
      "Epoch 172/300, Train Loss: 0.3155, Acc: 0.8605, F1: 0.7447, AUC: 0.8165\n",
      "Epoch 172/300, Valid Loss: 0.9528, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 173/300, Train Loss: 0.3485, Acc: 0.8663, F1: 0.7473, AUC: 0.8144\n",
      "Epoch 173/300, Valid Loss: 0.8547, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 174/300, Train Loss: 0.3703, Acc: 0.8198, F1: 0.6593, AUC: 0.7573\n",
      "Epoch 174/300, Valid Loss: 1.7291, Acc: 0.6047, F1: 0.0000, AUC: 0.4815\n",
      "Epoch 175/300, Train Loss: 0.4498, Acc: 0.7965, F1: 0.6316, AUC: 0.7411\n",
      "Epoch 175/300, Valid Loss: 0.7218, Acc: 0.5581, F1: 0.2400, AUC: 0.4826\n",
      "Epoch 176/300, Train Loss: 0.3508, Acc: 0.8547, F1: 0.7525, AUC: 0.8308\n",
      "Epoch 176/300, Valid Loss: 0.7477, Acc: 0.5814, F1: 0.3077, AUC: 0.5139\n",
      "Epoch 177/300, Train Loss: 0.3396, Acc: 0.8547, F1: 0.7312, AUC: 0.8063\n",
      "Epoch 177/300, Valid Loss: 0.6746, Acc: 0.5581, F1: 0.3871, AUC: 0.5208\n",
      "Epoch 178/300, Train Loss: 0.3124, Acc: 0.8663, F1: 0.7579, AUC: 0.8267\n",
      "Epoch 178/300, Valid Loss: 0.9016, Acc: 0.5116, F1: 0.2222, AUC: 0.4456\n",
      "Epoch 179/300, Train Loss: 0.3324, Acc: 0.8721, F1: 0.7800, AUC: 0.8492\n",
      "Epoch 179/300, Valid Loss: 0.6370, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 180/300, Train Loss: 0.3758, Acc: 0.8605, F1: 0.7647, AUC: 0.8410\n",
      "Epoch 180/300, Valid Loss: 0.7412, Acc: 0.5349, F1: 0.5833, AUC: 0.6042\n",
      "Epoch 181/300, Train Loss: 0.3451, Acc: 0.8372, F1: 0.7255, AUC: 0.8125\n",
      "Epoch 181/300, Valid Loss: 0.7758, Acc: 0.5814, F1: 0.3077, AUC: 0.5139\n",
      "Epoch 182/300, Train Loss: 0.2983, Acc: 0.8721, F1: 0.7885, AUC: 0.8615\n",
      "Epoch 182/300, Valid Loss: 0.6724, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 183/300, Train Loss: 0.3448, Acc: 0.8605, F1: 0.7736, AUC: 0.8533\n",
      "Epoch 183/300, Valid Loss: 1.1500, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 184/300, Train Loss: 0.3618, Acc: 0.8721, F1: 0.7708, AUC: 0.8369\n",
      "Epoch 184/300, Valid Loss: 0.7681, Acc: 0.6047, F1: 0.3200, AUC: 0.5324\n",
      "Epoch 185/300, Train Loss: 0.2745, Acc: 0.8837, F1: 0.7917, AUC: 0.8512\n",
      "Epoch 185/300, Valid Loss: 0.7712, Acc: 0.5581, F1: 0.3448, AUC: 0.5081\n",
      "Epoch 186/300, Train Loss: 0.4023, Acc: 0.8488, F1: 0.7174, AUC: 0.7961\n",
      "Epoch 186/300, Valid Loss: 1.1067, Acc: 0.6047, F1: 0.1053, AUC: 0.4942\n",
      "Epoch 187/300, Train Loss: 0.3862, Acc: 0.8314, F1: 0.7129, AUC: 0.8023\n",
      "Epoch 187/300, Valid Loss: 0.8738, Acc: 0.4651, F1: 0.3030, AUC: 0.4340\n",
      "Epoch 188/300, Train Loss: 0.3601, Acc: 0.8488, F1: 0.7500, AUC: 0.8329\n",
      "Epoch 188/300, Valid Loss: 0.7433, Acc: 0.5814, F1: 0.3077, AUC: 0.5139\n",
      "Epoch 189/300, Train Loss: 0.3106, Acc: 0.8547, F1: 0.7525, AUC: 0.8308\n",
      "Epoch 189/300, Valid Loss: 0.6971, Acc: 0.5581, F1: 0.4242, AUC: 0.5336\n",
      "Epoch 190/300, Train Loss: 0.3983, Acc: 0.7965, F1: 0.6729, AUC: 0.7779\n",
      "Epoch 190/300, Valid Loss: 0.5962, Acc: 0.5814, F1: 0.4000, AUC: 0.5394\n",
      "Epoch 191/300, Train Loss: 0.3425, Acc: 0.8430, F1: 0.7523, AUC: 0.8411\n",
      "Epoch 191/300, Valid Loss: 0.8965, Acc: 0.6279, F1: 0.2727, AUC: 0.5382\n",
      "Epoch 192/300, Train Loss: 0.3613, Acc: 0.8430, F1: 0.7429, AUC: 0.8289\n",
      "Epoch 192/300, Valid Loss: 0.8772, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 193/300, Train Loss: 0.3671, Acc: 0.8372, F1: 0.7255, AUC: 0.8125\n",
      "Epoch 193/300, Valid Loss: 0.6062, Acc: 0.6744, F1: 0.5625, AUC: 0.6516\n",
      "Epoch 194/300, Train Loss: 0.3539, Acc: 0.8256, F1: 0.7170, AUC: 0.8105\n",
      "Epoch 194/300, Valid Loss: 0.7133, Acc: 0.5116, F1: 0.5714, AUC: 0.5856\n",
      "Epoch 195/300, Train Loss: 0.3333, Acc: 0.8547, F1: 0.7573, AUC: 0.8370\n",
      "Epoch 195/300, Valid Loss: 0.7421, Acc: 0.5349, F1: 0.2857, AUC: 0.4769\n",
      "Epoch 196/300, Train Loss: 0.4196, Acc: 0.8081, F1: 0.6857, AUC: 0.7860\n",
      "Epoch 196/300, Valid Loss: 0.7677, Acc: 0.4419, F1: 0.3684, AUC: 0.4410\n",
      "Epoch 197/300, Train Loss: 0.3856, Acc: 0.8081, F1: 0.6598, AUC: 0.7615\n",
      "Epoch 197/300, Valid Loss: 0.8451, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 198/300, Train Loss: 0.2763, Acc: 0.8779, F1: 0.7640, AUC: 0.8225\n",
      "Epoch 198/300, Valid Loss: 0.8360, Acc: 0.5349, F1: 0.2308, AUC: 0.4641\n",
      "Epoch 199/300, Train Loss: 0.3641, Acc: 0.8081, F1: 0.6796, AUC: 0.7799\n",
      "Epoch 199/300, Valid Loss: 0.7557, Acc: 0.4884, F1: 0.3889, AUC: 0.4780\n",
      "Epoch 200/300, Train Loss: 0.3567, Acc: 0.8140, F1: 0.6596, AUC: 0.7594\n",
      "Epoch 200/300, Valid Loss: 0.7438, Acc: 0.5116, F1: 0.5532, AUC: 0.5729\n",
      "Epoch 201/300, Train Loss: 0.4074, Acc: 0.8140, F1: 0.7037, AUC: 0.8024\n",
      "Epoch 201/300, Valid Loss: 0.7160, Acc: 0.5349, F1: 0.2857, AUC: 0.4769\n",
      "Epoch 202/300, Train Loss: 0.2829, Acc: 0.8721, F1: 0.7755, AUC: 0.8430\n",
      "Epoch 202/300, Valid Loss: 1.1639, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 203/300, Train Loss: 0.3487, Acc: 0.8314, F1: 0.7071, AUC: 0.7962\n",
      "Epoch 203/300, Valid Loss: 1.0232, Acc: 0.5349, F1: 0.3333, AUC: 0.4896\n",
      "Epoch 204/300, Train Loss: 0.3159, Acc: 0.8837, F1: 0.7917, AUC: 0.8512\n",
      "Epoch 204/300, Valid Loss: 0.6673, Acc: 0.6512, F1: 0.4828, AUC: 0.6076\n",
      "Epoch 205/300, Train Loss: 0.3278, Acc: 0.8430, F1: 0.7327, AUC: 0.8166\n",
      "Epoch 205/300, Valid Loss: 0.6388, Acc: 0.6512, F1: 0.4444, AUC: 0.5949\n",
      "Epoch 206/300, Train Loss: 0.3054, Acc: 0.8663, F1: 0.7850, AUC: 0.8635\n",
      "Epoch 206/300, Valid Loss: 0.5880, Acc: 0.6744, F1: 0.5333, AUC: 0.6389\n",
      "Epoch 207/300, Train Loss: 0.3473, Acc: 0.8256, F1: 0.7222, AUC: 0.8167\n",
      "Epoch 207/300, Valid Loss: 0.8152, Acc: 0.4651, F1: 0.3784, AUC: 0.4595\n",
      "Epoch 208/300, Train Loss: 0.2865, Acc: 0.8721, F1: 0.7843, AUC: 0.8553\n",
      "Epoch 208/300, Valid Loss: 0.5798, Acc: 0.7209, F1: 0.6471, AUC: 0.7141\n",
      "Epoch 209/300, Train Loss: 0.2886, Acc: 0.8895, F1: 0.8190, AUC: 0.8859\n",
      "Epoch 209/300, Valid Loss: 0.8159, Acc: 0.5116, F1: 0.3226, AUC: 0.4711\n",
      "Epoch 210/300, Train Loss: 0.4513, Acc: 0.7500, F1: 0.6560, AUC: 0.7761\n",
      "Epoch 210/300, Valid Loss: 0.7787, Acc: 0.6744, F1: 0.4615, AUC: 0.6134\n",
      "Epoch 211/300, Train Loss: 0.4592, Acc: 0.7500, F1: 0.6387, AUC: 0.7577\n",
      "Epoch 211/300, Valid Loss: 0.5671, Acc: 0.6977, F1: 0.6061, AUC: 0.6829\n",
      "Epoch 212/300, Train Loss: 0.4100, Acc: 0.7733, F1: 0.6549, AUC: 0.7678\n",
      "Epoch 212/300, Valid Loss: 0.9336, Acc: 0.6512, F1: 0.4000, AUC: 0.5822\n",
      "Epoch 213/300, Train Loss: 0.3970, Acc: 0.7849, F1: 0.6476, AUC: 0.7575\n",
      "Epoch 213/300, Valid Loss: 0.8681, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 214/300, Train Loss: 0.3138, Acc: 0.8837, F1: 0.8039, AUC: 0.8696\n",
      "Epoch 214/300, Valid Loss: 0.5829, Acc: 0.5349, F1: 0.4118, AUC: 0.5150\n",
      "Epoch 215/300, Train Loss: 0.3134, Acc: 0.8779, F1: 0.7961, AUC: 0.8655\n",
      "Epoch 215/300, Valid Loss: 0.7400, Acc: 0.6512, F1: 0.4828, AUC: 0.6076\n",
      "Epoch 216/300, Train Loss: 0.3422, Acc: 0.8605, F1: 0.7647, AUC: 0.8410\n",
      "Epoch 216/300, Valid Loss: 0.6552, Acc: 0.5814, F1: 0.4375, AUC: 0.5521\n",
      "Epoch 217/300, Train Loss: 0.2877, Acc: 0.8895, F1: 0.8081, AUC: 0.8675\n",
      "Epoch 217/300, Valid Loss: 0.9507, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 218/300, Train Loss: 0.3731, Acc: 0.8314, F1: 0.6813, AUC: 0.7716\n",
      "Epoch 218/300, Valid Loss: 0.7000, Acc: 0.5814, F1: 0.4375, AUC: 0.5521\n",
      "Epoch 219/300, Train Loss: 0.2904, Acc: 0.8895, F1: 0.8081, AUC: 0.8675\n",
      "Epoch 219/300, Valid Loss: 0.6728, Acc: 0.5581, F1: 0.3871, AUC: 0.5208\n",
      "Epoch 220/300, Train Loss: 0.3187, Acc: 0.8430, F1: 0.7216, AUC: 0.8043\n",
      "Epoch 220/300, Valid Loss: 0.8123, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 221/300, Train Loss: 0.3748, Acc: 0.8314, F1: 0.7184, AUC: 0.8084\n",
      "Epoch 221/300, Valid Loss: 0.6395, Acc: 0.5581, F1: 0.4571, AUC: 0.5463\n",
      "Epoch 222/300, Train Loss: 0.3341, Acc: 0.8547, F1: 0.7368, AUC: 0.8124\n",
      "Epoch 222/300, Valid Loss: 0.9326, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 223/300, Train Loss: 0.3102, Acc: 0.8488, F1: 0.7174, AUC: 0.7961\n",
      "Epoch 223/300, Valid Loss: 1.0118, Acc: 0.5349, F1: 0.3333, AUC: 0.4896\n",
      "Epoch 224/300, Train Loss: 0.2392, Acc: 0.9186, F1: 0.8571, AUC: 0.9001\n",
      "Epoch 224/300, Valid Loss: 0.7607, Acc: 0.5581, F1: 0.3871, AUC: 0.5208\n",
      "Epoch 225/300, Train Loss: 0.2730, Acc: 0.8895, F1: 0.8000, AUC: 0.8552\n",
      "Epoch 225/300, Valid Loss: 0.7535, Acc: 0.4884, F1: 0.2667, AUC: 0.4398\n",
      "Epoch 226/300, Train Loss: 0.3162, Acc: 0.8721, F1: 0.7963, AUC: 0.8737\n",
      "Epoch 226/300, Valid Loss: 0.6060, Acc: 0.6512, F1: 0.4828, AUC: 0.6076\n",
      "Epoch 227/300, Train Loss: 0.6005, Acc: 0.6570, F1: 0.5816, AUC: 0.7111\n",
      "Epoch 227/300, Valid Loss: 0.6771, Acc: 0.6047, F1: 0.5854, AUC: 0.6343\n",
      "Epoch 228/300, Train Loss: 0.4711, Acc: 0.7209, F1: 0.4783, AUC: 0.6391\n",
      "Epoch 228/300, Valid Loss: 0.5995, Acc: 0.5581, F1: 0.3448, AUC: 0.5081\n",
      "Epoch 229/300, Train Loss: 0.3972, Acc: 0.7849, F1: 0.6408, AUC: 0.7514\n",
      "Epoch 229/300, Valid Loss: 0.7824, Acc: 0.6279, F1: 0.2727, AUC: 0.5382\n",
      "Epoch 230/300, Train Loss: 0.4532, Acc: 0.7442, F1: 0.6071, AUC: 0.7291\n",
      "Epoch 230/300, Valid Loss: 0.6461, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 231/300, Train Loss: 0.3717, Acc: 0.7907, F1: 0.6538, AUC: 0.7616\n",
      "Epoch 231/300, Valid Loss: 0.6858, Acc: 0.5349, F1: 0.3333, AUC: 0.4896\n",
      "Epoch 232/300, Train Loss: 0.3961, Acc: 0.8140, F1: 0.7091, AUC: 0.8085\n",
      "Epoch 232/300, Valid Loss: 0.6780, Acc: 0.6512, F1: 0.4000, AUC: 0.5822\n",
      "Epoch 233/300, Train Loss: 0.3834, Acc: 0.8081, F1: 0.6972, AUC: 0.7983\n",
      "Epoch 233/300, Valid Loss: 0.5912, Acc: 0.6512, F1: 0.5455, AUC: 0.6331\n",
      "Epoch 234/300, Train Loss: 0.3947, Acc: 0.8198, F1: 0.7048, AUC: 0.8003\n",
      "Epoch 234/300, Valid Loss: 0.6016, Acc: 0.6047, F1: 0.5143, AUC: 0.5961\n",
      "Epoch 235/300, Train Loss: 0.4308, Acc: 0.7965, F1: 0.6903, AUC: 0.7963\n",
      "Epoch 235/300, Valid Loss: 0.5913, Acc: 0.6279, F1: 0.5000, AUC: 0.6019\n",
      "Epoch 236/300, Train Loss: 0.2895, Acc: 0.8721, F1: 0.7925, AUC: 0.8676\n",
      "Epoch 236/300, Valid Loss: 0.6921, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 237/300, Train Loss: 0.3974, Acc: 0.8198, F1: 0.7207, AUC: 0.8187\n",
      "Epoch 237/300, Valid Loss: 0.7067, Acc: 0.5581, F1: 0.2963, AUC: 0.4954\n",
      "Epoch 238/300, Train Loss: 0.2897, Acc: 0.8779, F1: 0.7879, AUC: 0.8532\n",
      "Epoch 238/300, Valid Loss: 0.7330, Acc: 0.5581, F1: 0.4865, AUC: 0.5590\n",
      "Epoch 239/300, Train Loss: 0.3551, Acc: 0.8430, F1: 0.7477, AUC: 0.8350\n",
      "Epoch 239/300, Valid Loss: 0.8618, Acc: 0.5116, F1: 0.4000, AUC: 0.4965\n",
      "Epoch 240/300, Train Loss: 0.3429, Acc: 0.8488, F1: 0.7500, AUC: 0.8329\n",
      "Epoch 240/300, Valid Loss: 0.6724, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 241/300, Train Loss: 0.2800, Acc: 0.8953, F1: 0.8333, AUC: 0.9023\n",
      "Epoch 241/300, Valid Loss: 0.6912, Acc: 0.5814, F1: 0.5909, AUC: 0.6285\n",
      "Epoch 242/300, Train Loss: 0.3981, Acc: 0.7907, F1: 0.6897, AUC: 0.7984\n",
      "Epoch 242/300, Valid Loss: 0.6925, Acc: 0.7209, F1: 0.5385, AUC: 0.6632\n",
      "Epoch 243/300, Train Loss: 0.4803, Acc: 0.7151, F1: 0.6080, AUC: 0.7333\n",
      "Epoch 243/300, Valid Loss: 0.6044, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 244/300, Train Loss: 0.4023, Acc: 0.7849, F1: 0.6838, AUC: 0.7943\n",
      "Epoch 244/300, Valid Loss: 0.6802, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 245/300, Train Loss: 0.4011, Acc: 0.7849, F1: 0.6667, AUC: 0.7759\n",
      "Epoch 245/300, Valid Loss: 0.6679, Acc: 0.5349, F1: 0.4118, AUC: 0.5150\n",
      "Epoch 246/300, Train Loss: 0.4076, Acc: 0.7791, F1: 0.6780, AUC: 0.7903\n",
      "Epoch 246/300, Valid Loss: 0.9458, Acc: 0.6279, F1: 0.2727, AUC: 0.5382\n",
      "Epoch 247/300, Train Loss: 0.4121, Acc: 0.7849, F1: 0.6783, AUC: 0.7882\n",
      "Epoch 247/300, Valid Loss: 0.6351, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 248/300, Train Loss: 0.4343, Acc: 0.7558, F1: 0.6557, AUC: 0.7740\n",
      "Epoch 248/300, Valid Loss: 0.7204, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 249/300, Train Loss: 0.3974, Acc: 0.7965, F1: 0.6903, AUC: 0.7963\n",
      "Epoch 249/300, Valid Loss: 0.6298, Acc: 0.6047, F1: 0.4516, AUC: 0.5706\n",
      "Epoch 250/300, Train Loss: 0.3940, Acc: 0.8023, F1: 0.6909, AUC: 0.7943\n",
      "Epoch 250/300, Valid Loss: 0.7412, Acc: 0.6047, F1: 0.3200, AUC: 0.5324\n",
      "Epoch 251/300, Train Loss: 0.3685, Acc: 0.8372, F1: 0.7455, AUC: 0.8371\n",
      "Epoch 251/300, Valid Loss: 0.6680, Acc: 0.5349, F1: 0.5652, AUC: 0.5914\n",
      "Epoch 252/300, Train Loss: 0.4363, Acc: 0.7500, F1: 0.6387, AUC: 0.7577\n",
      "Epoch 252/300, Valid Loss: 0.5910, Acc: 0.6047, F1: 0.5143, AUC: 0.5961\n",
      "Epoch 253/300, Train Loss: 0.3906, Acc: 0.7849, F1: 0.6783, AUC: 0.7882\n",
      "Epoch 253/300, Valid Loss: 0.6402, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 254/300, Train Loss: 0.4273, Acc: 0.7616, F1: 0.6372, AUC: 0.7535\n",
      "Epoch 254/300, Valid Loss: 0.6872, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 255/300, Train Loss: 0.3701, Acc: 0.7791, F1: 0.6607, AUC: 0.7719\n",
      "Epoch 255/300, Valid Loss: 0.7686, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 256/300, Train Loss: 0.3977, Acc: 0.8023, F1: 0.7069, AUC: 0.8127\n",
      "Epoch 256/300, Valid Loss: 0.5756, Acc: 0.6279, F1: 0.5789, AUC: 0.6400\n",
      "Epoch 257/300, Train Loss: 0.3390, Acc: 0.8198, F1: 0.7257, AUC: 0.8249\n",
      "Epoch 257/300, Valid Loss: 0.6847, Acc: 0.7209, F1: 0.5714, AUC: 0.6759\n",
      "Epoch 258/300, Train Loss: 0.4537, Acc: 0.7326, F1: 0.6406, AUC: 0.7639\n",
      "Epoch 258/300, Valid Loss: 0.7065, Acc: 0.5814, F1: 0.4706, AUC: 0.5648\n",
      "Epoch 259/300, Train Loss: 0.3505, Acc: 0.8081, F1: 0.7027, AUC: 0.8045\n",
      "Epoch 259/300, Valid Loss: 0.8586, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 260/300, Train Loss: 0.3879, Acc: 0.7616, F1: 0.6667, AUC: 0.7842\n",
      "Epoch 260/300, Valid Loss: 0.7013, Acc: 0.4884, F1: 0.5926, AUC: 0.5926\n",
      "Epoch 261/300, Train Loss: 0.5131, Acc: 0.6686, F1: 0.5957, AUC: 0.7253\n",
      "Epoch 261/300, Valid Loss: 0.6982, Acc: 0.3721, F1: 0.5263, AUC: 0.4873\n",
      "Epoch 262/300, Train Loss: 0.5091, Acc: 0.6628, F1: 0.3958, AUC: 0.5801\n",
      "Epoch 262/300, Valid Loss: 0.7115, Acc: 0.6279, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 263/300, Train Loss: 0.4006, Acc: 0.7733, F1: 0.4507, AUC: 0.6389\n",
      "Epoch 263/300, Valid Loss: 0.6856, Acc: 0.6512, F1: 0.4444, AUC: 0.5949\n",
      "Epoch 264/300, Train Loss: 0.4086, Acc: 0.7267, F1: 0.5983, AUC: 0.7230\n",
      "Epoch 264/300, Valid Loss: 1.0946, Acc: 0.6047, F1: 0.2609, AUC: 0.5197\n",
      "Epoch 265/300, Train Loss: 0.3975, Acc: 0.7558, F1: 0.6379, AUC: 0.7556\n",
      "Epoch 265/300, Valid Loss: 0.7072, Acc: 0.6512, F1: 0.4444, AUC: 0.5949\n",
      "Epoch 266/300, Train Loss: 0.3471, Acc: 0.8198, F1: 0.7350, AUC: 0.8371\n",
      "Epoch 266/300, Valid Loss: 0.6063, Acc: 0.5814, F1: 0.6400, AUC: 0.6667\n",
      "Epoch 267/300, Train Loss: 0.3603, Acc: 0.8314, F1: 0.7434, AUC: 0.8391\n",
      "Epoch 267/300, Valid Loss: 0.8532, Acc: 0.6279, F1: 0.3846, AUC: 0.5637\n",
      "Epoch 268/300, Train Loss: 0.4009, Acc: 0.7849, F1: 0.6891, AUC: 0.8005\n",
      "Epoch 268/300, Valid Loss: 0.6954, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 269/300, Train Loss: 0.7235, Acc: 0.7616, F1: 0.6372, AUC: 0.7535\n",
      "Epoch 269/300, Valid Loss: 0.6835, Acc: 0.6279, F1: 0.3333, AUC: 0.5509\n",
      "Epoch 270/300, Train Loss: 0.3373, Acc: 0.8372, F1: 0.7455, AUC: 0.8371\n",
      "Epoch 270/300, Valid Loss: 0.6371, Acc: 0.6512, F1: 0.5161, AUC: 0.6204\n",
      "Epoch 271/300, Train Loss: 0.3937, Acc: 0.7849, F1: 0.6667, AUC: 0.7759\n",
      "Epoch 271/300, Valid Loss: 0.6635, Acc: 0.6047, F1: 0.4138, AUC: 0.5579\n",
      "Epoch 272/300, Train Loss: 0.3812, Acc: 0.7965, F1: 0.7009, AUC: 0.8086\n",
      "Epoch 272/300, Valid Loss: 0.5776, Acc: 0.6047, F1: 0.4848, AUC: 0.5833\n",
      "Epoch 273/300, Train Loss: 0.3804, Acc: 0.8023, F1: 0.6964, AUC: 0.8004\n",
      "Epoch 273/300, Valid Loss: 0.6549, Acc: 0.5349, F1: 0.6154, AUC: 0.6296\n",
      "Epoch 274/300, Train Loss: 0.3715, Acc: 0.7849, F1: 0.6891, AUC: 0.8005\n",
      "Epoch 274/300, Valid Loss: 1.1488, Acc: 0.6279, F1: 0.2727, AUC: 0.5382\n",
      "Epoch 275/300, Train Loss: 0.4833, Acc: 0.6919, F1: 0.6015, AUC: 0.7293\n",
      "Epoch 275/300, Valid Loss: 0.5641, Acc: 0.6279, F1: 0.6190, AUC: 0.6655\n",
      "Epoch 276/300, Train Loss: 0.3358, Acc: 0.8605, F1: 0.7818, AUC: 0.8656\n",
      "Epoch 276/300, Valid Loss: 0.7617, Acc: 0.5814, F1: 0.3571, AUC: 0.5266\n",
      "Epoch 277/300, Train Loss: 0.3721, Acc: 0.7849, F1: 0.6891, AUC: 0.8005\n",
      "Epoch 277/300, Valid Loss: 0.5888, Acc: 0.6512, F1: 0.5455, AUC: 0.6331\n",
      "Epoch 278/300, Train Loss: 0.3693, Acc: 0.7965, F1: 0.6903, AUC: 0.7963\n",
      "Epoch 278/300, Valid Loss: 0.8219, Acc: 0.6512, F1: 0.4444, AUC: 0.5949\n",
      "Epoch 279/300, Train Loss: 0.4459, Acc: 0.7674, F1: 0.6721, AUC: 0.7883\n",
      "Epoch 279/300, Valid Loss: 0.6978, Acc: 0.6279, F1: 0.4667, AUC: 0.5891\n",
      "Epoch 280/300, Train Loss: 0.3846, Acc: 0.8314, F1: 0.7339, AUC: 0.8269\n",
      "Epoch 280/300, Valid Loss: 0.5851, Acc: 0.6047, F1: 0.5854, AUC: 0.6343\n",
      "Epoch 281/300, Train Loss: 0.3869, Acc: 0.7907, F1: 0.6727, AUC: 0.7800\n",
      "Epoch 281/300, Valid Loss: 0.6047, Acc: 0.6279, F1: 0.5000, AUC: 0.6019\n",
      "Epoch 282/300, Train Loss: 0.3432, Acc: 0.8140, F1: 0.7091, AUC: 0.8085\n",
      "Epoch 282/300, Valid Loss: 0.6388, Acc: 0.5814, F1: 0.6400, AUC: 0.6667\n",
      "Epoch 283/300, Train Loss: 0.3871, Acc: 0.8081, F1: 0.7080, AUC: 0.8106\n",
      "Epoch 283/300, Valid Loss: 0.7298, Acc: 0.5349, F1: 0.2857, AUC: 0.4769\n",
      "Epoch 284/300, Train Loss: 0.3340, Acc: 0.8488, F1: 0.7636, AUC: 0.8513\n",
      "Epoch 284/300, Valid Loss: 0.8073, Acc: 0.6977, F1: 0.4800, AUC: 0.6319\n",
      "Epoch 285/300, Train Loss: 0.4685, Acc: 0.7209, F1: 0.6066, AUC: 0.7312\n",
      "Epoch 285/300, Valid Loss: 0.7125, Acc: 0.6047, F1: 0.3200, AUC: 0.5324\n",
      "Epoch 286/300, Train Loss: 0.3285, Acc: 0.8430, F1: 0.7429, AUC: 0.8289\n",
      "Epoch 286/300, Valid Loss: 0.6062, Acc: 0.5581, F1: 0.5128, AUC: 0.5718\n",
      "Epoch 287/300, Train Loss: 0.3320, Acc: 0.8547, F1: 0.7748, AUC: 0.8615\n",
      "Epoch 287/300, Valid Loss: 1.0974, Acc: 0.5814, F1: 0.2500, AUC: 0.5012\n",
      "Epoch 288/300, Train Loss: 0.3179, Acc: 0.8372, F1: 0.7455, AUC: 0.8371\n",
      "Epoch 288/300, Valid Loss: 0.8750, Acc: 0.6512, F1: 0.4444, AUC: 0.5949\n",
      "Epoch 289/300, Train Loss: 0.3601, Acc: 0.7849, F1: 0.6783, AUC: 0.7882\n",
      "Epoch 289/300, Valid Loss: 0.8168, Acc: 0.6512, F1: 0.4828, AUC: 0.6076\n",
      "Epoch 290/300, Train Loss: 0.2963, Acc: 0.8605, F1: 0.7857, AUC: 0.8717\n",
      "Epoch 290/300, Valid Loss: 0.8649, Acc: 0.6047, F1: 0.3704, AUC: 0.5451\n",
      "Epoch 291/300, Train Loss: 0.2960, Acc: 0.8547, F1: 0.7748, AUC: 0.8615\n",
      "Epoch 291/300, Valid Loss: 0.7316, Acc: 0.6744, F1: 0.5000, AUC: 0.6262\n",
      "Epoch 292/300, Train Loss: 0.3467, Acc: 0.8140, F1: 0.7241, AUC: 0.8269\n",
      "Epoch 292/300, Valid Loss: 0.8126, Acc: 0.6279, F1: 0.4286, AUC: 0.5764\n",
      "Epoch 293/300, Train Loss: 0.2697, Acc: 0.8837, F1: 0.8214, AUC: 0.9003\n",
      "Epoch 293/300, Valid Loss: 1.4106, Acc: 0.5581, F1: 0.1739, AUC: 0.4699\n",
      "Epoch 294/300, Train Loss: 0.4388, Acc: 0.7849, F1: 0.6783, AUC: 0.7882\n",
      "Epoch 294/300, Valid Loss: 0.5871, Acc: 0.6047, F1: 0.5405, AUC: 0.6088\n",
      "Epoch 295/300, Train Loss: 0.3569, Acc: 0.8198, F1: 0.7304, AUC: 0.8310\n",
      "Epoch 295/300, Valid Loss: 0.7348, Acc: 0.6279, F1: 0.3846, AUC: 0.5637\n",
      "Epoch 296/300, Train Loss: 0.4027, Acc: 0.7791, F1: 0.6885, AUC: 0.8026\n",
      "Epoch 296/300, Valid Loss: 0.6229, Acc: 0.6744, F1: 0.5000, AUC: 0.6262\n",
      "Epoch 297/300, Train Loss: 0.3359, Acc: 0.8081, F1: 0.7027, AUC: 0.8045\n",
      "Epoch 297/300, Valid Loss: 0.9211, Acc: 0.6047, F1: 0.3200, AUC: 0.5324\n",
      "Epoch 298/300, Train Loss: 0.3421, Acc: 0.8430, F1: 0.7568, AUC: 0.8473\n",
      "Epoch 298/300, Valid Loss: 0.7075, Acc: 0.5581, F1: 0.3871, AUC: 0.5208\n",
      "Epoch 299/300, Train Loss: 0.3936, Acc: 0.7907, F1: 0.6897, AUC: 0.7984\n",
      "Epoch 299/300, Valid Loss: 0.6465, Acc: 0.6279, F1: 0.4667, AUC: 0.5891\n",
      "Epoch 300/300, Train Loss: 0.3469, Acc: 0.8256, F1: 0.7414, AUC: 0.8412\n",
      "Epoch 300/300, Valid Loss: 0.7800, Acc: 0.5581, F1: 0.3871, AUC: 0.5208\n",
      "-- Fold 4/5 --\n",
      "Epoch 1/300, Train Loss: 0.6645, Acc: 0.6628, F1: 0.0645, AUC: 0.5006\n",
      "Epoch 1/300, Valid Loss: 0.5832, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Train Loss: 0.6461, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Valid Loss: 0.6187, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Train Loss: 0.6646, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6234, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Train Loss: 0.6544, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Valid Loss: 0.5831, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Train Loss: 0.6368, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.6174, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Train Loss: 0.6567, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.5996, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Train Loss: 0.6465, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.5446, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Train Loss: 0.6365, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.5263, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Train Loss: 0.6482, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.5504, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Train Loss: 0.6365, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.5372, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Train Loss: 0.6341, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.5214, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Train Loss: 0.6232, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.5723, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Train Loss: 0.6299, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.5659, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Train Loss: 0.6488, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.5759, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Train Loss: 0.6323, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.5411, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Train Loss: 0.6346, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.5636, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Train Loss: 0.6222, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.5966, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Train Loss: 0.6125, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Valid Loss: 0.5905, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Train Loss: 0.6322, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Valid Loss: 0.5695, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Train Loss: 0.6297, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Valid Loss: 0.5251, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Train Loss: 0.6252, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Valid Loss: 0.5663, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Train Loss: 0.6335, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Valid Loss: 0.6000, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Train Loss: 0.6243, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Valid Loss: 0.5380, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Train Loss: 0.6075, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Valid Loss: 0.6011, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Train Loss: 0.6218, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Valid Loss: 0.5599, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Train Loss: 0.6156, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Valid Loss: 0.5424, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Train Loss: 0.6119, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Valid Loss: 0.5824, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Train Loss: 0.6082, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Valid Loss: 0.5165, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Train Loss: 0.6067, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Valid Loss: 0.5253, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Train Loss: 0.6187, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Valid Loss: 0.5553, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Train Loss: 0.6197, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Valid Loss: 0.5977, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Train Loss: 0.6140, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Valid Loss: 0.5817, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Train Loss: 0.6104, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Valid Loss: 0.5810, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Train Loss: 0.6129, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Valid Loss: 0.5901, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Train Loss: 0.6266, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Valid Loss: 0.5976, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Train Loss: 0.6095, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Valid Loss: 0.5799, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Train Loss: 0.6012, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Valid Loss: 0.5573, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Train Loss: 0.5959, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Valid Loss: 0.5191, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Train Loss: 0.6045, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Valid Loss: 0.5694, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Train Loss: 0.5877, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Valid Loss: 0.5286, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Train Loss: 0.6055, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Valid Loss: 0.5519, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Train Loss: 0.6117, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Valid Loss: 0.5802, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 43/300, Train Loss: 0.6015, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 43/300, Valid Loss: 0.5313, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Train Loss: 0.5988, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Valid Loss: 0.5370, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Train Loss: 0.5902, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Valid Loss: 0.5279, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Train Loss: 0.6226, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Valid Loss: 0.5537, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Train Loss: 0.6036, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Valid Loss: 0.5343, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Train Loss: 0.5989, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Valid Loss: 0.5564, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Train Loss: 0.6028, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Valid Loss: 0.5544, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Train Loss: 0.6031, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Valid Loss: 0.5321, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Train Loss: 0.6100, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Valid Loss: 0.5337, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Train Loss: 0.5895, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Valid Loss: 0.5404, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 53/300, Train Loss: 0.5976, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 53/300, Valid Loss: 0.5663, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 54/300, Train Loss: 0.5973, Acc: 0.6686, F1: 0.0000, AUC: 0.4957\n",
      "Epoch 54/300, Valid Loss: 0.5319, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 55/300, Train Loss: 0.6085, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 55/300, Valid Loss: 0.6272, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Train Loss: 0.5964, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Valid Loss: 0.5908, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Train Loss: 0.5893, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Valid Loss: 0.5591, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 58/300, Train Loss: 0.5767, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 58/300, Valid Loss: 0.5595, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 59/300, Train Loss: 0.5748, Acc: 0.6628, F1: 0.0000, AUC: 0.4914\n",
      "Epoch 59/300, Valid Loss: 0.5682, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 60/300, Train Loss: 0.5916, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 60/300, Valid Loss: 0.5446, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 61/300, Train Loss: 0.5847, Acc: 0.6744, F1: 0.1765, AUC: 0.5277\n",
      "Epoch 61/300, Valid Loss: 0.6004, Acc: 0.6977, F1: 0.2353, AUC: 0.5229\n",
      "Epoch 62/300, Train Loss: 0.5724, Acc: 0.6802, F1: 0.1791, AUC: 0.5320\n",
      "Epoch 62/300, Valid Loss: 0.5352, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 63/300, Train Loss: 0.5825, Acc: 0.6919, F1: 0.3291, AUC: 0.5730\n",
      "Epoch 63/300, Valid Loss: 0.5333, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 64/300, Train Loss: 0.5647, Acc: 0.7035, F1: 0.2817, AUC: 0.5677\n",
      "Epoch 64/300, Valid Loss: 0.5353, Acc: 0.7674, F1: 0.0000, AUC: 0.4853\n",
      "Epoch 65/300, Train Loss: 0.5827, Acc: 0.6919, F1: 0.4301, AUC: 0.6053\n",
      "Epoch 65/300, Valid Loss: 0.5351, Acc: 0.7442, F1: 0.0000, AUC: 0.4706\n",
      "Epoch 66/300, Train Loss: 0.5812, Acc: 0.6395, F1: 0.4259, AUC: 0.5804\n",
      "Epoch 66/300, Valid Loss: 0.5304, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 67/300, Train Loss: 0.5611, Acc: 0.6860, F1: 0.2895, AUC: 0.5594\n",
      "Epoch 67/300, Valid Loss: 0.5672, Acc: 0.8140, F1: 0.2000, AUC: 0.5556\n",
      "Epoch 68/300, Train Loss: 0.5828, Acc: 0.6453, F1: 0.3297, AUC: 0.5477\n",
      "Epoch 68/300, Valid Loss: 0.6561, Acc: 0.3721, F1: 0.3077, AUC: 0.4804\n",
      "Epoch 69/300, Train Loss: 0.6003, Acc: 0.6802, F1: 0.4211, AUC: 0.5967\n",
      "Epoch 69/300, Valid Loss: 0.5877, Acc: 0.6512, F1: 0.2105, AUC: 0.4935\n",
      "Epoch 70/300, Train Loss: 0.5616, Acc: 0.6686, F1: 0.3294, AUC: 0.5603\n",
      "Epoch 70/300, Valid Loss: 0.5814, Acc: 0.6512, F1: 0.2105, AUC: 0.4935\n",
      "Epoch 71/300, Train Loss: 0.5884, Acc: 0.6337, F1: 0.3077, AUC: 0.5345\n",
      "Epoch 71/300, Valid Loss: 0.6090, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Train Loss: 0.5439, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Valid Loss: 0.5272, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 73/300, Train Loss: 0.5770, Acc: 0.6744, F1: 0.2632, AUC: 0.5462\n",
      "Epoch 73/300, Valid Loss: 0.6850, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 74/300, Train Loss: 0.6610, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 74/300, Valid Loss: 0.6343, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 75/300, Train Loss: 0.6430, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 75/300, Valid Loss: 0.6017, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 76/300, Train Loss: 0.6341, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 76/300, Valid Loss: 0.5788, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Train Loss: 0.6311, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Valid Loss: 0.5592, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 78/300, Train Loss: 0.6287, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 78/300, Valid Loss: 0.5581, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Train Loss: 0.6278, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Valid Loss: 0.5542, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Train Loss: 0.6302, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Valid Loss: 0.5547, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 81/300, Train Loss: 0.6287, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 81/300, Valid Loss: 0.5489, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 82/300, Train Loss: 0.6277, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 82/300, Valid Loss: 0.5528, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 83/300, Train Loss: 0.6279, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 83/300, Valid Loss: 0.5270, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 84/300, Train Loss: 0.6034, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 84/300, Valid Loss: 0.5460, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Train Loss: 0.5992, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Valid Loss: 0.5334, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 86/300, Train Loss: 0.5996, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 86/300, Valid Loss: 0.5684, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 87/300, Train Loss: 0.5838, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 87/300, Valid Loss: 0.5457, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 88/300, Train Loss: 0.6100, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 88/300, Valid Loss: 0.5363, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 89/300, Train Loss: 0.6024, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 89/300, Valid Loss: 0.5386, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 90/300, Train Loss: 0.5724, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 90/300, Valid Loss: 0.5190, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 91/300, Train Loss: 0.5695, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 91/300, Valid Loss: 0.5418, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 92/300, Train Loss: 0.5492, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 92/300, Valid Loss: 0.5706, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 93/300, Train Loss: 0.5903, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 93/300, Valid Loss: 0.5348, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 94/300, Train Loss: 0.5443, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 94/300, Valid Loss: 0.5660, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 95/300, Train Loss: 0.5634, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 95/300, Valid Loss: 0.5522, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Train Loss: 0.6098, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Valid Loss: 0.5873, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 97/300, Train Loss: 0.5383, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 97/300, Valid Loss: 0.5403, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 98/300, Train Loss: 0.5880, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 98/300, Valid Loss: 0.5371, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 99/300, Train Loss: 0.5357, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 99/300, Valid Loss: 0.5372, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 100/300, Train Loss: 0.5922, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 100/300, Valid Loss: 0.5706, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 101/300, Train Loss: 0.5367, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 101/300, Valid Loss: 0.5410, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 102/300, Train Loss: 0.5191, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 102/300, Valid Loss: 0.5872, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 103/300, Train Loss: 0.5392, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 103/300, Valid Loss: 0.5592, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 104/300, Train Loss: 0.5140, Acc: 0.6744, F1: 0.0345, AUC: 0.5046\n",
      "Epoch 104/300, Valid Loss: 0.5465, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 105/300, Train Loss: 0.5323, Acc: 0.6628, F1: 0.0000, AUC: 0.4914\n",
      "Epoch 105/300, Valid Loss: 0.5501, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 106/300, Train Loss: 0.5408, Acc: 0.6802, F1: 0.0351, AUC: 0.5089\n",
      "Epoch 106/300, Valid Loss: 0.5561, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 107/300, Train Loss: 0.5209, Acc: 0.6628, F1: 0.2162, AUC: 0.5283\n",
      "Epoch 107/300, Valid Loss: 0.5526, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 108/300, Train Loss: 0.5528, Acc: 0.6860, F1: 0.1562, AUC: 0.5317\n",
      "Epoch 108/300, Valid Loss: 0.5654, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 109/300, Train Loss: 0.5214, Acc: 0.6860, F1: 0.1000, AUC: 0.5225\n",
      "Epoch 109/300, Valid Loss: 0.5507, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 110/300, Train Loss: 0.5266, Acc: 0.6744, F1: 0.0968, AUC: 0.5139\n",
      "Epoch 110/300, Valid Loss: 0.6155, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 111/300, Train Loss: 0.5339, Acc: 0.6570, F1: 0.4158, AUC: 0.5841\n",
      "Epoch 111/300, Valid Loss: 0.6099, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 112/300, Train Loss: 0.5345, Acc: 0.6221, F1: 0.3434, AUC: 0.5397\n",
      "Epoch 112/300, Valid Loss: 0.5607, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 113/300, Train Loss: 0.5295, Acc: 0.6570, F1: 0.2532, AUC: 0.5333\n",
      "Epoch 113/300, Valid Loss: 0.5681, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 114/300, Train Loss: 0.5125, Acc: 0.6628, F1: 0.3556, AUC: 0.5653\n",
      "Epoch 114/300, Valid Loss: 0.5568, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 115/300, Train Loss: 0.5123, Acc: 0.6337, F1: 0.3883, AUC: 0.5622\n",
      "Epoch 115/300, Valid Loss: 0.5543, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 116/300, Train Loss: 0.5699, Acc: 0.6628, F1: 0.3409, AUC: 0.5607\n",
      "Epoch 116/300, Valid Loss: 0.6068, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 117/300, Train Loss: 0.5360, Acc: 0.6744, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 117/300, Valid Loss: 0.5826, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 118/300, Train Loss: 0.5478, Acc: 0.6744, F1: 0.0345, AUC: 0.5046\n",
      "Epoch 118/300, Valid Loss: 0.5467, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 119/300, Train Loss: 0.5389, Acc: 0.6744, F1: 0.0345, AUC: 0.5046\n",
      "Epoch 119/300, Valid Loss: 0.5534, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 120/300, Train Loss: 0.5179, Acc: 0.7035, F1: 0.4742, AUC: 0.6278\n",
      "Epoch 120/300, Valid Loss: 0.5381, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 121/300, Train Loss: 0.5061, Acc: 0.6919, F1: 0.5391, AUC: 0.6561\n",
      "Epoch 121/300, Valid Loss: 0.5848, Acc: 0.7442, F1: 0.2667, AUC: 0.5523\n",
      "Epoch 122/300, Train Loss: 0.5250, Acc: 0.6977, F1: 0.5185, AUC: 0.6466\n",
      "Epoch 122/300, Valid Loss: 0.5527, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 123/300, Train Loss: 0.4900, Acc: 0.7209, F1: 0.6190, AUC: 0.7146\n",
      "Epoch 123/300, Valid Loss: 0.5547, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 124/300, Train Loss: 0.4908, Acc: 0.6744, F1: 0.5942, AUC: 0.6893\n",
      "Epoch 124/300, Valid Loss: 0.7549, Acc: 0.8140, F1: 0.3333, AUC: 0.5964\n",
      "Epoch 125/300, Train Loss: 0.5454, Acc: 0.6744, F1: 0.5692, AUC: 0.6709\n",
      "Epoch 125/300, Valid Loss: 0.5625, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 126/300, Train Loss: 0.4825, Acc: 0.7151, F1: 0.6080, AUC: 0.7057\n",
      "Epoch 126/300, Valid Loss: 0.6076, Acc: 0.5349, F1: 0.3750, AUC: 0.5833\n",
      "Epoch 127/300, Train Loss: 0.5029, Acc: 0.6860, F1: 0.5970, AUC: 0.6933\n",
      "Epoch 127/300, Valid Loss: 0.6060, Acc: 0.5116, F1: 0.3636, AUC: 0.5686\n",
      "Epoch 128/300, Train Loss: 0.4821, Acc: 0.6919, F1: 0.5954, AUC: 0.6930\n",
      "Epoch 128/300, Valid Loss: 0.6259, Acc: 0.4419, F1: 0.3684, AUC: 0.5654\n",
      "Epoch 129/300, Train Loss: 0.5115, Acc: 0.7151, F1: 0.6316, AUC: 0.7241\n",
      "Epoch 129/300, Valid Loss: 0.5838, Acc: 0.5814, F1: 0.4000, AUC: 0.6127\n",
      "Epoch 130/300, Train Loss: 0.4856, Acc: 0.7209, F1: 0.6364, AUC: 0.7284\n",
      "Epoch 130/300, Valid Loss: 0.5899, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 131/300, Train Loss: 0.4841, Acc: 0.7326, F1: 0.6462, AUC: 0.7371\n",
      "Epoch 131/300, Valid Loss: 0.5791, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 132/300, Train Loss: 0.4779, Acc: 0.7558, F1: 0.6613, AUC: 0.7497\n",
      "Epoch 132/300, Valid Loss: 0.6851, Acc: 0.3721, F1: 0.3415, AUC: 0.5212\n",
      "Epoch 133/300, Train Loss: 0.4916, Acc: 0.6860, F1: 0.5714, AUC: 0.6749\n",
      "Epoch 133/300, Valid Loss: 0.6041, Acc: 0.5814, F1: 0.3571, AUC: 0.5719\n",
      "Epoch 134/300, Train Loss: 0.5168, Acc: 0.6802, F1: 0.6154, AUC: 0.7075\n",
      "Epoch 134/300, Valid Loss: 0.5778, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 135/300, Train Loss: 0.4727, Acc: 0.7326, F1: 0.6406, AUC: 0.7325\n",
      "Epoch 135/300, Valid Loss: 0.5709, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 136/300, Train Loss: 0.5147, Acc: 0.7209, F1: 0.6129, AUC: 0.7100\n",
      "Epoch 136/300, Valid Loss: 0.5411, Acc: 0.7442, F1: 0.4211, AUC: 0.6340\n",
      "Epoch 137/300, Train Loss: 0.4690, Acc: 0.7267, F1: 0.6466, AUC: 0.7374\n",
      "Epoch 137/300, Valid Loss: 0.5961, Acc: 0.6047, F1: 0.4138, AUC: 0.6275\n",
      "Epoch 138/300, Train Loss: 0.4854, Acc: 0.7093, F1: 0.6324, AUC: 0.7244\n",
      "Epoch 138/300, Valid Loss: 0.6529, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 139/300, Train Loss: 0.5116, Acc: 0.7035, F1: 0.6222, AUC: 0.7155\n",
      "Epoch 139/300, Valid Loss: 0.5509, Acc: 0.6512, F1: 0.2857, AUC: 0.5343\n",
      "Epoch 140/300, Train Loss: 0.4823, Acc: 0.7384, F1: 0.6763, AUC: 0.7645\n",
      "Epoch 140/300, Valid Loss: 0.5837, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 141/300, Train Loss: 0.4683, Acc: 0.7326, F1: 0.6567, AUC: 0.7463\n",
      "Epoch 141/300, Valid Loss: 0.5982, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 142/300, Train Loss: 0.5787, Acc: 0.5872, F1: 0.5749, AUC: 0.6570\n",
      "Epoch 142/300, Valid Loss: 0.5781, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 143/300, Train Loss: 0.4902, Acc: 0.6686, F1: 0.4865, AUC: 0.6204\n",
      "Epoch 143/300, Valid Loss: 0.6048, Acc: 0.7907, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 144/300, Train Loss: 0.4850, Acc: 0.7267, F1: 0.5841, AUC: 0.6912\n",
      "Epoch 144/300, Valid Loss: 0.6008, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 145/300, Train Loss: 0.4537, Acc: 0.7791, F1: 0.6833, AUC: 0.7669\n",
      "Epoch 145/300, Valid Loss: 0.6096, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 146/300, Train Loss: 0.4644, Acc: 0.7558, F1: 0.6667, AUC: 0.7543\n",
      "Epoch 146/300, Valid Loss: 0.6327, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 147/300, Train Loss: 0.5152, Acc: 0.6686, F1: 0.5839, AUC: 0.6804\n",
      "Epoch 147/300, Valid Loss: 0.7116, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 148/300, Train Loss: 0.5306, Acc: 0.7093, F1: 0.6269, AUC: 0.7198\n",
      "Epoch 148/300, Valid Loss: 0.5492, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 149/300, Train Loss: 0.4666, Acc: 0.7674, F1: 0.6774, AUC: 0.7629\n",
      "Epoch 149/300, Valid Loss: 0.5513, Acc: 0.6512, F1: 0.4000, AUC: 0.6160\n",
      "Epoch 150/300, Train Loss: 0.4514, Acc: 0.7674, F1: 0.6774, AUC: 0.7629\n",
      "Epoch 150/300, Valid Loss: 0.5898, Acc: 0.5814, F1: 0.3571, AUC: 0.5719\n",
      "Epoch 151/300, Train Loss: 0.4901, Acc: 0.7035, F1: 0.6383, AUC: 0.7294\n",
      "Epoch 151/300, Valid Loss: 0.5790, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 152/300, Train Loss: 0.5524, Acc: 0.6802, F1: 0.5985, AUC: 0.6937\n",
      "Epoch 152/300, Valid Loss: 0.5538, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 153/300, Train Loss: 0.5014, Acc: 0.6802, F1: 0.5985, AUC: 0.6937\n",
      "Epoch 153/300, Valid Loss: 0.5616, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 154/300, Train Loss: 0.5026, Acc: 0.6919, F1: 0.5760, AUC: 0.6792\n",
      "Epoch 154/300, Valid Loss: 0.5633, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 155/300, Train Loss: 0.4439, Acc: 0.7791, F1: 0.6935, AUC: 0.7762\n",
      "Epoch 155/300, Valid Loss: 0.6076, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 156/300, Train Loss: 0.4561, Acc: 0.7616, F1: 0.6870, AUC: 0.7725\n",
      "Epoch 156/300, Valid Loss: 0.6438, Acc: 0.6977, F1: 0.2353, AUC: 0.5229\n",
      "Epoch 157/300, Train Loss: 0.4970, Acc: 0.7500, F1: 0.6718, AUC: 0.7592\n",
      "Epoch 157/300, Valid Loss: 0.5919, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 158/300, Train Loss: 0.4707, Acc: 0.7384, F1: 0.6218, AUC: 0.7183\n",
      "Epoch 158/300, Valid Loss: 0.5791, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 159/300, Train Loss: 0.4200, Acc: 0.7791, F1: 0.6935, AUC: 0.7762\n",
      "Epoch 159/300, Valid Loss: 0.7450, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 160/300, Train Loss: 0.5228, Acc: 0.6744, F1: 0.5942, AUC: 0.6893\n",
      "Epoch 160/300, Valid Loss: 0.6379, Acc: 0.5349, F1: 0.3750, AUC: 0.5833\n",
      "Epoch 161/300, Train Loss: 0.4964, Acc: 0.6802, F1: 0.6259, AUC: 0.7167\n",
      "Epoch 161/300, Valid Loss: 0.6617, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 162/300, Train Loss: 0.5067, Acc: 0.6802, F1: 0.6099, AUC: 0.7029\n",
      "Epoch 162/300, Valid Loss: 0.6594, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 163/300, Train Loss: 0.5153, Acc: 0.6977, F1: 0.6338, AUC: 0.7251\n",
      "Epoch 163/300, Valid Loss: 0.6272, Acc: 0.4651, F1: 0.3429, AUC: 0.5392\n",
      "Epoch 164/300, Train Loss: 0.4506, Acc: 0.7384, F1: 0.6512, AUC: 0.7414\n",
      "Epoch 164/300, Valid Loss: 0.6422, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 165/300, Train Loss: 0.4935, Acc: 0.6686, F1: 0.6014, AUC: 0.6943\n",
      "Epoch 165/300, Valid Loss: 0.5667, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 166/300, Train Loss: 0.4339, Acc: 0.7674, F1: 0.6875, AUC: 0.7722\n",
      "Epoch 166/300, Valid Loss: 0.6537, Acc: 0.4884, F1: 0.4211, AUC: 0.6356\n",
      "Epoch 167/300, Train Loss: 0.4634, Acc: 0.7384, F1: 0.6565, AUC: 0.7460\n",
      "Epoch 167/300, Valid Loss: 0.6214, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 168/300, Train Loss: 0.3880, Acc: 0.8140, F1: 0.7500, AUC: 0.8251\n",
      "Epoch 168/300, Valid Loss: 0.6743, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 169/300, Train Loss: 0.5427, Acc: 0.5930, F1: 0.5783, AUC: 0.6613\n",
      "Epoch 169/300, Valid Loss: 0.6109, Acc: 0.5581, F1: 0.3871, AUC: 0.5980\n",
      "Epoch 170/300, Train Loss: 0.4177, Acc: 0.7965, F1: 0.7244, AUC: 0.8030\n",
      "Epoch 170/300, Valid Loss: 0.6278, Acc: 0.4884, F1: 0.3889, AUC: 0.5948\n",
      "Epoch 171/300, Train Loss: 0.4810, Acc: 0.7209, F1: 0.6620, AUC: 0.7515\n",
      "Epoch 171/300, Valid Loss: 0.6086, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 172/300, Train Loss: 0.5195, Acc: 0.6628, F1: 0.6329, AUC: 0.7223\n",
      "Epoch 172/300, Valid Loss: 0.7752, Acc: 0.7442, F1: 0.2667, AUC: 0.5523\n",
      "Epoch 173/300, Train Loss: 0.5269, Acc: 0.6628, F1: 0.5972, AUC: 0.6900\n",
      "Epoch 173/300, Valid Loss: 0.6170, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 174/300, Train Loss: 0.4133, Acc: 0.7907, F1: 0.7231, AUC: 0.8033\n",
      "Epoch 174/300, Valid Loss: 0.7796, Acc: 0.6977, F1: 0.2353, AUC: 0.5229\n",
      "Epoch 175/300, Train Loss: 0.4869, Acc: 0.7733, F1: 0.6880, AUC: 0.7719\n",
      "Epoch 175/300, Valid Loss: 0.5882, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 176/300, Train Loss: 0.4931, Acc: 0.7384, F1: 0.6400, AUC: 0.7321\n",
      "Epoch 176/300, Valid Loss: 0.5723, Acc: 0.6512, F1: 0.4444, AUC: 0.6569\n",
      "Epoch 177/300, Train Loss: 0.4880, Acc: 0.6977, F1: 0.6232, AUC: 0.7158\n",
      "Epoch 177/300, Valid Loss: 0.6351, Acc: 0.6977, F1: 0.2353, AUC: 0.5229\n",
      "Epoch 178/300, Train Loss: 0.4336, Acc: 0.7965, F1: 0.7059, AUC: 0.7845\n",
      "Epoch 178/300, Valid Loss: 0.5782, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 179/300, Train Loss: 0.4345, Acc: 0.7733, F1: 0.6929, AUC: 0.7765\n",
      "Epoch 179/300, Valid Loss: 0.5966, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 180/300, Train Loss: 0.5283, Acc: 0.8198, F1: 0.7156, AUC: 0.7879\n",
      "Epoch 180/300, Valid Loss: 0.5412, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 181/300, Train Loss: 0.4934, Acc: 0.7035, F1: 0.6383, AUC: 0.7294\n",
      "Epoch 181/300, Valid Loss: 0.5651, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 182/300, Train Loss: 0.4606, Acc: 0.7500, F1: 0.6861, AUC: 0.7731\n",
      "Epoch 182/300, Valid Loss: 0.7216, Acc: 0.3488, F1: 0.3636, AUC: 0.5474\n",
      "Epoch 183/300, Train Loss: 0.4905, Acc: 0.6802, F1: 0.6405, AUC: 0.7306\n",
      "Epoch 183/300, Valid Loss: 0.7479, Acc: 0.7674, F1: 0.2857, AUC: 0.5670\n",
      "Epoch 184/300, Train Loss: 0.4214, Acc: 0.7849, F1: 0.7176, AUC: 0.7990\n",
      "Epoch 184/300, Valid Loss: 0.6170, Acc: 0.5116, F1: 0.4000, AUC: 0.6095\n",
      "Epoch 185/300, Train Loss: 0.4797, Acc: 0.7384, F1: 0.6853, AUC: 0.7737\n",
      "Epoch 185/300, Valid Loss: 0.7458, Acc: 0.7442, F1: 0.3529, AUC: 0.5931\n",
      "Epoch 186/300, Train Loss: 0.4447, Acc: 0.7616, F1: 0.6720, AUC: 0.7586\n",
      "Epoch 186/300, Valid Loss: 0.6227, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 187/300, Train Loss: 0.4513, Acc: 0.7674, F1: 0.6923, AUC: 0.7768\n",
      "Epoch 187/300, Valid Loss: 0.6597, Acc: 0.7442, F1: 0.3529, AUC: 0.5931\n",
      "Epoch 188/300, Train Loss: 0.4809, Acc: 0.7035, F1: 0.6383, AUC: 0.7294\n",
      "Epoch 188/300, Valid Loss: 0.5860, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 189/300, Train Loss: 0.4131, Acc: 0.8140, F1: 0.7500, AUC: 0.8251\n",
      "Epoch 189/300, Valid Loss: 0.5886, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 190/300, Train Loss: 0.4506, Acc: 0.7558, F1: 0.6769, AUC: 0.7635\n",
      "Epoch 190/300, Valid Loss: 0.6368, Acc: 0.6512, F1: 0.2857, AUC: 0.5343\n",
      "Epoch 191/300, Train Loss: 0.4085, Acc: 0.7674, F1: 0.7015, AUC: 0.7860\n",
      "Epoch 191/300, Valid Loss: 0.6418, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 192/300, Train Loss: 0.5070, Acc: 0.6919, F1: 0.6294, AUC: 0.7208\n",
      "Epoch 192/300, Valid Loss: 0.6330, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 193/300, Train Loss: 0.4367, Acc: 0.7674, F1: 0.6825, AUC: 0.7675\n",
      "Epoch 193/300, Valid Loss: 0.6491, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 194/300, Train Loss: 0.4408, Acc: 0.7674, F1: 0.6970, AUC: 0.7814\n",
      "Epoch 194/300, Valid Loss: 0.7276, Acc: 0.3721, F1: 0.3721, AUC: 0.5621\n",
      "Epoch 195/300, Train Loss: 0.4559, Acc: 0.7442, F1: 0.6901, AUC: 0.7780\n",
      "Epoch 195/300, Valid Loss: 0.6919, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 196/300, Train Loss: 0.4142, Acc: 0.8198, F1: 0.7597, AUC: 0.8341\n",
      "Epoch 196/300, Valid Loss: 0.6925, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 197/300, Train Loss: 0.4036, Acc: 0.7558, F1: 0.6957, AUC: 0.7820\n",
      "Epoch 197/300, Valid Loss: 0.9675, Acc: 0.7907, F1: 0.4000, AUC: 0.6225\n",
      "Epoch 198/300, Train Loss: 0.4749, Acc: 0.7384, F1: 0.6897, AUC: 0.7783\n",
      "Epoch 198/300, Valid Loss: 0.7231, Acc: 0.3256, F1: 0.3556, AUC: 0.5327\n",
      "Epoch 199/300, Train Loss: 0.4749, Acc: 0.6919, F1: 0.6536, AUC: 0.7438\n",
      "Epoch 199/300, Valid Loss: 0.5990, Acc: 0.6047, F1: 0.3704, AUC: 0.5866\n",
      "Epoch 200/300, Train Loss: 0.4038, Acc: 0.7674, F1: 0.7143, AUC: 0.7999\n",
      "Epoch 200/300, Valid Loss: 0.8192, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 201/300, Train Loss: 0.4893, Acc: 0.7151, F1: 0.6423, AUC: 0.7334\n",
      "Epoch 201/300, Valid Loss: 0.5832, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 202/300, Train Loss: 0.4489, Acc: 0.7209, F1: 0.6571, AUC: 0.7469\n",
      "Epoch 202/300, Valid Loss: 0.6314, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 203/300, Train Loss: 0.4395, Acc: 0.7733, F1: 0.7068, AUC: 0.7903\n",
      "Epoch 203/300, Valid Loss: 0.6029, Acc: 0.5581, F1: 0.3871, AUC: 0.5980\n",
      "Epoch 204/300, Train Loss: 0.4564, Acc: 0.7733, F1: 0.7111, AUC: 0.7950\n",
      "Epoch 204/300, Valid Loss: 0.6975, Acc: 0.7674, F1: 0.3750, AUC: 0.6078\n",
      "Epoch 205/300, Train Loss: 0.4176, Acc: 0.8023, F1: 0.7385, AUC: 0.8165\n",
      "Epoch 205/300, Valid Loss: 0.7457, Acc: 0.7907, F1: 0.4000, AUC: 0.6225\n",
      "Epoch 206/300, Train Loss: 0.4276, Acc: 0.7733, F1: 0.6929, AUC: 0.7765\n",
      "Epoch 206/300, Valid Loss: 0.6658, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 207/300, Train Loss: 0.4292, Acc: 0.7616, F1: 0.7050, AUC: 0.7909\n",
      "Epoch 207/300, Valid Loss: 0.7473, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 208/300, Train Loss: 0.4371, Acc: 0.7616, F1: 0.6870, AUC: 0.7725\n",
      "Epoch 208/300, Valid Loss: 0.6405, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 209/300, Train Loss: 0.4398, Acc: 0.8081, F1: 0.7442, AUC: 0.8208\n",
      "Epoch 209/300, Valid Loss: 0.5787, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 210/300, Train Loss: 0.4843, Acc: 0.7093, F1: 0.6479, AUC: 0.7383\n",
      "Epoch 210/300, Valid Loss: 0.5841, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 211/300, Train Loss: 0.4563, Acc: 0.7267, F1: 0.6619, AUC: 0.7512\n",
      "Epoch 211/300, Valid Loss: 0.7129, Acc: 0.3488, F1: 0.3636, AUC: 0.5474\n",
      "Epoch 212/300, Train Loss: 0.4420, Acc: 0.7616, F1: 0.7050, AUC: 0.7909\n",
      "Epoch 212/300, Valid Loss: 0.7246, Acc: 0.7674, F1: 0.3750, AUC: 0.6078\n",
      "Epoch 213/300, Train Loss: 0.4048, Acc: 0.7965, F1: 0.7107, AUC: 0.7891\n",
      "Epoch 213/300, Valid Loss: 0.6117, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 214/300, Train Loss: 0.4688, Acc: 0.7500, F1: 0.6906, AUC: 0.7777\n",
      "Epoch 214/300, Valid Loss: 0.6377, Acc: 0.7674, F1: 0.2857, AUC: 0.5670\n",
      "Epoch 215/300, Train Loss: 0.3965, Acc: 0.8023, F1: 0.7167, AUC: 0.7934\n",
      "Epoch 215/300, Valid Loss: 0.6395, Acc: 0.7442, F1: 0.2667, AUC: 0.5523\n",
      "Epoch 216/300, Train Loss: 0.4784, Acc: 0.7151, F1: 0.6573, AUC: 0.7472\n",
      "Epoch 216/300, Valid Loss: 0.6491, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 217/300, Train Loss: 0.4170, Acc: 0.7965, F1: 0.7244, AUC: 0.8030\n",
      "Epoch 217/300, Valid Loss: 0.6620, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 218/300, Train Loss: 0.3962, Acc: 0.7791, F1: 0.7031, AUC: 0.7854\n",
      "Epoch 218/300, Valid Loss: 0.6313, Acc: 0.6744, F1: 0.4167, AUC: 0.6307\n",
      "Epoch 219/300, Train Loss: 0.4723, Acc: 0.7093, F1: 0.6479, AUC: 0.7383\n",
      "Epoch 219/300, Valid Loss: 0.6782, Acc: 0.4186, F1: 0.3902, AUC: 0.5915\n",
      "Epoch 220/300, Train Loss: 0.4068, Acc: 0.7733, F1: 0.7153, AUC: 0.7996\n",
      "Epoch 220/300, Valid Loss: 0.9524, Acc: 0.7907, F1: 0.3077, AUC: 0.5817\n",
      "Epoch 221/300, Train Loss: 0.4208, Acc: 0.7907, F1: 0.7353, AUC: 0.8171\n",
      "Epoch 221/300, Valid Loss: 0.7507, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 222/300, Train Loss: 0.4152, Acc: 0.7733, F1: 0.7023, AUC: 0.7857\n",
      "Epoch 222/300, Valid Loss: 0.6991, Acc: 0.4186, F1: 0.3902, AUC: 0.5915\n",
      "Epoch 223/300, Train Loss: 0.4672, Acc: 0.7500, F1: 0.7114, AUC: 0.8008\n",
      "Epoch 223/300, Valid Loss: 0.5888, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 224/300, Train Loss: 0.4468, Acc: 0.7674, F1: 0.7015, AUC: 0.7860\n",
      "Epoch 224/300, Valid Loss: 0.6564, Acc: 0.5116, F1: 0.4324, AUC: 0.6503\n",
      "Epoch 225/300, Train Loss: 0.4126, Acc: 0.7791, F1: 0.7121, AUC: 0.7946\n",
      "Epoch 225/300, Valid Loss: 0.7363, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 226/300, Train Loss: 0.4966, Acc: 0.7267, F1: 0.6619, AUC: 0.7512\n",
      "Epoch 226/300, Valid Loss: 0.6165, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 227/300, Train Loss: 0.4081, Acc: 0.7791, F1: 0.7164, AUC: 0.7993\n",
      "Epoch 227/300, Valid Loss: 0.6196, Acc: 0.6512, F1: 0.4000, AUC: 0.6160\n",
      "Epoch 228/300, Train Loss: 0.4076, Acc: 0.7674, F1: 0.6970, AUC: 0.7814\n",
      "Epoch 228/300, Valid Loss: 0.5865, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 229/300, Train Loss: 0.4187, Acc: 0.7791, F1: 0.7077, AUC: 0.7900\n",
      "Epoch 229/300, Valid Loss: 0.6118, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 230/300, Train Loss: 0.4278, Acc: 0.7500, F1: 0.6993, AUC: 0.7869\n",
      "Epoch 230/300, Valid Loss: 0.7459, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 231/300, Train Loss: 0.4297, Acc: 0.7500, F1: 0.6906, AUC: 0.7777\n",
      "Epoch 231/300, Valid Loss: 0.7557, Acc: 0.6744, F1: 0.2222, AUC: 0.5082\n",
      "Epoch 232/300, Train Loss: 0.4500, Acc: 0.7151, F1: 0.6711, AUC: 0.7611\n",
      "Epoch 232/300, Valid Loss: 0.8277, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 233/300, Train Loss: 0.4376, Acc: 0.7674, F1: 0.7059, AUC: 0.7906\n",
      "Epoch 233/300, Valid Loss: 0.5855, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 234/300, Train Loss: 0.3765, Acc: 0.7965, F1: 0.7244, AUC: 0.8030\n",
      "Epoch 234/300, Valid Loss: 0.6848, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 235/300, Train Loss: 0.4169, Acc: 0.7616, F1: 0.6963, AUC: 0.7817\n",
      "Epoch 235/300, Valid Loss: 0.9538, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 236/300, Train Loss: 0.4105, Acc: 0.7907, F1: 0.7313, AUC: 0.8125\n",
      "Epoch 236/300, Valid Loss: 0.9230, Acc: 0.7907, F1: 0.3077, AUC: 0.5817\n",
      "Epoch 237/300, Train Loss: 0.4211, Acc: 0.7558, F1: 0.6866, AUC: 0.7728\n",
      "Epoch 237/300, Valid Loss: 0.6042, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 238/300, Train Loss: 0.4198, Acc: 0.7791, F1: 0.7121, AUC: 0.7946\n",
      "Epoch 238/300, Valid Loss: 0.6489, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 239/300, Train Loss: 0.4549, Acc: 0.7267, F1: 0.6803, AUC: 0.7697\n",
      "Epoch 239/300, Valid Loss: 0.5979, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 240/300, Train Loss: 0.4353, Acc: 0.7849, F1: 0.7218, AUC: 0.8036\n",
      "Epoch 240/300, Valid Loss: 0.7249, Acc: 0.7674, F1: 0.2857, AUC: 0.5670\n",
      "Epoch 241/300, Train Loss: 0.4643, Acc: 0.7384, F1: 0.6715, AUC: 0.7599\n",
      "Epoch 241/300, Valid Loss: 0.6409, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 242/300, Train Loss: 0.3961, Acc: 0.7558, F1: 0.6769, AUC: 0.7635\n",
      "Epoch 242/300, Valid Loss: 0.7689, Acc: 0.7442, F1: 0.4211, AUC: 0.6340\n",
      "Epoch 243/300, Train Loss: 0.4077, Acc: 0.7733, F1: 0.7153, AUC: 0.7996\n",
      "Epoch 243/300, Valid Loss: 0.6385, Acc: 0.5116, F1: 0.4000, AUC: 0.6095\n",
      "Epoch 244/300, Train Loss: 0.4172, Acc: 0.7616, F1: 0.7050, AUC: 0.7909\n",
      "Epoch 244/300, Valid Loss: 0.8184, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 245/300, Train Loss: 0.3995, Acc: 0.7791, F1: 0.7121, AUC: 0.7946\n",
      "Epoch 245/300, Valid Loss: 0.9773, Acc: 0.7442, F1: 0.3529, AUC: 0.5931\n",
      "Epoch 246/300, Train Loss: 0.4418, Acc: 0.7674, F1: 0.7143, AUC: 0.7999\n",
      "Epoch 246/300, Valid Loss: 0.6191, Acc: 0.6047, F1: 0.3704, AUC: 0.5866\n",
      "Epoch 247/300, Train Loss: 0.4203, Acc: 0.7616, F1: 0.7007, AUC: 0.7863\n",
      "Epoch 247/300, Valid Loss: 0.6402, Acc: 0.6512, F1: 0.4000, AUC: 0.6160\n",
      "Epoch 248/300, Train Loss: 0.4177, Acc: 0.7849, F1: 0.7176, AUC: 0.7990\n",
      "Epoch 248/300, Valid Loss: 0.6274, Acc: 0.5814, F1: 0.4375, AUC: 0.6536\n",
      "Epoch 249/300, Train Loss: 0.3781, Acc: 0.7965, F1: 0.7200, AUC: 0.7983\n",
      "Epoch 249/300, Valid Loss: 0.6421, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 250/300, Train Loss: 0.4333, Acc: 0.7500, F1: 0.7034, AUC: 0.7916\n",
      "Epoch 250/300, Valid Loss: 1.0486, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 251/300, Train Loss: 0.4297, Acc: 0.7442, F1: 0.6667, AUC: 0.7549\n",
      "Epoch 251/300, Valid Loss: 0.7492, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 252/300, Train Loss: 0.4491, Acc: 0.7674, F1: 0.7143, AUC: 0.7999\n",
      "Epoch 252/300, Valid Loss: 0.6742, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 253/300, Train Loss: 0.3873, Acc: 0.8023, F1: 0.7302, AUC: 0.8073\n",
      "Epoch 253/300, Valid Loss: 0.6961, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 254/300, Train Loss: 0.3962, Acc: 0.7791, F1: 0.7206, AUC: 0.8039\n",
      "Epoch 254/300, Valid Loss: 0.9957, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 255/300, Train Loss: 0.3972, Acc: 0.7674, F1: 0.7101, AUC: 0.7953\n",
      "Epoch 255/300, Valid Loss: 0.8176, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 256/300, Train Loss: 0.4497, Acc: 0.7500, F1: 0.6993, AUC: 0.7869\n",
      "Epoch 256/300, Valid Loss: 0.7824, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 257/300, Train Loss: 0.3902, Acc: 0.8081, F1: 0.7317, AUC: 0.8070\n",
      "Epoch 257/300, Valid Loss: 0.7535, Acc: 0.7209, F1: 0.3333, AUC: 0.5784\n",
      "Epoch 258/300, Train Loss: 0.3940, Acc: 0.8023, F1: 0.7344, AUC: 0.8119\n",
      "Epoch 258/300, Valid Loss: 0.7623, Acc: 0.3721, F1: 0.3721, AUC: 0.5621\n",
      "Epoch 259/300, Train Loss: 0.3499, Acc: 0.8314, F1: 0.7752, AUC: 0.8473\n",
      "Epoch 259/300, Valid Loss: 0.6931, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 260/300, Train Loss: 0.3974, Acc: 0.7907, F1: 0.7231, AUC: 0.8033\n",
      "Epoch 260/300, Valid Loss: 0.7027, Acc: 0.5581, F1: 0.4242, AUC: 0.6389\n",
      "Epoch 261/300, Train Loss: 0.4159, Acc: 0.7733, F1: 0.7194, AUC: 0.8042\n",
      "Epoch 261/300, Valid Loss: 0.8059, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 262/300, Train Loss: 0.3921, Acc: 0.7965, F1: 0.7407, AUC: 0.8214\n",
      "Epoch 262/300, Valid Loss: 0.6336, Acc: 0.6279, F1: 0.3846, AUC: 0.6013\n",
      "Epoch 263/300, Train Loss: 0.3616, Acc: 0.8081, F1: 0.7442, AUC: 0.8208\n",
      "Epoch 263/300, Valid Loss: 0.6843, Acc: 0.6512, F1: 0.4000, AUC: 0.6160\n",
      "Epoch 264/300, Train Loss: 0.4123, Acc: 0.7791, F1: 0.7246, AUC: 0.8085\n",
      "Epoch 264/300, Valid Loss: 0.7185, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 265/300, Train Loss: 0.3855, Acc: 0.7849, F1: 0.7376, AUC: 0.8220\n",
      "Epoch 265/300, Valid Loss: 0.7891, Acc: 0.6744, F1: 0.3000, AUC: 0.5490\n",
      "Epoch 266/300, Train Loss: 0.3952, Acc: 0.8023, F1: 0.7344, AUC: 0.8119\n",
      "Epoch 266/300, Valid Loss: 0.6797, Acc: 0.5814, F1: 0.4000, AUC: 0.6127\n",
      "Epoch 267/300, Train Loss: 0.3889, Acc: 0.7907, F1: 0.7353, AUC: 0.8171\n",
      "Epoch 267/300, Valid Loss: 0.6620, Acc: 0.6512, F1: 0.4444, AUC: 0.6569\n",
      "Epoch 268/300, Train Loss: 0.3641, Acc: 0.8314, F1: 0.7752, AUC: 0.8473\n",
      "Epoch 268/300, Valid Loss: 0.7151, Acc: 0.6047, F1: 0.3200, AUC: 0.5458\n",
      "Epoch 269/300, Train Loss: 0.4020, Acc: 0.7849, F1: 0.7338, AUC: 0.8174\n",
      "Epoch 269/300, Valid Loss: 0.8331, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 270/300, Train Loss: 0.3624, Acc: 0.7965, F1: 0.7368, AUC: 0.8168\n",
      "Epoch 270/300, Valid Loss: 0.9165, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 271/300, Train Loss: 0.3702, Acc: 0.8023, F1: 0.7424, AUC: 0.8211\n",
      "Epoch 271/300, Valid Loss: 0.6985, Acc: 0.5116, F1: 0.4000, AUC: 0.6095\n",
      "Epoch 272/300, Train Loss: 0.5050, Acc: 0.6977, F1: 0.6623, AUC: 0.7528\n",
      "Epoch 272/300, Valid Loss: 0.9235, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 273/300, Train Loss: 0.3680, Acc: 0.8256, F1: 0.7541, AUC: 0.8245\n",
      "Epoch 273/300, Valid Loss: 0.8521, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 274/300, Train Loss: 0.3894, Acc: 0.7849, F1: 0.7338, AUC: 0.8174\n",
      "Epoch 274/300, Valid Loss: 0.8967, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 275/300, Train Loss: 0.4231, Acc: 0.7500, F1: 0.6993, AUC: 0.7869\n",
      "Epoch 275/300, Valid Loss: 0.6902, Acc: 0.6047, F1: 0.3704, AUC: 0.5866\n",
      "Epoch 276/300, Train Loss: 0.4327, Acc: 0.7442, F1: 0.6667, AUC: 0.7549\n",
      "Epoch 276/300, Valid Loss: 0.6733, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 277/300, Train Loss: 0.4782, Acc: 0.7558, F1: 0.6957, AUC: 0.7820\n",
      "Epoch 277/300, Valid Loss: 0.6371, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "Epoch 278/300, Train Loss: 0.4144, Acc: 0.7849, F1: 0.7218, AUC: 0.8036\n",
      "Epoch 278/300, Valid Loss: 0.7104, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 279/300, Train Loss: 0.3933, Acc: 0.7733, F1: 0.7111, AUC: 0.7950\n",
      "Epoch 279/300, Valid Loss: 0.7028, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 280/300, Train Loss: 0.3737, Acc: 0.8314, F1: 0.7642, AUC: 0.8334\n",
      "Epoch 280/300, Valid Loss: 0.6272, Acc: 0.6512, F1: 0.4444, AUC: 0.6569\n",
      "Epoch 281/300, Train Loss: 0.4000, Acc: 0.7907, F1: 0.7231, AUC: 0.8033\n",
      "Epoch 281/300, Valid Loss: 0.7425, Acc: 0.7674, F1: 0.3750, AUC: 0.6078\n",
      "Epoch 282/300, Train Loss: 0.3582, Acc: 0.8140, F1: 0.7538, AUC: 0.8297\n",
      "Epoch 282/300, Valid Loss: 0.7175, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 283/300, Train Loss: 0.3793, Acc: 0.8023, F1: 0.7500, AUC: 0.8304\n",
      "Epoch 283/300, Valid Loss: 0.7250, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 284/300, Train Loss: 0.4115, Acc: 0.8140, F1: 0.7612, AUC: 0.8390\n",
      "Epoch 284/300, Valid Loss: 0.6825, Acc: 0.6977, F1: 0.3810, AUC: 0.6046\n",
      "Epoch 285/300, Train Loss: 0.4022, Acc: 0.7907, F1: 0.7313, AUC: 0.8125\n",
      "Epoch 285/300, Valid Loss: 0.6004, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 286/300, Train Loss: 0.3579, Acc: 0.8140, F1: 0.7538, AUC: 0.8297\n",
      "Epoch 286/300, Valid Loss: 0.8135, Acc: 0.7209, F1: 0.4000, AUC: 0.6193\n",
      "Epoch 287/300, Train Loss: 0.3552, Acc: 0.8140, F1: 0.7538, AUC: 0.8297\n",
      "Epoch 287/300, Valid Loss: 0.6391, Acc: 0.5814, F1: 0.4375, AUC: 0.6536\n",
      "Epoch 288/300, Train Loss: 0.4142, Acc: 0.7849, F1: 0.7376, AUC: 0.8220\n",
      "Epoch 288/300, Valid Loss: 0.5969, Acc: 0.6279, F1: 0.4286, AUC: 0.6422\n",
      "Epoch 289/300, Train Loss: 0.3451, Acc: 0.8081, F1: 0.7442, AUC: 0.8208\n",
      "Epoch 289/300, Valid Loss: 1.1117, Acc: 0.8140, F1: 0.3333, AUC: 0.5964\n",
      "Epoch 290/300, Train Loss: 0.3840, Acc: 0.8140, F1: 0.7612, AUC: 0.8390\n",
      "Epoch 290/300, Valid Loss: 0.8560, Acc: 0.7442, F1: 0.4211, AUC: 0.6340\n",
      "Epoch 291/300, Train Loss: 0.3685, Acc: 0.8023, F1: 0.7258, AUC: 0.8026\n",
      "Epoch 291/300, Valid Loss: 0.6819, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 292/300, Train Loss: 0.4349, Acc: 0.7442, F1: 0.6944, AUC: 0.7826\n",
      "Epoch 292/300, Valid Loss: 0.8700, Acc: 0.6977, F1: 0.3158, AUC: 0.5637\n",
      "Epoch 293/300, Train Loss: 0.3405, Acc: 0.8314, F1: 0.7680, AUC: 0.8381\n",
      "Epoch 293/300, Valid Loss: 1.0732, Acc: 0.7209, F1: 0.2500, AUC: 0.5376\n",
      "Epoch 294/300, Train Loss: 0.4438, Acc: 0.7267, F1: 0.6759, AUC: 0.7651\n",
      "Epoch 294/300, Valid Loss: 0.7260, Acc: 0.3721, F1: 0.3721, AUC: 0.5621\n",
      "Epoch 295/300, Train Loss: 0.4699, Acc: 0.7093, F1: 0.6711, AUC: 0.7614\n",
      "Epoch 295/300, Valid Loss: 0.7844, Acc: 0.6279, F1: 0.3333, AUC: 0.5605\n",
      "Epoch 296/300, Train Loss: 0.3854, Acc: 0.7907, F1: 0.7273, AUC: 0.8079\n",
      "Epoch 296/300, Valid Loss: 0.6474, Acc: 0.5814, F1: 0.4706, AUC: 0.6944\n",
      "Epoch 297/300, Train Loss: 0.3779, Acc: 0.8023, F1: 0.7571, AUC: 0.8396\n",
      "Epoch 297/300, Valid Loss: 1.2741, Acc: 0.8140, F1: 0.3333, AUC: 0.5964\n",
      "Epoch 298/300, Train Loss: 0.3984, Acc: 0.7733, F1: 0.7194, AUC: 0.8042\n",
      "Epoch 298/300, Valid Loss: 0.7019, Acc: 0.3953, F1: 0.3810, AUC: 0.5768\n",
      "Epoch 299/300, Train Loss: 0.5448, Acc: 0.6163, F1: 0.6118, AUC: 0.6970\n",
      "Epoch 299/300, Valid Loss: 0.6264, Acc: 0.6744, F1: 0.3636, AUC: 0.5899\n",
      "Epoch 300/300, Train Loss: 0.3881, Acc: 0.8140, F1: 0.7538, AUC: 0.8297\n",
      "Epoch 300/300, Valid Loss: 0.5644, Acc: 0.6512, F1: 0.3478, AUC: 0.5752\n",
      "-- Fold 5/5 --\n",
      "Epoch 1/300, Train Loss: 0.6675, Acc: 0.6802, F1: 0.0000, AUC: 0.4875\n",
      "Epoch 1/300, Valid Loss: 0.6286, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Train Loss: 0.6420, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 2/300, Valid Loss: 0.6106, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Train Loss: 0.6291, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 3/300, Valid Loss: 0.6126, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 4/300, Train Loss: 0.6555, Acc: 0.6686, F1: 0.0952, AUC: 0.4955\n",
      "Epoch 4/300, Valid Loss: 0.6151, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Train Loss: 0.6424, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 5/300, Valid Loss: 0.6286, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Train Loss: 0.6152, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 6/300, Valid Loss: 0.6604, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Train Loss: 0.6352, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 7/300, Valid Loss: 0.6413, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Train Loss: 0.6378, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 8/300, Valid Loss: 0.6225, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Train Loss: 0.6209, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 9/300, Valid Loss: 0.6367, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Train Loss: 0.6181, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 10/300, Valid Loss: 0.6252, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Train Loss: 0.6131, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 11/300, Valid Loss: 0.6095, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Train Loss: 0.6301, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 12/300, Valid Loss: 0.6121, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Train Loss: 0.6131, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 13/300, Valid Loss: 0.6326, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Train Loss: 0.6171, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 14/300, Valid Loss: 0.6255, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Train Loss: 0.6150, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 15/300, Valid Loss: 0.6160, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Train Loss: 0.6193, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 16/300, Valid Loss: 0.6188, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Train Loss: 0.6056, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 17/300, Valid Loss: 0.6060, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Train Loss: 0.6150, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 18/300, Valid Loss: 0.6137, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Train Loss: 0.6225, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 19/300, Valid Loss: 0.6127, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Train Loss: 0.6210, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 20/300, Valid Loss: 0.6130, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Train Loss: 0.6232, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 21/300, Valid Loss: 0.6150, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Train Loss: 0.6233, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 22/300, Valid Loss: 0.6083, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Train Loss: 0.6057, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 23/300, Valid Loss: 0.6016, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Train Loss: 0.6029, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 24/300, Valid Loss: 0.6024, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Train Loss: 0.6197, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 25/300, Valid Loss: 0.6035, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Train Loss: 0.5987, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 26/300, Valid Loss: 0.5979, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Train Loss: 0.6053, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 27/300, Valid Loss: 0.6116, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Train Loss: 0.6151, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 28/300, Valid Loss: 0.6073, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Train Loss: 0.6007, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 29/300, Valid Loss: 0.6119, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Train Loss: 0.5940, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 30/300, Valid Loss: 0.5998, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Train Loss: 0.5991, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 31/300, Valid Loss: 0.6021, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Train Loss: 0.6041, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 32/300, Valid Loss: 0.5917, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Train Loss: 0.5940, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 33/300, Valid Loss: 0.5848, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Train Loss: 0.6111, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 34/300, Valid Loss: 0.6070, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Train Loss: 0.5855, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 35/300, Valid Loss: 0.5880, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Train Loss: 0.5989, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 36/300, Valid Loss: 0.5880, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Train Loss: 0.6027, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 37/300, Valid Loss: 0.5844, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Train Loss: 0.5989, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 38/300, Valid Loss: 0.5849, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Train Loss: 0.5878, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 39/300, Valid Loss: 0.5886, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Train Loss: 0.5840, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 40/300, Valid Loss: 0.5811, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Train Loss: 0.6256, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 41/300, Valid Loss: 0.6135, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Train Loss: 0.6110, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 42/300, Valid Loss: 0.6107, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 43/300, Train Loss: 0.5946, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 43/300, Valid Loss: 0.6004, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Train Loss: 0.5882, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 44/300, Valid Loss: 0.5869, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Train Loss: 0.5803, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 45/300, Valid Loss: 0.5833, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Train Loss: 0.5931, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 46/300, Valid Loss: 0.5803, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Train Loss: 0.5977, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 47/300, Valid Loss: 0.5927, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Train Loss: 0.5858, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 48/300, Valid Loss: 0.6098, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Train Loss: 0.5970, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 49/300, Valid Loss: 0.5865, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Train Loss: 0.6039, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 50/300, Valid Loss: 0.6133, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Train Loss: 0.6034, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 51/300, Valid Loss: 0.6282, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Train Loss: 0.6212, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 52/300, Valid Loss: 0.6159, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 53/300, Train Loss: 0.6162, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 53/300, Valid Loss: 0.6050, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 54/300, Train Loss: 0.6105, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 54/300, Valid Loss: 0.6131, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 55/300, Train Loss: 0.6146, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 55/300, Valid Loss: 0.6026, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Train Loss: 0.6177, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 56/300, Valid Loss: 0.6102, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Train Loss: 0.6090, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 57/300, Valid Loss: 0.6062, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 58/300, Train Loss: 0.6020, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 58/300, Valid Loss: 0.6103, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 59/300, Train Loss: 0.6030, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 59/300, Valid Loss: 0.6555, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 60/300, Train Loss: 0.6196, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 60/300, Valid Loss: 0.6051, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 61/300, Train Loss: 0.5906, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 61/300, Valid Loss: 0.6076, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 62/300, Train Loss: 0.5919, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 62/300, Valid Loss: 0.6118, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 63/300, Train Loss: 0.5794, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 63/300, Valid Loss: 0.6062, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 64/300, Train Loss: 0.5967, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 64/300, Valid Loss: 0.5969, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 65/300, Train Loss: 0.5882, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 65/300, Valid Loss: 0.5979, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 66/300, Train Loss: 0.5887, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 66/300, Valid Loss: 0.6071, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 67/300, Train Loss: 0.5897, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 67/300, Valid Loss: 0.5950, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 68/300, Train Loss: 0.5765, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 68/300, Valid Loss: 0.5892, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 69/300, Train Loss: 0.5782, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 69/300, Valid Loss: 0.5916, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 70/300, Train Loss: 0.5939, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 70/300, Valid Loss: 0.6244, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 71/300, Train Loss: 0.5917, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 71/300, Valid Loss: 0.5927, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Train Loss: 0.5591, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 72/300, Valid Loss: 0.5752, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 73/300, Train Loss: 0.5645, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 73/300, Valid Loss: 0.5720, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 74/300, Train Loss: 0.5382, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 74/300, Valid Loss: 0.6411, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 75/300, Train Loss: 0.5514, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 75/300, Valid Loss: 0.5790, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 76/300, Train Loss: 0.5775, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 76/300, Valid Loss: 0.6343, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Train Loss: 0.6044, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 77/300, Valid Loss: 0.5946, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 78/300, Train Loss: 0.6000, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 78/300, Valid Loss: 0.6233, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Train Loss: 0.6186, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 79/300, Valid Loss: 0.6144, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Train Loss: 0.6102, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 80/300, Valid Loss: 0.6063, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 81/300, Train Loss: 0.5522, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 81/300, Valid Loss: 0.6222, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 82/300, Train Loss: 0.6141, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 82/300, Valid Loss: 0.6086, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 83/300, Train Loss: 0.5643, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 83/300, Valid Loss: 0.5880, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 84/300, Train Loss: 0.5481, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 84/300, Valid Loss: 0.6105, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Train Loss: 0.5659, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 85/300, Valid Loss: 0.6353, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 86/300, Train Loss: 0.6266, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 86/300, Valid Loss: 0.6196, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 87/300, Train Loss: 0.5872, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 87/300, Valid Loss: 0.5840, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 88/300, Train Loss: 0.5583, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 88/300, Valid Loss: 0.5962, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 89/300, Train Loss: 0.5440, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 89/300, Valid Loss: 0.6493, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 90/300, Train Loss: 0.6032, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 90/300, Valid Loss: 0.5979, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 91/300, Train Loss: 0.5897, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 91/300, Valid Loss: 0.5874, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 92/300, Train Loss: 0.5619, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 92/300, Valid Loss: 0.5923, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 93/300, Train Loss: 0.5366, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 93/300, Valid Loss: 0.6289, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 94/300, Train Loss: 0.5760, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 94/300, Valid Loss: 0.6971, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 95/300, Train Loss: 0.5468, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 95/300, Valid Loss: 0.6992, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Train Loss: 0.5205, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 96/300, Valid Loss: 0.5718, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 97/300, Train Loss: 0.5155, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 97/300, Valid Loss: 0.6290, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 98/300, Train Loss: 0.5183, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 98/300, Valid Loss: 0.5983, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 99/300, Train Loss: 0.5199, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 99/300, Valid Loss: 0.6751, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 100/300, Train Loss: 0.5273, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 100/300, Valid Loss: 0.5912, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 101/300, Train Loss: 0.5180, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 101/300, Valid Loss: 0.7179, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 102/300, Train Loss: 0.5502, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 102/300, Valid Loss: 0.6119, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 103/300, Train Loss: 0.5065, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 103/300, Valid Loss: 0.6145, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 104/300, Train Loss: 0.5244, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 104/300, Valid Loss: 0.6291, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 105/300, Train Loss: 0.5457, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 105/300, Valid Loss: 0.5792, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 106/300, Train Loss: 0.5058, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 106/300, Valid Loss: 0.5908, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 107/300, Train Loss: 0.5055, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 107/300, Valid Loss: 0.6124, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 108/300, Train Loss: 0.5092, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 108/300, Valid Loss: 0.5790, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 109/300, Train Loss: 0.4977, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 109/300, Valid Loss: 0.5819, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 110/300, Train Loss: 0.4892, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 110/300, Valid Loss: 0.6095, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 111/300, Train Loss: 0.4935, Acc: 0.7035, F1: 0.1053, AUC: 0.5205\n",
      "Epoch 111/300, Valid Loss: 0.5918, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 112/300, Train Loss: 0.4885, Acc: 0.6977, F1: 0.0714, AUC: 0.5109\n",
      "Epoch 112/300, Valid Loss: 0.6263, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 113/300, Train Loss: 0.5002, Acc: 0.6802, F1: 0.0678, AUC: 0.4984\n",
      "Epoch 113/300, Valid Loss: 0.6616, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 114/300, Train Loss: 0.5148, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 114/300, Valid Loss: 0.6322, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 115/300, Train Loss: 0.4451, Acc: 0.6977, F1: 0.3659, AUC: 0.5817\n",
      "Epoch 115/300, Valid Loss: 0.6432, Acc: 0.5814, F1: 0.3571, AUC: 0.5256\n",
      "Epoch 116/300, Train Loss: 0.4656, Acc: 0.7500, F1: 0.6261, AUC: 0.7337\n",
      "Epoch 116/300, Valid Loss: 0.6835, Acc: 0.6744, F1: 0.3000, AUC: 0.5487\n",
      "Epoch 117/300, Train Loss: 0.4906, Acc: 0.6919, F1: 0.5470, AUC: 0.6702\n",
      "Epoch 117/300, Valid Loss: 0.6048, Acc: 0.5581, F1: 0.4865, AUC: 0.5962\n",
      "Epoch 118/300, Train Loss: 0.4857, Acc: 0.7442, F1: 0.6393, AUC: 0.7458\n",
      "Epoch 118/300, Valid Loss: 0.6452, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 119/300, Train Loss: 0.4661, Acc: 0.6860, F1: 0.5424, AUC: 0.6660\n",
      "Epoch 119/300, Valid Loss: 0.6518, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 120/300, Train Loss: 0.4442, Acc: 0.7616, F1: 0.6667, AUC: 0.7692\n",
      "Epoch 120/300, Valid Loss: 0.7932, Acc: 0.6512, F1: 0.2857, AUC: 0.5321\n",
      "Epoch 121/300, Train Loss: 0.4659, Acc: 0.7384, F1: 0.6218, AUC: 0.7308\n",
      "Epoch 121/300, Valid Loss: 0.6021, Acc: 0.5349, F1: 0.5238, AUC: 0.6231\n",
      "Epoch 122/300, Train Loss: 0.4713, Acc: 0.6860, F1: 0.5970, AUC: 0.7096\n",
      "Epoch 122/300, Valid Loss: 0.6414, Acc: 0.5581, F1: 0.3871, AUC: 0.5308\n",
      "Epoch 123/300, Train Loss: 0.5034, Acc: 0.6570, F1: 0.5630, AUC: 0.6779\n",
      "Epoch 123/300, Valid Loss: 0.6238, Acc: 0.5349, F1: 0.5238, AUC: 0.6231\n",
      "Epoch 124/300, Train Loss: 0.4655, Acc: 0.7151, F1: 0.5051, AUC: 0.6487\n",
      "Epoch 124/300, Valid Loss: 0.6087, Acc: 0.4884, F1: 0.4762, AUC: 0.5679\n",
      "Epoch 125/300, Train Loss: 0.4639, Acc: 0.7267, F1: 0.5841, AUC: 0.7006\n",
      "Epoch 125/300, Valid Loss: 0.8029, Acc: 0.6744, F1: 0.3636, AUC: 0.5705\n",
      "Epoch 126/300, Train Loss: 0.4143, Acc: 0.7791, F1: 0.6667, AUC: 0.7654\n",
      "Epoch 126/300, Valid Loss: 0.7181, Acc: 0.6279, F1: 0.3846, AUC: 0.5590\n",
      "Epoch 127/300, Train Loss: 0.4425, Acc: 0.7326, F1: 0.6462, AUC: 0.7538\n",
      "Epoch 127/300, Valid Loss: 0.6415, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 128/300, Train Loss: 0.4628, Acc: 0.6919, F1: 0.6187, AUC: 0.7301\n",
      "Epoch 128/300, Valid Loss: 0.6613, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 129/300, Train Loss: 0.6481, Acc: 0.5523, F1: 0.3529, AUC: 0.5103\n",
      "Epoch 129/300, Valid Loss: 0.6730, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 130/300, Train Loss: 0.6493, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 130/300, Valid Loss: 0.6476, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 131/300, Train Loss: 0.6289, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 131/300, Valid Loss: 0.6322, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 132/300, Train Loss: 0.6206, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 132/300, Valid Loss: 0.6233, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 133/300, Train Loss: 0.6140, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 133/300, Valid Loss: 0.6175, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 134/300, Train Loss: 0.6115, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 134/300, Valid Loss: 0.6150, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 135/300, Train Loss: 0.6131, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 135/300, Valid Loss: 0.5926, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 136/300, Train Loss: 0.5336, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 136/300, Valid Loss: 0.5758, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 137/300, Train Loss: 0.5012, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 137/300, Valid Loss: 0.6147, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 138/300, Train Loss: 0.4922, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 138/300, Valid Loss: 0.6208, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 139/300, Train Loss: 0.5876, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 139/300, Valid Loss: 0.6270, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 140/300, Train Loss: 0.6190, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 140/300, Valid Loss: 0.6206, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 141/300, Train Loss: 0.6129, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 141/300, Valid Loss: 0.6162, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 142/300, Train Loss: 0.6112, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 142/300, Valid Loss: 0.6154, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 143/300, Train Loss: 0.6064, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 143/300, Valid Loss: 0.6057, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 144/300, Train Loss: 0.5658, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 144/300, Valid Loss: 0.6271, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 145/300, Train Loss: 0.5825, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 145/300, Valid Loss: 0.6828, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 146/300, Train Loss: 0.5116, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 146/300, Valid Loss: 0.7875, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 147/300, Train Loss: 0.5240, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 147/300, Valid Loss: 0.6741, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 148/300, Train Loss: 0.4779, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 148/300, Valid Loss: 0.6173, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 149/300, Train Loss: 0.5556, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 149/300, Valid Loss: 0.5747, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 150/300, Train Loss: 0.4950, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 150/300, Valid Loss: 0.5731, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 151/300, Train Loss: 0.4682, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 151/300, Valid Loss: 0.6042, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 152/300, Train Loss: 0.4744, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 152/300, Valid Loss: 0.8147, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 153/300, Train Loss: 0.5244, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 153/300, Valid Loss: 0.5765, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 154/300, Train Loss: 0.4644, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 154/300, Valid Loss: 0.6222, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 155/300, Train Loss: 0.5962, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 155/300, Valid Loss: 0.5857, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 156/300, Train Loss: 0.4323, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 156/300, Valid Loss: 0.6168, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 157/300, Train Loss: 0.4538, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 157/300, Valid Loss: 0.5853, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 158/300, Train Loss: 0.4268, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 158/300, Valid Loss: 0.7518, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 159/300, Train Loss: 0.3916, Acc: 0.7209, F1: 0.2500, AUC: 0.5603\n",
      "Epoch 159/300, Valid Loss: 1.5556, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 160/300, Train Loss: 0.5744, Acc: 0.6570, F1: 0.2892, AUC: 0.5362\n",
      "Epoch 160/300, Valid Loss: 0.6098, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 161/300, Train Loss: 0.5424, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 161/300, Valid Loss: 0.7109, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 162/300, Train Loss: 0.4804, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 162/300, Valid Loss: 0.7730, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 163/300, Train Loss: 0.5018, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 163/300, Valid Loss: 0.6631, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 164/300, Train Loss: 0.4307, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 164/300, Valid Loss: 0.6229, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 165/300, Train Loss: 0.4243, Acc: 0.6977, F1: 0.1034, AUC: 0.5163\n",
      "Epoch 165/300, Valid Loss: 0.6668, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 166/300, Train Loss: 0.4580, Acc: 0.6860, F1: 0.2895, AUC: 0.5516\n",
      "Epoch 166/300, Valid Loss: 0.6026, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 167/300, Train Loss: 0.4615, Acc: 0.6919, F1: 0.0702, AUC: 0.5067\n",
      "Epoch 167/300, Valid Loss: 0.6115, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 168/300, Train Loss: 0.4489, Acc: 0.6919, F1: 0.2090, AUC: 0.5340\n",
      "Epoch 168/300, Valid Loss: 0.8454, Acc: 0.6977, F1: 0.0000, AUC: 0.5000\n",
      "Epoch 169/300, Train Loss: 0.4453, Acc: 0.7209, F1: 0.5472, AUC: 0.6747\n",
      "Epoch 169/300, Valid Loss: 0.8115, Acc: 0.6279, F1: 0.3333, AUC: 0.5372\n",
      "Epoch 170/300, Train Loss: 0.4229, Acc: 0.7384, F1: 0.6218, AUC: 0.7308\n",
      "Epoch 170/300, Valid Loss: 0.6149, Acc: 0.5116, F1: 0.5116, AUC: 0.6064\n",
      "Epoch 171/300, Train Loss: 0.4322, Acc: 0.7267, F1: 0.6179, AUC: 0.7279\n",
      "Epoch 171/300, Valid Loss: 0.8128, Acc: 0.6744, F1: 0.4615, AUC: 0.6141\n",
      "Epoch 172/300, Train Loss: 0.4356, Acc: 0.7442, F1: 0.6333, AUC: 0.7404\n",
      "Epoch 172/300, Valid Loss: 0.7786, Acc: 0.5814, F1: 0.4375, AUC: 0.5692\n",
      "Epoch 173/300, Train Loss: 0.5035, Acc: 0.7384, F1: 0.6512, AUC: 0.7580\n",
      "Epoch 173/300, Valid Loss: 0.6527, Acc: 0.6279, F1: 0.4286, AUC: 0.5808\n",
      "Epoch 174/300, Train Loss: 0.4339, Acc: 0.7209, F1: 0.6129, AUC: 0.7237\n",
      "Epoch 174/300, Valid Loss: 0.6410, Acc: 0.4884, F1: 0.5217, AUC: 0.6115\n",
      "Epoch 175/300, Train Loss: 0.4533, Acc: 0.6919, F1: 0.6187, AUC: 0.7301\n",
      "Epoch 175/300, Valid Loss: 0.6688, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 176/300, Train Loss: 0.3867, Acc: 0.7733, F1: 0.6777, AUC: 0.7776\n",
      "Epoch 176/300, Valid Loss: 0.9622, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 177/300, Train Loss: 0.5066, Acc: 0.6628, F1: 0.6133, AUC: 0.7256\n",
      "Epoch 177/300, Valid Loss: 0.6692, Acc: 0.5116, F1: 0.4615, AUC: 0.5628\n",
      "Epoch 178/300, Train Loss: 0.4923, Acc: 0.6570, F1: 0.5426, AUC: 0.6615\n",
      "Epoch 178/300, Valid Loss: 0.6037, Acc: 0.5116, F1: 0.4615, AUC: 0.5628\n",
      "Epoch 179/300, Train Loss: 0.4490, Acc: 0.6919, F1: 0.5546, AUC: 0.6756\n",
      "Epoch 179/300, Valid Loss: 0.8479, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 180/300, Train Loss: 0.3849, Acc: 0.7442, F1: 0.5849, AUC: 0.7022\n",
      "Epoch 180/300, Valid Loss: 0.8365, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 181/300, Train Loss: 0.4231, Acc: 0.7733, F1: 0.6777, AUC: 0.7776\n",
      "Epoch 181/300, Valid Loss: 0.8418, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 182/300, Train Loss: 0.4849, Acc: 0.7093, F1: 0.6324, AUC: 0.7426\n",
      "Epoch 182/300, Valid Loss: 0.8455, Acc: 0.6279, F1: 0.4286, AUC: 0.5808\n",
      "Epoch 183/300, Train Loss: 0.4489, Acc: 0.6860, F1: 0.6143, AUC: 0.7260\n",
      "Epoch 183/300, Valid Loss: 0.6409, Acc: 0.4884, F1: 0.5217, AUC: 0.6115\n",
      "Epoch 184/300, Train Loss: 0.4557, Acc: 0.7093, F1: 0.6094, AUC: 0.7208\n",
      "Epoch 184/300, Valid Loss: 0.6802, Acc: 0.5349, F1: 0.4444, AUC: 0.5577\n",
      "Epoch 185/300, Train Loss: 0.4139, Acc: 0.7500, F1: 0.6504, AUC: 0.7554\n",
      "Epoch 185/300, Valid Loss: 1.1338, Acc: 0.7442, F1: 0.3529, AUC: 0.5987\n",
      "Epoch 186/300, Train Loss: 0.4271, Acc: 0.7616, F1: 0.6555, AUC: 0.7583\n",
      "Epoch 186/300, Valid Loss: 1.1102, Acc: 0.7442, F1: 0.3529, AUC: 0.5987\n",
      "Epoch 187/300, Train Loss: 0.4276, Acc: 0.7442, F1: 0.6615, AUC: 0.7676\n",
      "Epoch 187/300, Valid Loss: 0.6584, Acc: 0.5349, F1: 0.4444, AUC: 0.5577\n",
      "Epoch 188/300, Train Loss: 0.4058, Acc: 0.7616, F1: 0.6667, AUC: 0.7692\n",
      "Epoch 188/300, Valid Loss: 0.6514, Acc: 0.5116, F1: 0.4324, AUC: 0.5410\n",
      "Epoch 189/300, Train Loss: 0.4845, Acc: 0.7093, F1: 0.6429, AUC: 0.7535\n",
      "Epoch 189/300, Valid Loss: 0.6804, Acc: 0.5814, F1: 0.4375, AUC: 0.5692\n",
      "Epoch 190/300, Train Loss: 0.4024, Acc: 0.7965, F1: 0.7154, AUC: 0.8106\n",
      "Epoch 190/300, Valid Loss: 0.7335, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 191/300, Train Loss: 0.4409, Acc: 0.7500, F1: 0.6667, AUC: 0.7718\n",
      "Epoch 191/300, Valid Loss: 0.6412, Acc: 0.4884, F1: 0.4762, AUC: 0.5679\n",
      "Epoch 192/300, Train Loss: 0.4413, Acc: 0.7326, F1: 0.6515, AUC: 0.7593\n",
      "Epoch 192/300, Valid Loss: 0.6727, Acc: 0.6047, F1: 0.4848, AUC: 0.6077\n",
      "Epoch 193/300, Train Loss: 0.3604, Acc: 0.8198, F1: 0.7438, AUC: 0.8327\n",
      "Epoch 193/300, Valid Loss: 0.7097, Acc: 0.6047, F1: 0.4848, AUC: 0.6077\n",
      "Epoch 194/300, Train Loss: 0.4605, Acc: 0.6860, F1: 0.6400, AUC: 0.7532\n",
      "Epoch 194/300, Valid Loss: 0.8939, Acc: 0.6744, F1: 0.4615, AUC: 0.6141\n",
      "Epoch 195/300, Train Loss: 0.4695, Acc: 0.6860, F1: 0.6250, AUC: 0.7369\n",
      "Epoch 195/300, Valid Loss: 0.7686, Acc: 0.4884, F1: 0.4211, AUC: 0.5244\n",
      "Epoch 196/300, Train Loss: 0.4688, Acc: 0.6860, F1: 0.5574, AUC: 0.6769\n",
      "Epoch 196/300, Valid Loss: 0.8402, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 197/300, Train Loss: 0.4204, Acc: 0.7558, F1: 0.6111, AUC: 0.7215\n",
      "Epoch 197/300, Valid Loss: 0.6897, Acc: 0.5581, F1: 0.4571, AUC: 0.5744\n",
      "Epoch 198/300, Train Loss: 0.3779, Acc: 0.7791, F1: 0.6984, AUC: 0.7981\n",
      "Epoch 198/300, Valid Loss: 0.8798, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 199/300, Train Loss: 0.4146, Acc: 0.7151, F1: 0.6475, AUC: 0.7577\n",
      "Epoch 199/300, Valid Loss: 1.3802, Acc: 0.7442, F1: 0.4211, AUC: 0.6205\n",
      "Epoch 200/300, Train Loss: 0.3796, Acc: 0.7558, F1: 0.6818, AUC: 0.7869\n",
      "Epoch 200/300, Valid Loss: 0.6861, Acc: 0.5349, F1: 0.5238, AUC: 0.6231\n",
      "Epoch 201/300, Train Loss: 0.4840, Acc: 0.6570, F1: 0.6093, AUC: 0.7215\n",
      "Epoch 201/300, Valid Loss: 0.9489, Acc: 0.6279, F1: 0.4286, AUC: 0.5808\n",
      "Epoch 202/300, Train Loss: 0.4666, Acc: 0.6512, F1: 0.5833, AUC: 0.6955\n",
      "Epoch 202/300, Valid Loss: 0.8224, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 203/300, Train Loss: 0.4227, Acc: 0.7442, F1: 0.6452, AUC: 0.7513\n",
      "Epoch 203/300, Valid Loss: 0.7048, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 204/300, Train Loss: 0.4104, Acc: 0.7442, F1: 0.6667, AUC: 0.7731\n",
      "Epoch 204/300, Valid Loss: 0.7914, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 205/300, Train Loss: 0.4236, Acc: 0.7384, F1: 0.6809, AUC: 0.7907\n",
      "Epoch 205/300, Valid Loss: 0.6033, Acc: 0.5349, F1: 0.5238, AUC: 0.6231\n",
      "Epoch 206/300, Train Loss: 0.3987, Acc: 0.7849, F1: 0.7132, AUC: 0.8131\n",
      "Epoch 206/300, Valid Loss: 0.9239, Acc: 0.5814, F1: 0.4375, AUC: 0.5692\n",
      "Epoch 207/300, Train Loss: 0.4884, Acc: 0.7384, F1: 0.6763, AUC: 0.7853\n",
      "Epoch 207/300, Valid Loss: 0.8530, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 208/300, Train Loss: 0.3729, Acc: 0.7733, F1: 0.6723, AUC: 0.7721\n",
      "Epoch 208/300, Valid Loss: 0.7180, Acc: 0.5581, F1: 0.4865, AUC: 0.5962\n",
      "Epoch 209/300, Train Loss: 0.3880, Acc: 0.7849, F1: 0.7132, AUC: 0.8131\n",
      "Epoch 209/300, Valid Loss: 0.7687, Acc: 0.5116, F1: 0.3636, AUC: 0.4974\n",
      "Epoch 210/300, Train Loss: 0.3943, Acc: 0.7849, F1: 0.7176, AUC: 0.8186\n",
      "Epoch 210/300, Valid Loss: 0.8727, Acc: 0.5116, F1: 0.4000, AUC: 0.5192\n",
      "Epoch 211/300, Train Loss: 0.4120, Acc: 0.7733, F1: 0.6977, AUC: 0.7994\n",
      "Epoch 211/300, Valid Loss: 0.7809, Acc: 0.6047, F1: 0.4516, AUC: 0.5859\n",
      "Epoch 212/300, Train Loss: 0.4174, Acc: 0.7500, F1: 0.6767, AUC: 0.7827\n",
      "Epoch 212/300, Valid Loss: 0.8111, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 213/300, Train Loss: 0.4137, Acc: 0.7558, F1: 0.6818, AUC: 0.7869\n",
      "Epoch 213/300, Valid Loss: 0.7518, Acc: 0.3023, F1: 0.4643, AUC: 0.5000\n",
      "Epoch 214/300, Train Loss: 0.4598, Acc: 0.7035, F1: 0.6434, AUC: 0.7548\n",
      "Epoch 214/300, Valid Loss: 0.6842, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 215/300, Train Loss: 0.3876, Acc: 0.8256, F1: 0.7541, AUC: 0.8423\n",
      "Epoch 215/300, Valid Loss: 0.7689, Acc: 0.5581, F1: 0.3448, AUC: 0.5090\n",
      "Epoch 216/300, Train Loss: 0.3756, Acc: 0.7965, F1: 0.7154, AUC: 0.8106\n",
      "Epoch 216/300, Valid Loss: 0.6404, Acc: 0.4884, F1: 0.4500, AUC: 0.5462\n",
      "Epoch 217/300, Train Loss: 0.4387, Acc: 0.7384, F1: 0.6457, AUC: 0.7526\n",
      "Epoch 217/300, Valid Loss: 0.8196, Acc: 0.6047, F1: 0.3704, AUC: 0.5423\n",
      "Epoch 218/300, Train Loss: 0.4461, Acc: 0.7209, F1: 0.6471, AUC: 0.7564\n",
      "Epoch 218/300, Valid Loss: 0.8505, Acc: 0.6977, F1: 0.4348, AUC: 0.6090\n",
      "Epoch 219/300, Train Loss: 0.4135, Acc: 0.7500, F1: 0.6767, AUC: 0.7827\n",
      "Epoch 219/300, Valid Loss: 0.6769, Acc: 0.4884, F1: 0.4762, AUC: 0.5679\n",
      "Epoch 220/300, Train Loss: 0.3906, Acc: 0.7616, F1: 0.6870, AUC: 0.7910\n",
      "Epoch 220/300, Valid Loss: 0.8241, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 221/300, Train Loss: 0.4445, Acc: 0.7267, F1: 0.6569, AUC: 0.7660\n",
      "Epoch 221/300, Valid Loss: 0.6444, Acc: 0.4884, F1: 0.5217, AUC: 0.6115\n",
      "Epoch 222/300, Train Loss: 0.3719, Acc: 0.7849, F1: 0.7087, AUC: 0.8077\n",
      "Epoch 222/300, Valid Loss: 0.6901, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 223/300, Train Loss: 0.4090, Acc: 0.7791, F1: 0.7121, AUC: 0.8144\n",
      "Epoch 223/300, Valid Loss: 1.0828, Acc: 0.7442, F1: 0.4211, AUC: 0.6205\n",
      "Epoch 224/300, Train Loss: 0.3874, Acc: 0.7965, F1: 0.7287, AUC: 0.8269\n",
      "Epoch 224/300, Valid Loss: 0.8040, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 225/300, Train Loss: 0.5254, Acc: 0.6047, F1: 0.5750, AUC: 0.6840\n",
      "Epoch 225/300, Valid Loss: 0.8846, Acc: 0.6279, F1: 0.4286, AUC: 0.5808\n",
      "Epoch 226/300, Train Loss: 0.4897, Acc: 0.6686, F1: 0.6069, AUC: 0.7189\n",
      "Epoch 226/300, Valid Loss: 0.6169, Acc: 0.4884, F1: 0.5417, AUC: 0.6333\n",
      "Epoch 227/300, Train Loss: 0.4135, Acc: 0.7733, F1: 0.7068, AUC: 0.8103\n",
      "Epoch 227/300, Valid Loss: 0.8979, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 228/300, Train Loss: 0.3915, Acc: 0.7558, F1: 0.6866, AUC: 0.7923\n",
      "Epoch 228/300, Valid Loss: 0.7729, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 229/300, Train Loss: 0.3743, Acc: 0.8081, F1: 0.7317, AUC: 0.8244\n",
      "Epoch 229/300, Valid Loss: 0.6637, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 230/300, Train Loss: 0.3772, Acc: 0.8314, F1: 0.7563, AUC: 0.8410\n",
      "Epoch 230/300, Valid Loss: 1.3277, Acc: 0.6977, F1: 0.2353, AUC: 0.5436\n",
      "Epoch 231/300, Train Loss: 0.4454, Acc: 0.7267, F1: 0.6466, AUC: 0.7551\n",
      "Epoch 231/300, Valid Loss: 0.7355, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 232/300, Train Loss: 0.3810, Acc: 0.7907, F1: 0.7188, AUC: 0.8173\n",
      "Epoch 232/300, Valid Loss: 0.7806, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 233/300, Train Loss: 0.3548, Acc: 0.7965, F1: 0.7244, AUC: 0.8215\n",
      "Epoch 233/300, Valid Loss: 0.6893, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 234/300, Train Loss: 0.3884, Acc: 0.7674, F1: 0.7015, AUC: 0.8061\n",
      "Epoch 234/300, Valid Loss: 0.7142, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 235/300, Train Loss: 0.4371, Acc: 0.7849, F1: 0.7087, AUC: 0.8077\n",
      "Epoch 235/300, Valid Loss: 0.8484, Acc: 0.6744, F1: 0.4615, AUC: 0.6141\n",
      "Epoch 236/300, Train Loss: 0.3803, Acc: 0.7558, F1: 0.6719, AUC: 0.7760\n",
      "Epoch 236/300, Valid Loss: 0.7323, Acc: 0.5581, F1: 0.4865, AUC: 0.5962\n",
      "Epoch 237/300, Train Loss: 0.3856, Acc: 0.7791, F1: 0.7246, AUC: 0.8308\n",
      "Epoch 237/300, Valid Loss: 1.0027, Acc: 0.6047, F1: 0.3704, AUC: 0.5423\n",
      "Epoch 238/300, Train Loss: 0.4306, Acc: 0.7500, F1: 0.6718, AUC: 0.7772\n",
      "Epoch 238/300, Valid Loss: 0.6809, Acc: 0.4884, F1: 0.4762, AUC: 0.5679\n",
      "Epoch 239/300, Train Loss: 0.4463, Acc: 0.7093, F1: 0.6575, AUC: 0.7699\n",
      "Epoch 239/300, Valid Loss: 0.8969, Acc: 0.6512, F1: 0.4828, AUC: 0.6192\n",
      "Epoch 240/300, Train Loss: 0.4366, Acc: 0.7209, F1: 0.6571, AUC: 0.7673\n",
      "Epoch 240/300, Valid Loss: 0.8335, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 241/300, Train Loss: 0.3452, Acc: 0.8256, F1: 0.7581, AUC: 0.8478\n",
      "Epoch 241/300, Valid Loss: 1.5751, Acc: 0.6977, F1: 0.1333, AUC: 0.5218\n",
      "Epoch 242/300, Train Loss: 0.5976, Acc: 0.5233, F1: 0.5287, AUC: 0.6256\n",
      "Epoch 242/300, Valid Loss: 1.0915, Acc: 0.7674, F1: 0.5000, AUC: 0.6590\n",
      "Epoch 243/300, Train Loss: 0.4107, Acc: 0.7791, F1: 0.6724, AUC: 0.7708\n",
      "Epoch 243/300, Valid Loss: 0.8528, Acc: 0.6744, F1: 0.4615, AUC: 0.6141\n",
      "Epoch 244/300, Train Loss: 0.4028, Acc: 0.7500, F1: 0.6667, AUC: 0.7718\n",
      "Epoch 244/300, Valid Loss: 0.9507, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 245/300, Train Loss: 0.3526, Acc: 0.7965, F1: 0.7287, AUC: 0.8269\n",
      "Epoch 245/300, Valid Loss: 1.1327, Acc: 0.6512, F1: 0.4444, AUC: 0.5974\n",
      "Epoch 246/300, Train Loss: 0.4158, Acc: 0.7384, F1: 0.6667, AUC: 0.7744\n",
      "Epoch 246/300, Valid Loss: 0.6355, Acc: 0.5349, F1: 0.5238, AUC: 0.6231\n",
      "Epoch 247/300, Train Loss: 0.4814, Acc: 0.6628, F1: 0.6081, AUC: 0.7202\n",
      "Epoch 247/300, Valid Loss: 1.1068, Acc: 0.6744, F1: 0.4167, AUC: 0.5923\n",
      "Epoch 248/300, Train Loss: 0.3687, Acc: 0.8140, F1: 0.7500, AUC: 0.8449\n",
      "Epoch 248/300, Valid Loss: 1.1716, Acc: 0.7442, F1: 0.4762, AUC: 0.6423\n",
      "Epoch 249/300, Train Loss: 0.4372, Acc: 0.7442, F1: 0.6765, AUC: 0.7840\n",
      "Epoch 249/300, Valid Loss: 1.0446, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 250/300, Train Loss: 0.3908, Acc: 0.8081, F1: 0.7317, AUC: 0.8244\n",
      "Epoch 250/300, Valid Loss: 1.0604, Acc: 0.7209, F1: 0.5000, AUC: 0.6474\n",
      "Epoch 251/300, Train Loss: 0.3876, Acc: 0.7674, F1: 0.6970, AUC: 0.8006\n",
      "Epoch 251/300, Valid Loss: 0.8384, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 252/300, Train Loss: 0.4099, Acc: 0.7326, F1: 0.6714, AUC: 0.7811\n",
      "Epoch 252/300, Valid Loss: 1.0486, Acc: 0.5814, F1: 0.3571, AUC: 0.5256\n",
      "Epoch 253/300, Train Loss: 0.3796, Acc: 0.7791, F1: 0.7077, AUC: 0.8090\n",
      "Epoch 253/300, Valid Loss: 1.2265, Acc: 0.6977, F1: 0.4348, AUC: 0.6090\n",
      "Epoch 254/300, Train Loss: 0.3331, Acc: 0.8372, F1: 0.7742, AUC: 0.8615\n",
      "Epoch 254/300, Valid Loss: 0.9962, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 255/300, Train Loss: 0.3950, Acc: 0.7674, F1: 0.7059, AUC: 0.8115\n",
      "Epoch 255/300, Valid Loss: 0.9374, Acc: 0.6047, F1: 0.4516, AUC: 0.5859\n",
      "Epoch 256/300, Train Loss: 0.4410, Acc: 0.7500, F1: 0.6861, AUC: 0.7936\n",
      "Epoch 256/300, Valid Loss: 0.7475, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 257/300, Train Loss: 0.4538, Acc: 0.7267, F1: 0.6519, AUC: 0.7606\n",
      "Epoch 257/300, Valid Loss: 0.7272, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 258/300, Train Loss: 0.4072, Acc: 0.7674, F1: 0.6970, AUC: 0.8006\n",
      "Epoch 258/300, Valid Loss: 0.8008, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 259/300, Train Loss: 0.3649, Acc: 0.7907, F1: 0.7188, AUC: 0.8173\n",
      "Epoch 259/300, Valid Loss: 0.6783, Acc: 0.4884, F1: 0.5217, AUC: 0.6115\n",
      "Epoch 260/300, Train Loss: 0.4110, Acc: 0.7326, F1: 0.6714, AUC: 0.7811\n",
      "Epoch 260/300, Valid Loss: 0.7284, Acc: 0.3721, F1: 0.4906, AUC: 0.5500\n",
      "Epoch 261/300, Train Loss: 0.4457, Acc: 0.7151, F1: 0.6667, AUC: 0.7795\n",
      "Epoch 261/300, Valid Loss: 1.1548, Acc: 0.6279, F1: 0.3846, AUC: 0.5590\n",
      "Epoch 262/300, Train Loss: 0.4494, Acc: 0.7035, F1: 0.6483, AUC: 0.7603\n",
      "Epoch 262/300, Valid Loss: 0.6656, Acc: 0.4419, F1: 0.5200, AUC: 0.6000\n",
      "Epoch 263/300, Train Loss: 0.3793, Acc: 0.7733, F1: 0.7194, AUC: 0.8266\n",
      "Epoch 263/300, Valid Loss: 1.0772, Acc: 0.5814, F1: 0.4375, AUC: 0.5692\n",
      "Epoch 264/300, Train Loss: 0.3951, Acc: 0.8140, F1: 0.7419, AUC: 0.8340\n",
      "Epoch 264/300, Valid Loss: 1.3253, Acc: 0.7442, F1: 0.3529, AUC: 0.5987\n",
      "Epoch 265/300, Train Loss: 0.3605, Acc: 0.7965, F1: 0.7287, AUC: 0.8269\n",
      "Epoch 265/300, Valid Loss: 0.7720, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 266/300, Train Loss: 0.3707, Acc: 0.7965, F1: 0.7328, AUC: 0.8324\n",
      "Epoch 266/300, Valid Loss: 0.8416, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 267/300, Train Loss: 0.3656, Acc: 0.8198, F1: 0.7480, AUC: 0.8381\n",
      "Epoch 267/300, Valid Loss: 0.6869, Acc: 0.4884, F1: 0.5217, AUC: 0.6115\n",
      "Epoch 268/300, Train Loss: 0.4125, Acc: 0.7733, F1: 0.6977, AUC: 0.7994\n",
      "Epoch 268/300, Valid Loss: 0.6609, Acc: 0.4651, F1: 0.4390, AUC: 0.5295\n",
      "Epoch 269/300, Train Loss: 0.4055, Acc: 0.7674, F1: 0.7015, AUC: 0.8061\n",
      "Epoch 269/300, Valid Loss: 0.6242, Acc: 0.5349, F1: 0.5455, AUC: 0.6449\n",
      "Epoch 270/300, Train Loss: 0.3746, Acc: 0.7849, F1: 0.7176, AUC: 0.8186\n",
      "Epoch 270/300, Valid Loss: 0.8933, Acc: 0.5581, F1: 0.3871, AUC: 0.5308\n",
      "Epoch 271/300, Train Loss: 0.3980, Acc: 0.7907, F1: 0.7231, AUC: 0.8228\n",
      "Epoch 271/300, Valid Loss: 0.8186, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 272/300, Train Loss: 0.3510, Acc: 0.8023, F1: 0.7302, AUC: 0.8256\n",
      "Epoch 272/300, Valid Loss: 0.8687, Acc: 0.6279, F1: 0.4667, AUC: 0.6026\n",
      "Epoch 273/300, Train Loss: 0.3531, Acc: 0.8198, F1: 0.7438, AUC: 0.8327\n",
      "Epoch 273/300, Valid Loss: 0.7860, Acc: 0.5349, F1: 0.4444, AUC: 0.5577\n",
      "Epoch 274/300, Train Loss: 0.3391, Acc: 0.8198, F1: 0.7669, AUC: 0.8654\n",
      "Epoch 274/300, Valid Loss: 0.9260, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 275/300, Train Loss: 0.4128, Acc: 0.7558, F1: 0.6912, AUC: 0.7978\n",
      "Epoch 275/300, Valid Loss: 0.9084, Acc: 0.5349, F1: 0.3750, AUC: 0.5141\n",
      "Epoch 276/300, Train Loss: 0.4651, Acc: 0.7384, F1: 0.6715, AUC: 0.7798\n",
      "Epoch 276/300, Valid Loss: 0.7877, Acc: 0.3023, F1: 0.4643, AUC: 0.5000\n",
      "Epoch 277/300, Train Loss: 0.5050, Acc: 0.6221, F1: 0.5963, AUC: 0.7074\n",
      "Epoch 277/300, Valid Loss: 0.7798, Acc: 0.6279, F1: 0.4667, AUC: 0.6026\n",
      "Epoch 278/300, Train Loss: 0.3496, Acc: 0.8081, F1: 0.7402, AUC: 0.8353\n",
      "Epoch 278/300, Valid Loss: 0.8043, Acc: 0.5116, F1: 0.4000, AUC: 0.5192\n",
      "Epoch 279/300, Train Loss: 0.4059, Acc: 0.7965, F1: 0.7287, AUC: 0.8269\n",
      "Epoch 279/300, Valid Loss: 0.7310, Acc: 0.5581, F1: 0.4571, AUC: 0.5744\n",
      "Epoch 280/300, Train Loss: 0.3406, Acc: 0.8256, F1: 0.7581, AUC: 0.8478\n",
      "Epoch 280/300, Valid Loss: 0.8834, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 281/300, Train Loss: 0.3612, Acc: 0.7907, F1: 0.7188, AUC: 0.8173\n",
      "Epoch 281/300, Valid Loss: 0.7137, Acc: 0.5349, F1: 0.4737, AUC: 0.5795\n",
      "Epoch 282/300, Train Loss: 0.3864, Acc: 0.7965, F1: 0.7328, AUC: 0.8324\n",
      "Epoch 282/300, Valid Loss: 0.9660, Acc: 0.7209, F1: 0.4545, AUC: 0.6256\n",
      "Epoch 283/300, Train Loss: 0.3481, Acc: 0.8140, F1: 0.7419, AUC: 0.8340\n",
      "Epoch 283/300, Valid Loss: 0.8377, Acc: 0.6047, F1: 0.4848, AUC: 0.6077\n",
      "Epoch 284/300, Train Loss: 0.3678, Acc: 0.7907, F1: 0.7313, AUC: 0.8337\n",
      "Epoch 284/300, Valid Loss: 1.1757, Acc: 0.6512, F1: 0.4000, AUC: 0.5756\n",
      "Epoch 285/300, Train Loss: 0.3852, Acc: 0.7674, F1: 0.7183, AUC: 0.8279\n",
      "Epoch 285/300, Valid Loss: 1.2099, Acc: 0.6047, F1: 0.3704, AUC: 0.5423\n",
      "Epoch 286/300, Train Loss: 0.4322, Acc: 0.7500, F1: 0.6815, AUC: 0.7881\n",
      "Epoch 286/300, Valid Loss: 0.9484, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 287/300, Train Loss: 0.3491, Acc: 0.7965, F1: 0.7328, AUC: 0.8324\n",
      "Epoch 287/300, Valid Loss: 1.0153, Acc: 0.5581, F1: 0.3871, AUC: 0.5308\n",
      "Epoch 288/300, Train Loss: 0.4138, Acc: 0.7674, F1: 0.7059, AUC: 0.8115\n",
      "Epoch 288/300, Valid Loss: 1.3618, Acc: 0.6977, F1: 0.4348, AUC: 0.6090\n",
      "Epoch 289/300, Train Loss: 0.3684, Acc: 0.8198, F1: 0.7597, AUC: 0.8545\n",
      "Epoch 289/300, Valid Loss: 0.7870, Acc: 0.5581, F1: 0.4571, AUC: 0.5744\n",
      "Epoch 290/300, Train Loss: 0.3783, Acc: 0.7965, F1: 0.7328, AUC: 0.8324\n",
      "Epoch 290/300, Valid Loss: 0.9448, Acc: 0.6047, F1: 0.4516, AUC: 0.5859\n",
      "Epoch 291/300, Train Loss: 0.3737, Acc: 0.7791, F1: 0.7206, AUC: 0.8253\n",
      "Epoch 291/300, Valid Loss: 0.8568, Acc: 0.5581, F1: 0.4242, AUC: 0.5526\n",
      "Epoch 292/300, Train Loss: 0.3878, Acc: 0.7733, F1: 0.7153, AUC: 0.8212\n",
      "Epoch 292/300, Valid Loss: 0.8783, Acc: 0.5814, F1: 0.4706, AUC: 0.5910\n",
      "Epoch 293/300, Train Loss: 0.4385, Acc: 0.7500, F1: 0.6718, AUC: 0.7772\n",
      "Epoch 293/300, Valid Loss: 0.8035, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 294/300, Train Loss: 0.3619, Acc: 0.8140, F1: 0.7419, AUC: 0.8340\n",
      "Epoch 294/300, Valid Loss: 0.7082, Acc: 0.4186, F1: 0.5098, AUC: 0.5833\n",
      "Epoch 295/300, Train Loss: 0.4048, Acc: 0.7558, F1: 0.7042, AUC: 0.8141\n",
      "Epoch 295/300, Valid Loss: 0.7818, Acc: 0.5349, F1: 0.4118, AUC: 0.5359\n",
      "Epoch 296/300, Train Loss: 0.3870, Acc: 0.7733, F1: 0.7068, AUC: 0.8103\n",
      "Epoch 296/300, Valid Loss: 0.9184, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 297/300, Train Loss: 0.3982, Acc: 0.7674, F1: 0.6970, AUC: 0.8006\n",
      "Epoch 297/300, Valid Loss: 0.8719, Acc: 0.6047, F1: 0.4138, AUC: 0.5641\n",
      "Epoch 298/300, Train Loss: 0.3268, Acc: 0.8314, F1: 0.7680, AUC: 0.8574\n",
      "Epoch 298/300, Valid Loss: 1.0019, Acc: 0.5814, F1: 0.4000, AUC: 0.5474\n",
      "Epoch 299/300, Train Loss: 0.3396, Acc: 0.8198, F1: 0.7559, AUC: 0.8490\n",
      "Epoch 299/300, Valid Loss: 1.2578, Acc: 0.6279, F1: 0.3846, AUC: 0.5590\n",
      "Epoch 300/300, Train Loss: 0.5769, Acc: 0.5930, F1: 0.5570, AUC: 0.6647\n",
      "Epoch 300/300, Valid Loss: 1.5360, Acc: 0.7442, F1: 0.3529, AUC: 0.5987\n",
      "\n",
      "Average Results across all folds:\n",
      "Train Loss: nan\n",
      "Train Accuracy: nan\n",
      "Train F1 Score: nan\n",
      "Train AUC: nan\n",
      "Validation Loss: nan\n",
      "Validation Accuracy: nan\n",
      "Validation F1 Score: nan\n",
      "Validation AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizx43/anaconda3/envs/gigapath/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/lizx43/anaconda3/envs/gigapath/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'melt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[1;32m     76\u001b[0m results \u001b[38;5;241m=\u001b[39m cross_validate_model(slide_embed, slide_20X_df\u001b[38;5;241m.\u001b[39mdrug_response, input_dim, hidden_dims, output_dim, num_epochs, batch_size, k_folds)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mplot_cross_validation_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/lizx43/PathGra/model/../utils/train_utils.py:34\u001b[0m, in \u001b[0;36mplot_cross_validation_results\u001b[0;34m(results_df)\u001b[0m\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m results_melted \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelt\u001b[49m(id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m], value_vars\u001b[38;5;241m=\u001b[39mmetrics, var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m sns\u001b[38;5;241m.\u001b[39mboxplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mresults_melted)\n\u001b[1;32m     36\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'melt'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate_model(dataframe, labels, input_dim, hidden_dims, output_dim, num_epochs, batch_size, k_folds=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataframe)):\n",
    "        print(f'-- Fold {fold + 1}/{k_folds} --')\n",
    "\n",
    "        train_dataset = CustomDataset(dataframe.iloc[train_idx], labels.iloc[train_idx])\n",
    "        val_dataset = CustomDataset(dataframe.iloc[val_idx], labels.iloc[val_idx])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = MLPModel(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc, train_f1, train_auc = train_model(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc, val_f1, val_auc = validate_model(model, val_loader, criterion, device)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}')\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Valid Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}')\n",
    "\n",
    "        fold_results.append({\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_f1': train_f1,\n",
    "            'train_auc': train_auc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "            'val_auc': val_auc\n",
    "        })\n",
    "\n",
    "    # Calculate average results across folds\n",
    "    avg_results = {\n",
    "        'train_loss': np.mean([result['train_loss'] for result in fold_results]),\n",
    "        'train_acc': np.mean([result['train_acc'] for result in fold_results]),\n",
    "        'train_f1': np.mean([result['train_f1'] for result in fold_results]),\n",
    "        'train_auc': np.mean([result['train_auc'] for result in fold_results]),\n",
    "        'val_loss': np.mean([result['val_loss'] for result in fold_results]),\n",
    "        'val_acc': np.mean([result['val_acc'] for result in fold_results]),\n",
    "        'val_f1': np.mean([result['val_f1'] for result in fold_results]),\n",
    "        'val_auc': np.mean([result['val_auc'] for result in fold_results])\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage Results across all folds:\")\n",
    "    print(f\"Train Loss: {avg_results['train_loss']:.4f}\")\n",
    "    print(f\"Train Accuracy: {avg_results['train_acc']:.4f}\")\n",
    "    print(f\"Train F1 Score: {avg_results['train_f1']:.4f}\")\n",
    "    print(f\"Train AUC: {avg_results['train_auc']:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_results['val_loss']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {avg_results['val_acc']:.4f}\")\n",
    "    print(f\"Validation F1 Score: {avg_results['val_f1']:.4f}\")\n",
    "    print(f\"Validation AUC: {avg_results['val_auc']:.4f}\")\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "input_dim = 768\n",
    "hidden_dims = [512, 256, 128]\n",
    "output_dim = 1\n",
    "num_epochs = 300\n",
    "batch_size = 1\n",
    "k_folds = 5\n",
    "\n",
    "# Perform cross-validation\n",
    "results = cross_validate_model(slide_embed, slide_20X_df.drug_response, input_dim, hidden_dims, output_dim, num_epochs, batch_size, k_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold0\n",
    "# Epoch 248/300, Train Loss: 0.4380, Acc: 0.7907, F1: 0.7143, AUC: 0.8172\n",
    "# Epoch 248/300, Valid Loss: 4.2069, Acc: 0.6744, F1: 0.5882, AUC: 0.6847\n",
    "# fold 1\n",
    "# Epoch 262/300, Train Loss: 0.2750, Acc: 0.8605, F1: 0.7895, AUC: 0.8619\n",
    "# Epoch 262/300, Valid Loss: 1.4849, Acc: 0.7209, F1: 0.4545, AUC: 0.6256\n",
    "#fold3\n",
    "#Epoch 257/300, Train Loss: 0.3390, Acc: 0.8198, F1: 0.7257, AUC: 0.8249\n",
    "# Epoch 257/300, Valid Loss: 0.6847, Acc: 0.7209, F1: 0.5714, AUC: 0.6759\n",
    "#fold4\n",
    "# Epoch 296/300, Train Loss: 0.3854, Acc: 0.7907, F1: 0.7273, AUC: 0.8079\n",
    "# Epoch 296/300, Valid Loss: 0.6474, Acc: 0.5814, F1: 0.4706, AUC: 0.6944\n",
    "#fold\n",
    "# Epoch 269/300, Train Loss: 0.4055, Acc: 0.7674, F1: 0.7015, AUC: 0.8061\n",
    "# Epoch 269/300, Valid Loss: 0.6242, Acc: 0.5349, F1: 0.5455, AUC: 0.6449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=[{\n",
    "    'fold':0,\n",
    "    'train_loss': 0.43,\n",
    "  'train_acc': 0.7907,\n",
    "  'train_f1': 0.7143,\n",
    "  'train_auc': 0.8172,\n",
    "  'val_loss': 4.2069,\n",
    "  'val_acc': 0.6744,\n",
    "  'val_f1': 0.5882,\n",
    "  'val_auc': 0.6847},\n",
    " {\n",
    "         'fold':1,\n",
    "\n",
    "     'train_loss': 0.2750,\n",
    "  'train_acc': 0.8605,\n",
    "  'train_f1': 0.7895,\n",
    "  'train_auc': 0.8619,\n",
    "  'val_loss': 1.4849,\n",
    "  'val_acc': 0.7209,\n",
    "  'val_f1': 0.4545,\n",
    "  'val_auc': 0.6256},\n",
    " {\n",
    "         'fold':2,\n",
    "     'train_loss': 0.3390,\n",
    "  'train_acc': 0.8198,\n",
    "  'train_f1': 0.7257,\n",
    "  'train_auc': 0.8249,\n",
    "  'val_loss': 0.6847,\n",
    "  'val_acc': 0.7209,\n",
    "  'val_f1': 0.5714,\n",
    "  'val_auc': 0.6759},\n",
    " {\n",
    "         'fold':3,\n",
    "     'train_loss': 0.3854,\n",
    "  'train_acc': 0.7907,\n",
    "  'train_f1': 0.7273,\n",
    "  'train_auc': 0.8079,\n",
    "  'val_loss': 0.6474,\n",
    "  'val_acc': 0.5814,\n",
    "  'val_f1': 0.4706,\n",
    "  'val_auc': 0.6944},\n",
    " {    'fold':4,\n",
    "     'train_loss': 0.4055, \n",
    "  'train_acc': 0.7674,\n",
    "  'train_f1': 0.7015,\n",
    "  'train_auc': 0.80615,\n",
    "  'val_loss': 0.6242,\n",
    "  'val_acc': 0.5349,\n",
    "  'val_f1': 0.5455,\n",
    "  'val_auc': 0.6449}]\n",
    "#fold4\n",
    "# Epoch 296/300, Train Loss: 0.3854, Acc: 0.7907, F1: 0.7273, AUC: 0.8079\n",
    "# Epoch 296/300, Valid Loss: 0.6474, Acc: 0.5814, F1: 0.4706, AUC: 0.6944\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plot_cross_validation_results(results_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # metrics = ['train_loss', 'train_acc', 'train_f1', 'train_auc', 'val_loss', 'val_acc', 'val_f1', 'val_auc']\n",
    "    metrics = ['train_acc', 'train_f1', 'train_auc', 'val_acc', 'val_f1', 'val_auc']\n",
    "\n",
    "    results_melted = results_df.melt(id_vars=['fold'], value_vars=metrics, var_name='metric', value_name='value')\n",
    "    sns.boxplot(x='metric', y='value', data=results_melted)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cross-Validation Results')\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_validation_results(pd.DataFrame(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Epoch 52/100, Train Loss: 0.1270, Acc: 0.9401, F1: 0.9456, AUC: 0.9328\n",
    "# Epoch 52/100, Valid Loss: 0.7002, Acc: 0.8095, F1: 0.7941, AUC: 0.6950\n",
    "\n",
    "# Epoch 64/100, Train Loss: 0.4438, Acc: 0.7844, F1: 0.7636, AUC: 0.6688\n",
    "# Epoch 64/100, Valid Loss: 0.7932, Acc: 0.7143, F1: 0.7227, AUC: 0.6759\n",
    "\n",
    "# Epoch 64/100, Train Loss: 0.1905, Acc: 0.9162, F1: 0.9198, AUC: 0.8851\n",
    "# Epoch 64/100, Valid Loss: 1.4678, Acc: 0.5952, F1: 0.6104, AUC: 0.6667\n",
    "\n",
    "# Epoch 72/100, Train Loss: 0.4271, Acc: 0.8512, F1: 0.8521, AUC: 0.7997\n",
    "# Epoch 72/100, Valid Loss: 0.8073, Acc: 0.7561, F1: 0.7501, AUC: 0.6978\n",
    "\n",
    "# Epoch 45/100, Train Loss: 0.0941, Acc: 0.9641, F1: 0.9698, AUC: 0.9614\n",
    "# Epoch 45/100, Valid Loss: 1.0193, Acc: 0.7381, F1: 0.7509, AUC: 0.7346\n",
    "\n",
    "results=[{\n",
    "    'fold':0,\n",
    "    'train_loss': 0.1270,\n",
    "  'train_acc': 0.9401,\n",
    "  'train_f1': 0.9456,\n",
    "  'train_auc': 0.9328,\n",
    "  'val_loss': 0.7002,\n",
    "  'val_acc': 0.8095,\n",
    "  'val_f1': 0.7941,\n",
    "  'val_auc': 0.6950},\n",
    " {\n",
    "         'fold':1,\n",
    "     'train_loss': 0.4438,\n",
    "  'train_acc': 0.7844,\n",
    "  'train_f1': 0.7636,\n",
    "  'train_auc': 0.6688,\n",
    "  'val_loss': 0.7932,\n",
    "  'val_acc': 0.7143,\n",
    "  'val_f1': 0.7227,\n",
    "  'val_auc':0.6759},\n",
    " {\n",
    "         'fold':2,\n",
    "     'train_loss': 0.1905,\n",
    "  'train_acc': 0.9162,\n",
    "  'train_f1': 0.9198,\n",
    "  'train_auc': 0.8851,\n",
    "  'val_loss': 1.4678,\n",
    "  'val_acc': 0.5952,\n",
    "  'val_f1': 0.6104,\n",
    "  'val_auc': 0.6667},\n",
    " {\n",
    "         'fold':3,\n",
    "     'train_loss': 0.4271,\n",
    "  'train_acc': 0.8512,\n",
    "  'train_f1': 0.8521,\n",
    "  'train_auc': 0.7997,\n",
    "  'val_loss': 0.8073,\n",
    "  'val_acc': 0.7561,\n",
    "  'val_f1': 0.7501,\n",
    "  'val_auc': 0.6978},\n",
    " {    'fold':4,\n",
    "     'train_loss': 0.0941, \n",
    "  'train_acc':0.9641,\n",
    "  'train_f1': 0.9698,\n",
    "  'train_auc': 1.0193,\n",
    "  'val_loss': 1.0193,\n",
    "  'val_acc': 0.7381,\n",
    "  'val_f1': 0.7509,\n",
    "  'val_auc': 0.7346}]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plot_cross_validation_results(results_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # metrics = ['train_loss', 'train_acc', 'train_f1', 'train_auc', 'val_loss', 'val_acc', 'val_f1', 'val_auc']\n",
    "    metrics = ['train_acc', 'train_f1', 'train_auc', 'val_acc', 'val_f1', 'val_auc']\n",
    "\n",
    "    results_melted = results_df.melt(id_vars=['fold'], value_vars=metrics, var_name='metric', value_name='value')\n",
    "    sns.boxplot(x='metric', y='value', data=results_melted)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Cross-Validation Results')\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_validation_results(pd.DataFrame(results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigapath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
